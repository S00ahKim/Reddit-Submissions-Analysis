,date,year,month,day,hour,id,title,full_link,author,created,selftext,num_comments,score
0,2016-7-1,2016,7,1,9,4qox43,Day to day activity in this field,https://www.reddit.com/r/MachineLearning/comments/4qox43/day_to_day_activity_in_this_field/,iknowstuffwhatnow,1467332647,[removed],0,1
1,2016-7-1,2016,7,1,9,4qoyfu,Day to day activity and challenges,https://www.reddit.com/r/MachineLearning/comments/4qoyfu/day_to_day_activity_and_challenges/,[deleted],1467333161,[deleted],7,8
2,2016-7-1,2016,7,1,9,4qoza6,"100 experiments in parallel, 1 hour per iteration, no derivatives, how to solve?",https://www.reddit.com/r/MachineLearning/comments/4qoza6/100_experiments_in_parallel_1_hour_per_iteration/,hughperkins,1467333499,"Hi

Can run 100 experiments in parallel, one iteration an hour. Each experiment outputs one noisy loss estimate, at the end of the iteration, given an input (hyper?)parameter vector of 100 real valued scalars. No derivative function available. How to solve for the optimum hyperparameter vector?",12,0
3,2016-7-1,2016,7,1,9,4qp1b6,Robot's Rules of Order #ArtificialIntelligence @satyanadella @Slate #Robot,https://www.reddit.com/r/MachineLearning/comments/4qp1b6/robots_rules_of_order_artificialintelligence/,ksankar,1467334299,,0,1
4,2016-7-1,2016,7,1,10,4qp77h,Looking for Neural Networks and NLP Master Thesis,https://www.reddit.com/r/MachineLearning/comments/4qp77h/looking_for_neural_networks_and_nlp_master_thesis/,yazfield,1467336685,"I am starting preparation work on my master thesis, it's going to be about Text Classification with neural networks.
I am looking for some interesting thesis about NLP or vision primarily as I think it'll help me write mine, do you guys have suggestions? thanks.",2,0
5,2016-7-1,2016,7,1,13,4qpygv,Furniture images with labeled detailed attributes?,https://www.reddit.com/r/MachineLearning/comments/4qpygv/furniture_images_with_labeled_detailed_attributes/,Squirreldit,1467347987,"A group of friends and I are working on trying to create a multi labeling image classification system, in this case for furniture. We are mostly looking for images of furniture with a detailed set of attributes for each picture that we can use to train and refine our model. To run the training and system we have the most up-to-date Caffe framework setup on a pretty good hardware (Ubuntu GPU Nvida etc..).
We were curious if anyone out there had a labeled dataset of furniture images with attributes (pictures of chairs with their color, style, fabric or couches with similar attributes for example) that could be shared. 
Thank you",3,1
6,2016-7-1,2016,7,1,14,4qq60j,Arxiv Endorsement,https://www.reddit.com/r/MachineLearning/comments/4qq60j/arxiv_endorsement/,ArmenAg,1467351604,[removed],7,1
7,2016-7-1,2016,7,1,15,4qqaj4,Suggestions on how to go about creating OCR (Optical Character Recognition) software,https://www.reddit.com/r/MachineLearning/comments/4qqaj4/suggestions_on_how_to_go_about_creating_ocr/,Rizikio,1467353805,"Hey guys,

I have a pet project I want to work on that involves reading text from pictures, and I want to try my hand at OCR. Do you lot have any suggestions for tutorials or informative papers I can follow to start learning how to implement one?

Just for more background information: I have a strong background in mathematics (including linear) so that shouldn't be an issue. However, I have studied NN's in school at a mostly theoretical level and have very little applicational experience. I'm a ML novice, but I've been programming for a decade so I'm not exactly out of my realm. Also, the project basically involves reading text at various natural angles from a phone camera, but I don't care too much about the development environment I learn the OCR implementation in, as long as its not heavily dependent on existing libraries.

Any help and suggestions you guys have is greatly appreciated! Thanks so much in advance",2,0
8,2016-7-1,2016,7,1,16,4qqlu8,[1512.01693] Deep Attention Recurrent Q-Network,https://www.reddit.com/r/MachineLearning/comments/4qqlu8/151201693_deep_attention_recurrent_qnetwork/,DavidSJ,1467359637,,2,22
9,2016-7-1,2016,7,1,17,4qqs5z,Making Tree Ensembles Interpretable: A Bayesian Model Selection Approach,https://www.reddit.com/r/MachineLearning/comments/4qqs5z/making_tree_ensembles_interpretable_a_bayesian/,sato9hara,1467363103,,1,2
10,2016-7-1,2016,7,1,19,4qr31x,Neural Networks in iOS 10 and macOS,https://www.reddit.com/r/MachineLearning/comments/4qr31x/neural_networks_in_ios_10_and_macos/,alpyhp,1467369331,,0,7
11,2016-7-1,2016,7,1,21,4qrgh8,Is it legal to use copyright material as training data?,https://www.reddit.com/r/MachineLearning/comments/4qrgh8/is_it_legal_to_use_copyright_material_as_training/,alekhka,1467375837,"I am planning to train a Neural Network on some videos which happen to be copyrighted. Is it illegal to use them?

Edit: I probably should add this- The trained NN will actually be an Open Source alternative for the very software used to make the videos. i.e I essentially make the NN learn what their tool does.

Edit 2: After a little more considerations I have decided to go ahead and do it! A big thanks to all of you for this!! I'll update developments, someone might find it useful someday.
",82,76
12,2016-7-1,2016,7,1,21,4qrkou,Human Machine Interface Market Volume Forecast and Value Chain Analysis 2016-2020,https://www.reddit.com/r/MachineLearning/comments/4qrkou/human_machine_interface_market_volume_forecast/,randyn586,1467377559,[removed],0,1
13,2016-7-1,2016,7,1,22,4qrmci,Open source NLP models for building a bot?,https://www.reddit.com/r/MachineLearning/comments/4qrmci/open_source_nlp_models_for_building_a_bot/,shadow12348,1467378238,"Hey,

I'm trying to building a simple bot where I send a text message and I get a relevant text reply. Something like the way siri works. I'm not aware of any existing pre-trained models that are capable of high level text analysis. Are there any I can use with a backend web service like node js?

If I had to start from scratch and train my own bot like Apple's Siri, where do I start, in the context of programming?

Thanks in advance! Any help is appreciated.",9,5
14,2016-7-2,2016,7,2,0,4qs6ew,Using Neural Networks to predict user ratings - Florian Strub - RecsysFR,https://www.reddit.com/r/MachineLearning/comments/4qs6ew/using_neural_networks_to_predict_user_ratings/,Agagla,1467385324,,0,2
15,2016-7-2,2016,7,2,0,4qs78m,What prevents other companies to produce cheaper GPUs?,https://www.reddit.com/r/MachineLearning/comments/4qs78m/what_prevents_other_companies_to_produce_cheaper/,NovaRom,1467385585,"No, really. Why for example ARM doesn't try to design something similar to Maxwell/Pascal? Expensive patents, lack of engineers/experience, large investment/risks?",14,3
16,2016-7-2,2016,7,2,0,4qs8ch,How might quantum computers advance machine learning efforts?,https://www.reddit.com/r/MachineLearning/comments/4qs8ch/how_might_quantum_computers_advance_machine/,FuzziCat,1467385919,IBM and DWave are making their quantum computers available for research use.  What types of machine learning algorithms would be most appropriate for and/or could be completed much more quickly by quantum computers?,15,4
17,2016-7-2,2016,7,2,0,4qs948,"[1606.08813] EU regulations on algorithmic decision-making and a ""right to explanation""",https://www.reddit.com/r/MachineLearning/comments/4qs948/160608813_eu_regulations_on_algorithmic/,negazirana,1467386179,,29,39
18,2016-7-2,2016,7,2,0,4qsgbh,Machines learning differences between writing styles?,https://www.reddit.com/r/MachineLearning/comments/4qsgbh/machines_learning_differences_between_writing/,klop2031,1467388540,"I was interested in learning if it is possible to have a machine learn different styles of writing of the same topic.

Here is the scenario: 

&gt; If many people read a news paper article (same one), they then are asked to summarize it; I want the machine to be able to learn from the summaries and come up with some kind of common summary.

Is this possible? Where do I start? I assume we would need some sort of word embedding?",1,1
19,2016-7-2,2016,7,2,1,4qshk3,Max norm gradient/weight clipping for convolutional networks?,https://www.reddit.com/r/MachineLearning/comments/4qshk3/max_norm_gradientweight_clipping_for/,harharveryfunny,1467388943,"I'm curious what's best/common practice for applying gradient and/or weight clipping for convolutional networks, and specifically max norm clipping.

1) Is it usual to clip gradients or weights or both (or perhaps neither unless proved necessary)?

2) When applying max norm clipping to convolutional filters, is it better/usual to clip each filter individually to it's own max norm, or clip the entire filter set (of a given layer/module) to their collective max norm?

3) Are their common practices other than clipping to handle the large losses/gradients/weights seen at the beginning of training (i.e use different learning rate or update type or whatever for first few iterations only)?
",6,7
20,2016-7-2,2016,7,2,2,4qstm3,Stanford's Probabilistic Graphical Models class on Coursera will run again this August,https://www.reddit.com/r/MachineLearning/comments/4qstm3/stanfords_probabilistic_graphical_models_class_on/,dataoverflow,1467392853,,14,102
21,2016-7-2,2016,7,2,3,4qt47r,Anybody downloaded machine Learning by Pedro Domingos?,https://www.reddit.com/r/MachineLearning/comments/4qt47r/anybody_downloaded_machine_learning_by_pedro/,thatmlguy1,1467396232,"I was in the middle of downloading it when Coursera changed their platform. The course is no longer available. If anybody has it please send it to me.
",4,9
22,2016-7-2,2016,7,2,3,4qt6pi,Wrote my own Artificial Neural Network in Node as a learning exercise :) Feedback would be awesome!,https://www.reddit.com/r/MachineLearning/comments/4qt6pi/wrote_my_own_artificial_neural_network_in_node_as/,codeeverything,1467396993,,3,2
23,2016-7-2,2016,7,2,4,4qtoq0,High school student interested in deep learning?,https://www.reddit.com/r/MachineLearning/comments/4qtoq0/high_school_student_interested_in_deep_learning/,[deleted],1467402864,[deleted],1,0
24,2016-7-2,2016,7,2,7,4que7q,Inside a Neural Network - Computerphile,https://www.reddit.com/r/MachineLearning/comments/4que7q/inside_a_neural_network_computerphile/,danthemango,1467411989,,1,16
25,2016-7-2,2016,7,2,10,4qv278,What Learning Systems do Intelligent Agents Need? Complementary Learning Systems Theory Updated,https://www.reddit.com/r/MachineLearning/comments/4qv278/what_learning_systems_do_intelligent_agents_need/,jnhwkim,1467421649,,1,7
26,2016-7-2,2016,7,2,10,4qv81j,FastForest: fast but accurate oblique extreme random forest,https://www.reddit.com/r/MachineLearning/comments/4qv81j/fastforest_fast_but_accurate_oblique_extreme/,godspeed_china,1467424268,"available at http://sourceforge.net/p/fastforest  
source code will be available after paper publication.  
designed for big data. &lt;0.5 hours for 11 miilion samples with 28 features (Higgs dataset)  
If your have few test sample it can be even faster due to partitial tree induction.  
more accurate than random forest without tuning (default parameters).",10,10
27,2016-7-2,2016,7,2,11,4qv9es,Starting a new subreddit about AI ethics,https://www.reddit.com/r/MachineLearning/comments/4qv9es/starting_a_new_subreddit_about_ai_ethics/,UmamiSalami,1467424902,"I'm sure many of you have heard about the ethical issues with AI decisionmaking and disparate impact. Given the frequent discussions with thousands of comments that we see on Reddit about self driving cars, robot ethics, etc, I thought we should have a good subreddit devoted specifically for such topics. It will be a place where people with specific interest and understanding of ethics and AI can congregate and share their knowledge - as opposed to a place like /r/philosophy, where most people haven't studied computer science, or a place like /r/machinelearning, where the focus is very technical. Unlike restrictive subs like /r/philosophy or /r/askphilosophy, we will be liberal with allowing open-ended discussions and links.

Below is the link; if you are interested then please subscribe and post something that you find interesting. In a few days or weeks I expect that we will be taking off with a growing userbase.

https://www.reddit.com/r/AIethics

Thank you.",3,0
28,2016-7-2,2016,7,2,13,4qvrsl,Recommender System Based on Autoencoders,https://www.reddit.com/r/MachineLearning/comments/4qvrsl/recommender_system_based_on_autoencoders/,Jxieeducation,1467433616,,0,6
29,2016-7-2,2016,7,2,13,4qvv1m,Neural Networks for Machine Learning Quizzes,https://www.reddit.com/r/MachineLearning/comments/4qvv1m/neural_networks_for_machine_learning_quizzes/,MachineCoder,1467435317,[removed],0,1
30,2016-7-2,2016,7,2,14,4qvxvv,Request: Can someone link me to a texture-upscaling program?,https://www.reddit.com/r/MachineLearning/comments/4qvxvv/request_can_someone_link_me_to_a_textureupscaling/,Midhav,1467436769,"IIRC there was a machine learning algorithm posted on either r/futurology or r/singularity, so I assume that it would appeared here as well, about a month or so ago. It improves the quality/detail of a texture by upscaling the resolution. If anyone could provide me the link to it, I'd be really grateful. I tried looking on Google for it to no avail. It's for a game whose development team I'm a part of.",3,0
31,2016-7-2,2016,7,2,14,4qvzy1,How can I learn a function that maximizes the (supervised) correct vector from a non-fixed size set of these vectors as input?,https://www.reddit.com/r/MachineLearning/comments/4qvzy1/how_can_i_learn_a_function_that_maximizes_the/,compsc,1467437874,"The problem is this.  Given a non-fixed size set of low-dimensional feature vectors as input, output the feature vector corresponding to the best candidate.  My training data are labeled instances of this scenario, e.g.:

    {a, b, c, ...} -&gt; b is the feature vector of the best candidate, and so f(b) should be greatest among {f(a), f(b), f(c)...}


To illustrate, say I want to be able to, when given a set of houses, say which one is the most valuable among the set (I don't need to predict a price necessarily, just which is the most pricey).  I might have

    {[3 bd, 2br, 450 sq ft], [1bd, 1br, 500 sq ft], ... } -&gt; the ith of these is the most valuable

as training examples, so I want to learn a function f which tends to maximize the feature vector of supervised best candidate house.

Keep in mind, in trial A, a_j might be the best house, but in trial B, b_k is *not* the best house, and yet it's more valuable than a_j. There just happened to be less-good houses in set A.  Just like the best graduate from University of Phoenix might not be as 'good' as the 10th best from MIT.

I have no additional ordering information other than which is *most* correct.  In my case there is no 2nd-most correct.

So I can't picture this as a typical regression or binary classification problem.

Any help or leads is appreciated

thanks",4,1
32,2016-7-2,2016,7,2,15,4qw4xe,"Top pandas, numpy and scipy functions used in github repos",https://www.reddit.com/r/MachineLearning/comments/4qw4xe/top_pandas_numpy_and_scipy_functions_used_in/,kozikow,1467440602,,0,1
33,2016-7-2,2016,7,2,16,4qwaip,"How to combine harvester lubricate in ""Three summer""?",https://www.reddit.com/r/MachineLearning/comments/4qwaip/how_to_combine_harvester_lubricate_in_three_summer/,ricemachine,1467443937,,0,1
34,2016-7-2,2016,7,2,17,4qwfth,[1606.08415v1] Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units,https://www.reddit.com/r/MachineLearning/comments/4qwfth/160608415v1_bridging_nonlinearities_and/,x2342,1467447422,,11,9
35,2016-7-2,2016,7,2,17,4qwht7,Software faults raise questions about the validity of brain studies,https://www.reddit.com/r/MachineLearning/comments/4qwht7/software_faults_raise_questions_about_the/,abstractcontrol,1467448802,,18,130
36,2016-7-2,2016,7,2,18,4qwkh2,10 Ways Machine Learning Is Revolutionizing Manufacturing,https://www.reddit.com/r/MachineLearning/comments/4qwkh2/10_ways_machine_learning_is_revolutionizing/,subhkirti,1467450662,,0,1
37,2016-7-2,2016,7,2,18,4qwlje,New Julia Pkg for Variational Bayes Topic Modeling (x-post from /r/julia),https://www.reddit.com/r/MachineLearning/comments/4qwlje/new_julia_pkg_for_variational_bayes_topic/,topicmodelsvb,1467451447,"Hi all!

So I have just released a new variational Bayes topic modeling package for Julia, which can be found here:
https://github.com/esproff/TopicModelsVB.jl

The models included are:

1. Latent Dirichlet Allocation (LDA)

2. Filtered Latent Dirichlet Allocation (fLDA)

3. Correlated Topic Model (CTM)

4. Filtered Correlated Topic Model (fCTM)

5. Dynamic Topic Model (DTM)

6. Collaborative Topic Poisson Factorization (CTPF)

This is, as far as I can tell, the most comprehensive open-source topic modeling package to date. It's still a bit rough around the edges and I think there are still a few edge-case bugs deep in the belly of 1 or 2 of the algorithms, but overall it's polished enough that I think it needs to be tried out by other people besides myself.

I'm open to collaborators, and I'm especially interested in adding some GPGPU support, however, formally speaking, I'm trained as a mathematician, not a computer scientist or software engineer, and thus if you're an expert in GPGPU I'd be very interested in talking to you about adding this functionality as Bayesian learning can be *extremely* computationally intensive. (you can either contact me on this reddit account or through the email topicmodelsvb@mail.com)

On the other hand, if you're more into the applied math / machine learning side, there are still a number of models to implement, mostly non-parametric versions of the ones I've implemented, however I should warn you that Bayesian nonparametrics is not for the faint of heart.

Contributing a model could make a good school project or make up part of a thesis.",7,16
38,2016-7-2,2016,7,2,19,4qwszq,Neural Networks for Machine Learning Quizzes,https://www.reddit.com/r/MachineLearning/comments/4qwszq/neural_networks_for_machine_learning_quizzes/,MachineCoder,1467456889,https://class.coursera.org/neuralnets-2012-001/lecture is gone since 1. of July 2016. Does anyone have the in-video quizzes or quizzes with solutions and explanations/feedback?,6,0
39,2016-7-2,2016,7,2,20,4qwv12,Demis Hassabis: What Learning Systems do Intelligent Agents Need? [Full Paper],https://www.reddit.com/r/MachineLearning/comments/4qwv12/demis_hassabis_what_learning_systems_do/,polytop3,1467458248,,0,1
40,2016-7-2,2016,7,2,20,4qwwgf,Data ScienceTech Institute programmes recognised by Campus France,https://www.reddit.com/r/MachineLearning/comments/4qwwgf/data_sciencetech_institute_programmes_recognised/,datasciencetech,1467459118,[removed],0,1
41,2016-7-2,2016,7,2,22,4qx8sw,"Why do I need to prepare the train data, and what does it mean?",https://www.reddit.com/r/MachineLearning/comments/4qx8sw/why_do_i_need_to_prepare_the_train_data_and_what/,gabegabe6,1467466574,"So I was looking at a python script with opencv and I saw these 2 lines:

    train = x[:,:50].reshape(-1,400).astype(np.float32)
    test = x[:,50:100].reshape(-1,400).astype(np.float32)

And I understand the first part that we separate the training set to 2500 and 2500 images but why do we need to reshape after that?",9,0
42,2016-7-2,2016,7,2,23,4qxbpp,How to compensate lack of advanced degree in machine learning if one wants to work on ML related jobs? Will udacity nanodegree help? Any anecdotes of people who tried?,https://www.reddit.com/r/MachineLearning/comments/4qxbpp/how_to_compensate_lack_of_advanced_degree_in/,KulchaNinja,1467468099,"I'm recent CS graduate. I took couple of ML &amp; AI courses during my undergrad. I've decent foundation of probability, linear algebra &amp; calculus. I just completed CMU's Tom mitchell class. (10-701). I also worked on couple of awesome ML projects. I'm NOT looking for data science or data analyst job. 


Is there any possibility to get machine learning engineer job? or even software engineer recent grad (machine learning) position at big 4? Will Udacity nanodegree help me in this? I just browsed course materials and projects. It seemed very easy compared to Tom mitchell class. But they do have other features like projects, github review, mock interviews &amp; linkedin review. I'm not based in US. So, Nanodegree plus is not an option. How this nanodegree viewed in industry, especially when done by CS graduates? I'm asking because certifications are not viewed quite positively. (Think Java, Microsoft certification and all that). 


Is there any way I can compensate my lack of advanced degree in these field? 

**Edit** : Big 4 = Google, Microsoft, Facebook, Apple. Sorry for confusion. ",26,5
43,2016-7-2,2016,7,2,23,4qxfix,Has anyone taken Machine learning Udacity course,https://www.reddit.com/r/MachineLearning/comments/4qxfix/has_anyone_taken_machine_learning_udacity_course/,kailovesdata,1467469976,Is it good? Has anyone taken it ,2,0
44,2016-7-3,2016,7,3,0,4qxnzt,how to use NLP to predict price?,https://www.reddit.com/r/MachineLearning/comments/4qxnzt/how_to_use_nlp_to_predict_price/,asnee0,1467473661,"Hi, I'm  trying to use NLP to predict the price of product, below is my idea,

1. Get articles from WSJ and NY Times by using their API (set keywords like 'oil')

2. Clean the data( nltk or tm in R) and  make it into a dtm format.

3. Use TF-IDF to choose some keywords which effect the price most (like 'battle').

4. For each day, calculate the final score (comes from all articles in one day) to predict the price.

The reason I do so is because that these articles do reflects people's attitude towards products and they are objective, and my question is how to optimize the scoring system do that the accuracy of our prediction could be better. ( My personal opinion is to build a system from super negative to super positive and it has 10 levels or more, and do classification with methods like Neural Network) 

Can you give me some advice or a better way to achieve this goal?
also what tools and opensource is best to utilize?",10,0
45,2016-7-3,2016,7,3,0,4qxohc,Why AI's massive disruptions may be just what you're looking for,https://www.reddit.com/r/MachineLearning/comments/4qxohc/why_ais_massive_disruptions_may_be_just_what/,dunkin1980,1467473865,,0,0
46,2016-7-3,2016,7,3,2,4qy325,"Is ""Python Machine Learning"" by Sebastian Raschka a good book?",https://www.reddit.com/r/MachineLearning/comments/4qy325/is_python_machine_learning_by_sebastian_raschka_a/,adamnemecek,1467479749,"I've seen it generally praised online but I'm curious what do you guys think

https://smile.amazon.com/Python-Machine-Learning-Sebastian-Raschka-ebook/dp/B00YSILNL0?sa-no-redirect=1

",23,24
47,2016-7-3,2016,7,3,4,4qyjiq,Machine Learning - WAYR (What Are You Reading) - Week 1,https://www.reddit.com/r/MachineLearning/comments/4qyjiq/machine_learning_wayr_what_are_you_reading_week_1/,Deinos_Mousike,1467486378,"This is a place to share machine learning research papers, journals, and articles that you're reading this week. If it relates to what you're researching, by all means elaborate and give us your insight, otherwise it could just be an interesting paper you've read.

Preferably you should link the arxiv page (not the PDF, you can easily access the PDF from the summary page but not the other way around) or any other pertinent links.

Besides that, there are no rules, have fun.",34,95
48,2016-7-3,2016,7,3,5,4qyu4g,"If a binary classifier (neural network model) achieves 99% training accuracy with 65% validation accuracy, what to do next?",https://www.reddit.com/r/MachineLearning/comments/4qyu4g/if_a_binary_classifier_neural_network_model/,gongzhitaao,1467490713,"Generally I'm considering:

1. Add regularizer (dropout, L2, etc), which helps to bring accuracy to 75%.

2. Shrink model size.  Which does not help much.

Any other general advice?

BTW, if the model could achieve 99% training accuracy, does it mean that with proper configuration the validation accuracy could also be very high?

",21,1
49,2016-7-3,2016,7,3,5,4qyuv5,information technology news heavy equipment modern road construction machinery,https://www.reddit.com/r/MachineLearning/comments/4qyuv5/information_technology_news_heavy_equipment/,top10cartoons,1467491006,,0,1
50,2016-7-3,2016,7,3,5,4qyvfq,Tetiana Ivanova - How to become a Data Scientist in 6 months a hackers approach to career planning,https://www.reddit.com/r/MachineLearning/comments/4qyvfq/tetiana_ivanova_how_to_become_a_data_scientist_in/,slavat,1467491241,,1,4
51,2016-7-3,2016,7,3,5,4qywf6,information technology news heavy equipment modern road construction machinery,https://www.reddit.com/r/MachineLearning/comments/4qywf6/information_technology_news_heavy_equipment/,besstreetfightvideos,1467491638,,0,1
52,2016-7-3,2016,7,3,5,4qyx26,Good Books About ML,https://www.reddit.com/r/MachineLearning/comments/4qyx26/good_books_about_ml/,yeyemelocotom,1467491869, Hi!! Are there some books I can read about fundamentals of machine learning?,6,1
53,2016-7-3,2016,7,3,6,4qz0c6,Machine learning for large-scale SEM accounts,https://www.reddit.com/r/MachineLearning/comments/4qz0c6/machine_learning_for_largescale_sem_accounts/,seojoeschmo,1467493224,,1,1
54,2016-7-3,2016,7,3,6,4qz4kw,How did you first get your hands dirty in machine learning?,https://www.reddit.com/r/MachineLearning/comments/4qz4kw/how_did_you_first_get_your_hands_dirty_in_machine/,CS_throwaway_GOAL,1467494999,"Hello everyone, I'm a computer science student and I find machine learning very interesting. I have a very abstract understanding of how machine learning works (analyze a lot of data, use that to make guesses about the probabilities of certain outcomes, analyze results, repeat) but I feel like I'm far from being able to actually implement a neural network or something. I've already taken numerous calculus courses but I have yet to take a hard core statistics class which I know will be important. I was wondering where I should go to learn about the ""hello world"" of machine learning. Where can I learn how to implement an incredibly simple form of machine learning? How did you first start to learn about implementing machine learning concepts? For what its worth I like to work in C++ and python",5,0
55,2016-7-3,2016,7,3,7,4qzci3,GPU-accelerated Theano &amp; Keras on Windows 10 native,https://www.reddit.com/r/MachineLearning/comments/4qzci3/gpuaccelerated_theano_keras_on_windows_10_native/,tezcaML,1467498393,"If, and only if, you **MUST** run Windows 10 and still want decent performance running Theano &amp; Keras, the following guide may help. It describes GPU acceleration on Windows 10 in **native mode** -- no VMs, no Docker:

https://github.com/philferriere/dlwin

We tested it on the following hardware:

- Dell Precision T7500, 96BG RAM [Intel Xeon E5605 @ 2.13 GHz (2 processors, 8 cores total)]
- NVIDIA GeForce Titan X, 12GB RAM [Driver version: 10.18.13.5390 Beta (ForceWare 353.90) / Win 10 64]

We used the following libraries:

- Visual Studio 2013 Community Edition Update 4 [Used for its C/C++ compiler (not its IDE)]
- CUDA 7.5.18 (64-bit) [Used for its GPU math libraries, card driver, and CUDA compiler]
- MinGW-w64 (5.3.0) [Used for its Unix-like compiler and build tools (g++/gcc, make...) for Windows]
- Anaconda (64-bit) w. Python 2.7 (Anaconda2-4.1.0) [A Python distro that gives us NumPy, SciPy, and other scientific libraries]
- Theano 0.8.2 [Used to evaluate mathematical expressions on multi-dimensional arrays]
- Keras 1.0.5 [Used for deep learning on top of Theano]
- OpenBLAS 0.2.14 (Optional) [Used for its CPU-optimized implementation of many linear algebra operations]
- cuDNN v5 (Recommended) [Used to run vastly faster convolution neural networks]

Hope this helps!",4,29
56,2016-7-3,2016,7,3,7,4qzgnu,ML and floating point numbers,https://www.reddit.com/r/MachineLearning/comments/4qzgnu/ml_and_floating_point_numbers/,[deleted],1467500146,[deleted],0,1
57,2016-7-3,2016,7,3,8,4qzhpn,"Who would like to start a collaborative Youtube channel that provides an explanation of various research papers? [xpost /r/compsci, by request]",https://www.reddit.com/r/MachineLearning/comments/4qzhpn/who_would_like_to_start_a_collaborative_youtube/,ryandoughertyasu,1467500583,,40,236
58,2016-7-3,2016,7,3,8,4qzo9h,"Aside from the Deep Learning Hype, What are some other interesting research topics for grad students coming into the field of statistics/machine learning?",https://www.reddit.com/r/MachineLearning/comments/4qzo9h/aside_from_the_deep_learning_hype_what_are_some/,jxnlco,1467503472,,11,1
59,2016-7-3,2016,7,3,10,4r04cy,Cluster rows in Pandas Dataframes: Should I use k-means or just bin the data?,https://www.reddit.com/r/MachineLearning/comments/4r04cy/cluster_rows_in_pandas_dataframes_should_i_use/,Zeekawla99ii,1467511075,"I have the following pandas DataFrame.

    import pandas as pd
    df = pd.read_csv('filename.csv')
    
    print(df)

          A       B         C	        D
    0     2  	  0	        11      	0.053095
    1     2 	  0     	11      	0.059815
    2     0 	  35    	11      	0.055268
    3     0       35    	11      	0.054573
    4     0       1     	11      	0.054081
    5     0       2       	11       	0.054426
    6     0       1     	11       	0.054426
    7     0       1     	11       	0.054426
    8     42	  7	        3       	0.048208
    9     42	  7      	3        	0.050765
    10    42	  7      	3       	0.05325

        ....

The problem is, the data is naturally ""clustered"" into groups, but this data is not given. From the above, rows 0-1 are one group, rows 2-3 are a group, rows 4-7 are a group, and 8-10 are a group. 

I need to impute this information. One could use machine learning (or possibly do this only using pandas?)

Can users groupby the values of the columns to create these groups? The problem is the values are not *exact*. For the third group, column `B` has group 1, 2, 1, 1. 

It seems like the CORRECT we to unsure there are no errors is to use some sort of binning: let's say we use bin size +/- 1. Bin values A and compare with binned values from B. Based on some condition, A is the correct cluster. If not, keep comparing. 

How would you use something like sci-kit learn for this? Would you recommend k-means? 

Should I use k-means (or another clustering algorithm), or is it best to simply bin the data? ",3,0
60,2016-7-3,2016,7,3,17,4r17y5,A small and easy introduction to Transductive Learning,https://www.reddit.com/r/MachineLearning/comments/4r17y5/a_small_and_easy_introduction_to_transductive/,sachinrjoglekar,1467533569,,2,2
61,2016-7-3,2016,7,3,18,4r1ggo,technews machines collection agriculture technology,https://www.reddit.com/r/MachineLearning/comments/4r1ggo/technews_machines_collection_agriculture/,besstreetfightvideos,1467539872,,0,1
62,2016-7-3,2016,7,3,19,4r1ji8,[Meta] Sending emails to researchers/professors,https://www.reddit.com/r/MachineLearning/comments/4r1ji8/meta_sending_emails_to_researchersprofessors/,Kiuhnm,1467542002,"Is it normal for professors or researchers in ML not to reply to emails?

When I read some material and I find some mistakes or I have some suggestions for parts which are a bit lacking(*), I usually contact the author, but I get systematically ignored. Is it because I'm not a colleague or a student of theirs and they see themselves as too high up to be bothered by mere mortals?

---
(*) A clarification: For the ""lacking"" part, I was thinking about course notes. Many universities make their material available so I think it's useful to point out if some parts need some clarification, if there are mistakes (I'm not talking about typos) or if a rigorous derivation (when easy and short) should be added as a complement to an intuitive but vague explanation. I'm saying this because it seems someone got a little upset about the ""lacking"" part!

I never told a researcher (especially one I didn't know personally) that his/her paper was lacking!

---

I use the formula

    Dear FirstName Surname,
        ....

    Best regards
    MyFirstName MySurname

which, I think, is polite enough.

I find this behavior disappointing and irritating. After all, I'm not asking for explanations but pointing out some mistakes (confirmed by others).

What's your experience? Should I just stop sending emails?
",25,4
63,2016-7-3,2016,7,3,19,4r1l70,Right choice of framework,https://www.reddit.com/r/MachineLearning/comments/4r1l70/right_choice_of_framework/,dzyl,1467543130,"I'm fairly comfortable with 'normal' machine learning and I've been reading up a lot for deep learning. In a few months I will do a big project with deep learning and even though my theory is getting close to the basics that I will need I am uncertain of what framework to use. I will explain what I have, what I'm familiar with and what I need or prefer, and hopefully someone can point me in the right direction, there are so many frameworks to choose from.

What I can:

- Very comfortable programming in Python, also have some R, Matlab and Java experience, no C++. Would strongly prefer Python
- A lot of experience using third-party packages
- Windows or Linux are both fine for me

What I have:

- Budget of around $15000 for hardware, should I go for one (or two) very powerful GPUs or a small cluster of weaker GPUs?

What I want:

- Train on top of existing CNNs, by freezing the first number of layers of imported networks
- Both autoencoders and traditional CNNs are important
- For my use case I will also need to build up CNNs that combine several inputs

I've looked at Keras where the layer structure looks very nice, however most trained networks look to be in Caffe. Making predictions on new input will need to happen in Python.

Thanks a lot for thinking with me!",5,1
64,2016-7-3,2016,7,3,20,4r1np7,Heavy Metal and Natural Language Processing - Part 1,https://www.reddit.com/r/MachineLearning/comments/4r1np7/heavy_metal_and_natural_language_processing_part_1/,perceptron01,1467544920,,5,57
65,2016-7-3,2016,7,3,21,4r1rz1,Intel tunes its mega-chip for machine learning,https://www.reddit.com/r/MachineLearning/comments/4r1rz1/intel_tunes_its_megachip_for_machine_learning/,blanchettemack,1467547663,,0,0
66,2016-7-3,2016,7,3,23,4r26ff,What are some efficient methods of information extraction from research papers?,https://www.reddit.com/r/MachineLearning/comments/4r26ff/what_are_some_efficient_methods_of_information/,perceptron01,1467555618,"I am particularly interested in genomics research papers and finding interactions between genes, but any pointers would be appreciated.

I found [this paper](http://www.cs.utexas.edu/~ml/papers/bionlp-aimed-04.pdf) intersting, but I was wondering how methodology changed with the recent progress",5,2
67,2016-7-4,2016,7,4,1,4r2mi3,Making money from Machine Learning With James Harris Simons,https://www.reddit.com/r/MachineLearning/comments/4r2mi3/making_money_from_machine_learning_with_james/,3eyedravens,1467562690,,1,4
68,2016-7-4,2016,7,4,1,4r2qgh,What are your tips and tricks for training convolutional autoencorders?,https://www.reddit.com/r/MachineLearning/comments/4r2qgh/what_are_your_tips_and_tricks_for_training/,EdmondRR,1467564263,"Share tricks, architectures, regularization methods, input preprocessing, anything that you want!

I'm currently working on an autoencoder that looks like:
(convolution &gt; dropout &gt; convolution &gt; dropout &gt; maxpool ) x 3 and then upscales with the same layers, plus a sigmoidal convolution at the end.

 I'm gonna put the code on github soon!",3,11
69,2016-7-4,2016,7,4,5,4r3ke0,"What are the best events, hackathons, and/or places up and coming devs need to know about?",https://www.reddit.com/r/MachineLearning/comments/4r3ke0/what_are_the_best_events_hackathons_andor_places/,LazavsLackey,1467576431,Thanks for posting. Still impressed at what's been shared via this sub in the past few months so please keep it up.,3,0
70,2016-7-4,2016,7,4,5,4r3pjy,Variational Autoencoders (VAE) vs Generative Adversarial Networks (GAN)?,https://www.reddit.com/r/MachineLearning/comments/4r3pjy/variational_autoencoders_vae_vs_generative/,ill-logical,1467578478,"VAEs can be used with discrete inputs, while GANs can be used with discrete latent variables. 

However, assuming both are continuous, is there any reason to prefer one over the other?",18,17
71,2016-7-4,2016,7,4,5,4r3pv2,Free Ebook Madchine Learning,https://www.reddit.com/r/MachineLearning/comments/4r3pv2/free_ebook_madchine_learning/,cristmartin,1467578596,,1,0
72,2016-7-4,2016,7,4,5,4r3pyv,Are there any economics-related competitions?,https://www.reddit.com/r/MachineLearning/comments/4r3pyv/are_there_any_economicsrelated_competitions/,Icko_,1467578639,I found only [this](https://www.drivendata.org/competitions/1/) one. It does not seem there is any real competition though. Are there any other?,4,0
73,2016-7-4,2016,7,4,6,4r3vkr,This Week in ML &amp; AI Podcast - 7/1/16,https://www.reddit.com/r/MachineLearning/comments/4r3vkr/this_week_in_ml_ai_podcast_7116/,sbc1906,1467580821,"Feedback seemed to suggest a strong preference for these mini-summaries, so here goes.

Show Page =&gt; https://twimlai.com/7

Friday's Show:

* Tesla autopilot crash questions

* EU legislation may prohibit some ML use

* Business: Top 10 most well-funded AI startups, business implications of ML, Amazon's robot arms race

* Research: CVPR 2016, AI fighter pilot beats human expert

* Projects: AI XPRIZE, Messenger updates, Linguistic User Interfaces, building a chatbot, ML design patterns

As always, please don't hesitate to share feedback.",0,32
74,2016-7-4,2016,7,4,6,4r3xvy,"One-Shot Learning - Fresh Machine Learning #1 (New series, hope you guys like it! - Siraj)",https://www.reddit.com/r/MachineLearning/comments/4r3xvy/oneshot_learning_fresh_machine_learning_1_new/,llSourcell,1467581700,,16,64
75,2016-7-4,2016,7,4,12,4r5cwk,Theano help in saving model with multiple layers?,https://www.reddit.com/r/MachineLearning/comments/4r5cwk/theano_help_in_saving_model_with_multiple_layers/,iamquah,1467603515,"Hey all,

I've got a quick question: I'm extending the [deep learning tutorial code] (http://deeplearning.net/tutorial/code/convolutional_mlp.py) and want to save the model or the weights. It seems that examples on how to do this are sparse, and the tutorial itself only covers it for a logistic regression which is in some sense a single layer thing.

Anyone have any solutions on how to save the weights and biases of multiple layers that are ""connected"" (output of one to input of other in training)",10,0
76,2016-7-4,2016,7,4,13,4r5msm,"what is c', b' and h' in Restricted Boltzmann Machine energy function",https://www.reddit.com/r/MachineLearning/comments/4r5msm/what_is_c_b_and_h_in_restricted_boltzmann_machine/,John_Smith111,1467608238,[removed],1,2
77,2016-7-4,2016,7,4,16,4r62ru,"Getting into machine learning,are there any tutorials without teachers?",https://www.reddit.com/r/MachineLearning/comments/4r62ru/getting_into_machine_learningare_there_any/,TheSphaat,1467616471,I usually learn quickly and i dont really have 2-3 months to spare. Are there any sites similar to codecademy that could teach me the basics?,2,0
78,2016-7-4,2016,7,4,16,4r633l,neural style for sound/music?,https://www.reddit.com/r/MachineLearning/comments/4r633l/neural_style_for_soundmusic/,Dobias,1467616661,"With [neural-style](https://github.com/jcjohnson/neural-style) one can take the style of an artist and transfer it onto some photo.

Would it be possible to do something similar with music? I'm thinking in terms of outputting Mariah Carey singing ""Lithium"" by Kurt Cobain etc.
",10,5
79,2016-7-4,2016,7,4,16,4r65q0,[Caffe] Training output starts off 0 and stays at 0,https://www.reddit.com/r/MachineLearning/comments/4r65q0/caffe_training_output_starts_off_0_and_stays_at_0/,[deleted],1467618227,[deleted],0,0
80,2016-7-4,2016,7,4,18,4r6ewg,Applying Machine Learning to InfoSec,https://www.reddit.com/r/MachineLearning/comments/4r6ewg/applying_machine_learning_to_infosec/,arshakn,1467623489,,3,0
81,2016-7-4,2016,7,4,19,4r6m9o,How would one go about using ML techniques for diet plans?,https://www.reddit.com/r/MachineLearning/comments/4r6m9o/how_would_one_go_about_using_ml_techniques_for/,cosmikduster,1467627599,"Say, I am able to collect data for users which is both (a) static attributes: gender, age, height, diseases and (b) time-series data such as weight, blood sugar level, their daily diet logs, amount of water they drink, their medical test reports, their exercise history and so on.

What ML techniques can be used to automatically generate a diet plan for that user which helps him/her achieve her weight goal?

I am a beginner to ML and want to understand how time-series data can be incorporated in such ""online learning"" problems.",4,0
82,2016-7-4,2016,7,4,20,4r6vdt,What is the high dispersion tank test,https://www.reddit.com/r/MachineLearning/comments/4r6vdt/what_is_the_high_dispersion_tank_test/,mixmachinery,1467632758,,1,1
83,2016-7-4,2016,7,4,21,4r70n8,tensorflow question - tiling,https://www.reddit.com/r/MachineLearning/comments/4r70n8/tensorflow_question_tiling/,snapillar,1467635607,"hi all, I have one question about tensorflow.
I'm making tiling layer which transforms the shape of input tensors. 
The detail is like this. If the input shape is 15x20x128 (h,w,ch), I want to change  it into 120x160x2 (h,w,ch). Each cell of input has 128 features, and these go to 8x8x2 for each cell of outputs. 
For this, is there built-in function or do I need to make it? 
If I have to implement it myself, how can I check the process that is correct? 
It's quite confusing for me because the tensor element is hard to debug.

Please help me.
Thanks.
",3,3
84,2016-7-4,2016,7,4,22,4r75xk,"Sportsbetting using LSTM, looking for closed beta testers",https://www.reddit.com/r/MachineLearning/comments/4r75xk/sportsbetting_using_lstm_looking_for_closed_beta/,Vogt_AI,1467638208,"Hi all,

I'm currently developing a sportsbetting website using lstm networks. I bet on 5 different bookmakers myself and turned a profit on all of them.

Here is the performance of my own betting so far:
http://imgur.com/90zoojn

You get at least 2 weeks free trial in the closed beta. (probably more, since I'm busy with my Master's thesis on LSTM)
Only Email address needed for registration, nothing more.
Reporting of bugs would be nice, but not a must.

Leave a comment or PM me if interested or if you have questions. Thanks!",1,1
85,2016-7-4,2016,7,4,22,4r7bak,"Characterizations of Learnability for Classes of {0, ..., n}-valued functions",https://www.reddit.com/r/MachineLearning/comments/4r7bak/characterizations_of_learnability_for_classes_of/,jinpanZe,1467640620,,1,4
86,2016-7-4,2016,7,4,23,4r7eng,Wave Computing working on Deep Learning Chip!,https://www.reddit.com/r/MachineLearning/comments/4r7eng/wave_computing_working_on_deep_learning_chip/,j_lyf,1467642026,,3,0
87,2016-7-5,2016,7,5,0,4r7rl2,[1607.00036] Dynamic Neural Turing Machine with Soft and Hard Addressing Schemes,https://www.reddit.com/r/MachineLearning/comments/4r7rl2/160700036_dynamic_neural_turing_machine_with_soft/,cesarsalgado,1467647111,,1,17
88,2016-7-5,2016,7,5,0,4r7s4o,Massive study of chess games reveals how and why humans make mistakes,https://www.reddit.com/r/MachineLearning/comments/4r7s4o/massive_study_of_chess_games_reveals_how_and_why/,_joermungandr_,1467647304,,19,152
89,2016-7-5,2016,7,5,0,4r7spb,How to plot change in covariance over time?,https://www.reddit.com/r/MachineLearning/comments/4r7spb/how_to_plot_change_in_covariance_over_time/,soulslicer0,1467647489,whats the best way to do it. Its a 6x6 matrix. should i just plot the diagonal as 1d graphs each?,3,3
90,2016-7-5,2016,7,5,1,4r81u2,How do I find out if a feature is statiatically significant ?,https://www.reddit.com/r/MachineLearning/comments/4r81u2/how_do_i_find_out_if_a_feature_is_statiatically/,ckiesbkies,1467650763,"My background is in econometrics. A lot of time is spent adjusting your model so that your variables actually contribute to the fit of your model.

Like, if you run a linear regression in stata or r, you get various indicators such as p values, adjusted r square and so on.

How does this translate into significance testing in machine learning methods such as random forest, and so on. Like, I can add 10000 features but I'd probably be overfitting, is there a way to objectively determine whether you're overfitting or not?",7,0
91,2016-7-5,2016,7,5,1,4r822d,Derivative of softmax loss function,https://www.reddit.com/r/MachineLearning/comments/4r822d/derivative_of_softmax_loss_function/,[deleted],1467650852,[deleted],2,0
92,2016-7-5,2016,7,5,1,4r823p,"A web app I made to come up with rhymes like ""gymnastic elastic"" when you input the word ""trampoline."" Try it out!",https://www.reddit.com/r/MachineLearning/comments/4r823p/a_web_app_i_made_to_come_up_with_rhymes_like/,summerstay,1467650863,,41,64
93,2016-7-5,2016,7,5,2,4r84zl,Learning in Brains and Machines (3): Synergistic and Modular Action,https://www.reddit.com/r/MachineLearning/comments/4r84zl/learning_in_brains_and_machines_3_synergistic_and/,skrza,1467651886,,0,8
94,2016-7-5,2016,7,5,3,4r8hpb,Identifying recurring charges given transactions,https://www.reddit.com/r/MachineLearning/comments/4r8hpb/identifying_recurring_charges_given_transactions/,ownallogist,1467656324,"Hi,

Thanks in advance for your time. I have a personal problem and would appreciate some insight.

I have transaction data from my banks/credit cards and would like to identify recurring charges (typically bills). Is there a smart, machine-learning based approach for this? Or is it more of a table lookup with set conditions. 

Bills can occur more than once a month, at different intervals (think 30, 60, 90 days), and aren't always the same amount. However, the merchant name does not change.

Thoughts? Thanks again! ",1,0
95,2016-7-5,2016,7,5,4,4r8ryz,"Deep Learning for Chatbots, Part 2  Implementing a Retrieval-Based Model in Tensorflow",https://www.reddit.com/r/MachineLearning/comments/4r8ryz/deep_learning_for_chatbots_part_2_implementing_a/,pogopuschel_,1467659859,,0,28
96,2016-7-5,2016,7,5,4,4r8urw,Data mining/machine learning in the face of irrevocable choices,https://www.reddit.com/r/MachineLearning/comments/4r8urw/data_miningmachine_learning_in_the_face_of/,eamonnkeogh,1467660842,"
Dear colleagues.

Forgive the self promotional nature of this post. But I wanted to make the community aware of an interesting new set of problems. How would you modify your favorite algorithm, if some of the choices you made were irrevocable? 

For example, writing the size of a mosquito to a memory location can be done an infinite number of times, but killing a mosquito with a laser can be done only once, we cannot bring the insect back to life.

In a new paper we consider one such problem, irrevocable sampling from a stream. The project was inspired by the need to capture mosquitoes (see http://research.microsoft.com/en-us/um/redmond/projects/projectpremonition/default.aspx  ) but is more general than just mosquitos.

We would welcome feedback.
Eamonn

[a] http://www.cs.ucr.edu/~eamonn/Irrevocable_choice.pdf
",0,0
97,2016-7-5,2016,7,5,5,4r92iq,Would it be possible to train a NN to remove echo from an audio source?,https://www.reddit.com/r/MachineLearning/comments/4r92iq/would_it_be_possible_to_train_a_nn_to_remove_echo/,2Punx2Furious,1467663582,"By giving it samples of clean audio and the same audio with echo added, have it figure out the differences, and remove them maybe?

Then, by giving it a new audio source with some echo that wasn't present in the training data, have it edited so that the echo is removed?",17,2
98,2016-7-5,2016,7,5,6,4r9f3h,Power to the People: How One Unknown Group of Researchers Holds the Key to Using AI to Solve Real,https://www.reddit.com/r/MachineLearning/comments/4r9f3h/power_to_the_people_how_one_unknown_group_of/,_joermungandr_,1467668307,,2,0
99,2016-7-5,2016,7,5,6,4r9fue,Using trained topic model to predict held-out document probabilities,https://www.reddit.com/r/MachineLearning/comments/4r9fue/using_trained_topic_model_to_predict_heldout/,pg108,1467668612,"I have trained a Relational Topic Model on a corpus, and have the distributions of topics in the training corpus as well as the words associated with the learned topics.

How do I use this to predict the topic distributions on a held-out/unseen document? The library I am using (https://cran.r-project.org/web/packages/lda/lda.pdf) does not appear to have a way to do this.

I have read about different LDA evaluation methods, such as Chibs and Left-to-Right evaluation methods. Are these applicable here? Are there better ways to do this prediction than write another implementation of these?

Thank you!",0,1
100,2016-7-5,2016,7,5,7,4r9np1,Dealing with models with parameters larger than GPU shared memory,https://www.reddit.com/r/MachineLearning/comments/4r9np1/dealing_with_models_with_parameters_larger_than/,Powlerbare,1467671848,"Are there good single gpu solutions to this problem that are included as part of some framework?

EDIT: I meant global memory in the title :(",15,4
101,2016-7-5,2016,7,5,8,4r9r4m,New free audio dataset for spoken digits.,https://www.reddit.com/r/MachineLearning/comments/4r9r4m/new_free_audio_dataset_for_spoken_digits/,Jakobovski,1467673282,,5,6
102,2016-7-5,2016,7,5,8,4r9uck,PC build for Deep Learning?,https://www.reddit.com/r/MachineLearning/comments/4r9uck/pc_build_for_deep_learning/,Gear5th,1467674634,"Absolute beginner here. After going through [*cs231n: CNNs for Visual Recognition*](http://cs231n.stanford.edu/syllabus.html), it became clear that I will need a good system to carry out experiements with Large ConvNets.

**Budget: 2,200 to 2,900 USD (&lt; 2 Lakh Rs)**  
Note: I'm in India, so electronics is more expensive :(

[[this article]](http://timdettmers.com/2015/03/09/deep-learning-hardware-guide/) seems to suggest a GTX TITAN X is a must for CNNs if it fits the budget. It is kinda old though, and I'm wondering if any GPUs better suited to ML have been released.

Would a GTX 1080 work better?

Could someone kindly give me necessary pointers, or provide me with resources to read for this?

Thanks :)",17,2
103,2016-7-5,2016,7,5,9,4ra3eq,"Simple neural net implemented ""from scratch"" and equivalent TensorFlow version (Exercise)",https://www.reddit.com/r/MachineLearning/comments/4ra3eq/simple_neural_net_implemented_from_scratch_and/,wickedman,1467678484,,4,0
104,2016-7-5,2016,7,5,12,4rapu9,[Question] Intuition behind using tensors ?,https://www.reddit.com/r/MachineLearning/comments/4rapu9/question_intuition_behind_using_tensors/,__smurf__,1467688672,"I've noticed lately that a lot of people are developing tensor equivalents of many methods (tensor factorization, tensor kernels, tensors for topic modeling, etc), so I started learning how to use tensors to build models, my first self-taught exercise was building a recommendation model by using a tensor. In order to recommend an article to a user, I have built a model to predict users' preferable articles.

I am labeling articles based on topics, the result of my learning dataset is a 3-dimensions dataset (USers (X) - Topics (Y) - Time (Z) ), because user's interest might evolve over time, at the same time it doesn't change much.

In order to build predictive models, I usually deal with 2-Dimensions data X,Y and predict based on training/Testing. Now, I am trying to lean how to use the different techniques that might help solving similar problems (like Matrix Factorizations and tensors, Tensor Decompositio, convex tensor completion methods for tensors )

I am little bit lost in literature, what are some good resources (courses/tutorials) to learn more on how to use tensors and related techniques to model similar above-mentioned problems ?",2,4
105,2016-7-5,2016,7,5,12,4ratzt,The principle of dispersion power,https://www.reddit.com/r/MachineLearning/comments/4ratzt/the_principle_of_dispersion_power/,mixmachinery,1467690502,,1,1
106,2016-7-5,2016,7,5,13,4rb2ru,DMTK Machine Learning Toolkit Dari Microsoft,https://www.reddit.com/r/MachineLearning/comments/4rb2ru/dmtk_machine_learning_toolkit_dari_microsoft/,cristmartin,1467694716,,0,1
107,2016-7-5,2016,7,5,15,4rba35,Unsupervised Learning of 3D Structure from Images - DeepMind,https://www.reddit.com/r/MachineLearning/comments/4rba35/unsupervised_learning_of_3d_structure_from_images/,gwulfs,1467698413,,24,124
108,2016-7-5,2016,7,5,15,4rbgdd,The Business Implications of Machine Learning,https://www.reddit.com/r/MachineLearning/comments/4rbgdd/the_business_implications_of_machine_learning/,[deleted],1467701733,[deleted],0,1
109,2016-7-5,2016,7,5,19,4rbzd2,Please Help! problems with flicker training DNN,https://www.reddit.com/r/MachineLearning/comments/4rbzd2/please_help_problems_with_flicker_training_dnn/,Hidden_dreamz,1467713135,"Hi there!

 I am currently trying to flicker train GoogleLeNet on 400 of my own images using my SLI 780tis. But i keep getting errors. such as cannot find file -&gt; dir to file location(one of the images im training it on) but the file is there and the correct dir is in the train file. do you have any idea why this would be? also in the guide i followed to do this the guy had 4gb vram and used batch of 40 with 256256 images i did the same but with batch size of 30 to account for the 3gb vram. am i doing something wrong here? how can i optimise the training to work on my video card? i appreciate any help you can give! thanks Josh!",0,0
110,2016-7-5,2016,7,5,19,4rbztt,Neural Networs in MySQL,https://www.reddit.com/r/MachineLearning/comments/4rbztt/neural_networs_in_mysql/,yurii-shevchuk,1467713444,,2,1
111,2016-7-5,2016,7,5,19,4rc4oa,How to get access to the WSJ part of the Penn Treebank?,https://www.reddit.com/r/MachineLearning/comments/4rc4oa/how_to_get_access_to_the_wsj_part_of_the_penn/,bbsome,1467716356,"I looked online and did not manage to find anywhere description of how you can gain access to the Penn Treebank. The website https://www.cis.upenn.edu/~treebank/ does not specify it, so has anyone got any idea? 

PS: On the LDC website https://catalog.ldc.upenn.edu/LDC99T42 there is nothing under Download or View fees etc.",3,1
112,2016-7-5,2016,7,5,20,4rc5gm,mlpack: a scalable C++ machine learning library 2.02 released,https://www.reddit.com/r/MachineLearning/comments/4rc5gm/mlpack_a_scalable_c_machine_learning_library_202/,[deleted],1467716803,[deleted],0,4
113,2016-7-5,2016,7,5,20,4rc758,Scikit learn compare classification versus regression model,https://www.reddit.com/r/MachineLearning/comments/4rc758/scikit_learn_compare_classification_versus/,[deleted],1467717739,[deleted],0,0
114,2016-7-5,2016,7,5,22,4rcm63,Would You Survive the Titanic? A Guide to Machine Learning in Python,https://www.reddit.com/r/MachineLearning/comments/4rcm63/would_you_survive_the_titanic_a_guide_to_machine/,vortex_ape,1467724534,,0,4
115,2016-7-5,2016,7,5,22,4rcmlk,"So many things to learn, so little time! (Textbook advice/discussion sought)",https://www.reddit.com/r/MachineLearning/comments/4rcmlk/so_many_things_to_learn_so_little_time_textbook/,[deleted],1467724728,[removed],0,1
116,2016-7-5,2016,7,5,22,4rcrsi,Paul Mineiro ICML 2016 Thoughts,https://www.reddit.com/r/MachineLearning/comments/4rcrsi/paul_mineiro_icml_2016_thoughts/,gwulfs,1467726764,,3,23
117,2016-7-5,2016,7,5,22,4rcsb7,Learning for competition,https://www.reddit.com/r/MachineLearning/comments/4rcsb7/learning_for_competition/,Glatomme,1467726962,"hi,

I was just wondering something and if it would be possible to archieve the following idea.

In the competition of badminton, you have 8 matches (4 singles, 4 doubles).

Where you have 4 or more players, in each player can play max 1 single and 2 doubles
Each player has a ranking, score and place on overal leader-boards.

Given the following info:

&gt; The ranking is as following, from low to high. A being the best: D, C2, C1, B1, B2, A
&gt;
&gt; The order of the players for the singles, a player on a place must have a ranking higher or equal then the one below it.
&gt;
&gt; The order of the players for the doubles, the sum of the players ranking on a place must have a ranking higher or equal then the one below it.

and

&gt; Every weekend there are tournaments and competitions where a lot of players play against others (obviously :p ). So it's possible that player A has the play against Player B, He hasn't played against him yet, but Player B has lost against Player C, and Player A has won against player C. so probably Player A would win against B.

would it be possible to analyze the matches to:

* A: predict the players from the opposite team that they would  bring on a competition (given a list of all players of that team) 
* B: predict the best possible setup for the home team (if most players have the same ranking, or are one higher)


if so, how would you start preparing for this to let a machine learn / suggest this?
",0,0
118,2016-7-5,2016,7,5,23,4rcvby,What Deep Learning Can Offer to Businesses,https://www.reddit.com/r/MachineLearning/comments/4rcvby/what_deep_learning_can_offer_to_businesses/,victor_haydin,1467728111,,0,1
119,2016-7-5,2016,7,5,23,4rcxvu,Bridging PyMC3 and Lasagne to build a Hierarchical Neural Network,https://www.reddit.com/r/MachineLearning/comments/4rcxvu/bridging_pymc3_and_lasagne_to_build_a/,gwulfs,1467729046,,0,10
120,2016-7-6,2016,7,6,0,4rd8et,How to handle one ML Model by User ?,https://www.reddit.com/r/MachineLearning/comments/4rd8et/how_to_handle_one_ml_model_by_user/,[deleted],1467732659,[deleted],0,0
121,2016-7-6,2016,7,6,1,4rde9p,Textbook advice/discussion sought,https://www.reddit.com/r/MachineLearning/comments/4rde9p/textbook_advicediscussion_sought/,MidoriMind,1467734570,"I just finished [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/) all chapters, exercises, etc., and am looking for a new resource to continue my ML education. 

ISL was a very concise introduction to the topic, and I feel like I took away a good amount concerning the more common types of statistical learning tools/procedures. However, I feel like it was more Data Mining, rather than Learning.  A feeling for the latter is what I am mainly interested in, if that makes any sense. 

Perhaps this is a misunderstanding on my part, and I should just go ahead and read through Russel and Norvig's [Artificial Intelligence: A Modern Approach](https://www.amazon.com/Artificial-Intelligence-Modern-Approach-3rd/dp/0136042597), but I feel like there has to be some connection with traditional ML techniques, and the types of software that gets implemented in Autonomous systems (Not talking about vision here, that's a different story). However, I just can't seem to find the connection linking the information from books such as ISL, to these more complex systems.

ISL was rather shallow on both the mathematical and applied ends, and I'm hoping to find a resource that will fulfil both of these requirements. Further, ISL only briefly discussed Unsupervised techniques, which is more or less exactly what I think I'm interested in, and they completely ignore ANNs/Deep Learning. While I certainly want to understand the underlying theory, I find it hard to keep up motivation sometimes if I can't immediately tinker around with things. Also, I vastly prefer textbooks over anything with video, since I can pace myself better.

I've scoured all over for people's recommendations (Wiki/FAQ, University course pages, Quora, StackExchange), but naturally there's no unanimous consensus over a golden pathway. I've been able to find most of these in my library, and was able to get a peek at them. Below is what I have found:

***Textbooks:***

Hastie, Tibshirani and Friedman, [The Elements of Statistical Learning](http://statweb.stanford.edu/~tibs/ElemStatLearn)
Everyone recommends this book, and I can see why after reading ISL. I enjoyed ISL's treatment of the topic and I definitely wouldn't mind expanding upon this. However, as I said above, it felt mostly like Data Mining to me, and I'd rather not read 700+ more pages without feeling like I'm coming closer to getting a machine to learn something.

Kevin Murphy, [Machine Learning: A Probabilistic Perspective](https://www.amazon.com/Machine-Learning-Probabilistic-Perspective-Computation/dp/0262018020)
People seem to love/hate this book. Looking through it, it seems very broad and self-contained, doesn't shy away from the math, and attempts to offer applications for topics. Wonderful. However,  there seems to be no clear linear progression through the book, and almost countless errata. Both of these are fairly off-putting.

Christopher Bishop, [Pattern Recognition and Machine Learning](https://www.amazon.com/dp/0387310738/ref=pd_lpo_sbs_dp_ss_1?pf_rd_p=1944687542&amp;pf_rd_s=lpo-top-stripe-1&amp;pf_rd_t=201&amp;pf_rd_i=0262018020&amp;pf_rd_m=ATVPDKIKX0DER&amp;pf_rd_r=DC0QK9QCSDVD8XKMD2JE)
Seems to be the de-facto introduction to ML in almost every University. Definitely more linear and less error-prone than Murphy, but less breadth (probably for the best, honestly). Very theory oriented, which is perfectly fine, but it seems quite shallow on the application end. People have been telling me that it's slightly out of date already.

David Barber, [Bayesian Reasoning and Machine Learning](http://web4.cs.ucl.ac.uk/staff/D.Barber/pmwiki/pmwiki.php?n=Brml.Online)
Looks very much like Murphy's MLaPP in scope,is up to date, and has a clear progression to it..... But everyone seems to glance over this. If I saw more people speaking about this book, I'd probably just pick it up immediately. Is there a reason why folks never mention it? 

Goodfellow, Bengio, Courville, [Deep Learning](http://www.deeplearningbook.org/)
Apparently the most up-to-date book on Deep Learning, but there are no exercises! Still worth doing over a course like Hinton's or Karpathy's?

Tom Mitchell, [Machine Learning](http://www.cs.cmu.edu/~tom/mlbook.html)
Almost perfect, and were I looking to be introduced to ML nearly two decades ago, I wouldn't have even hesitated at going through this book. However, it seems quite out of date, and people are recommending other books these days. I am aware of his online class, but the slides aren't that great, and I don't enjoy videos much.

Duda, Hart, Stork, [Pattern Classification](https://www.amazon.com/Pattern-Classification-Pt-1-Richard-Duda/dp/0471056693/ref=sr_1_2?s=books&amp;ie=UTF8&amp;qid=1467720749&amp;sr=1-2&amp;keywords=pattern+classification)
I enjoy the layout of this one, and it looks like it's a very application-focused text, but nobody seems to mention it except as a supplement to Bishop's book. Its age is concerning, and perhaps this is why nobody recommends it anymore. Still worth going over?

Marsland, [Machine Learning: an Algorithmic Perspective](https://www.amazon.com/Machine-Learning-Algorithmic-Perspective-Recognition/dp/1420067184)
and 
Sebastian Raschka, [Python Machine Learning](https://www.amazon.com/Python-Machine-Learning-Sebastian-Raschka/dp/1783555130/ref=pd_sim_14_4?ie=UTF8&amp;dpID=51ONvTIz0iL&amp;dpSrc=sims&amp;preST=_AC_UL320_SR260%2C320_&amp;psc=1&amp;refRID=X3J5RBYR9M8MN5WS2WDV)
and
[Programming Collective Intelligence](https://www.amazon.com/Programming-Collective-Intelligence-Building-Applications/dp/0596529325?ie=UTF8&amp;qid=1305451320&amp;ref_=sr_1_1&amp;sr=8-1#reader_0596529325)
All of these seem very  cook-book  oriented, with a heavy focus on application. Raschka's seems to introduce most of the theory incredibly well, given what it is, but I'm not sure how deep a text like this can possibly go. Highly recommended on Amazon, but I never know how to take that information...

There's also MacKay, which people seem to love for various reasons, although the subject matter seems ever so slightly out of place. If I had more free time I'd work through it, since I always wanted an intro to information theory. Also,  Shwartz's Understanding Machine Learning:
 From Theory to Algorithms, which nobody ever mentions

***Courses:***

[Stanford's Computer Vision CS231n](cs231n.github.io)
I'm doing a PhD in a field of Physics where this can be almost immediately applied, without any hesitation (But I'm trying to get out of physics in favor of ML/AI ;D). Also, this is a huge field on it's own, the class has wonderful notes and assignments, and this would give me a great overview of modern ANNs, which were ignored in ISL. However, I'm unsure if I'm interested in vision per-se...

Hinton's Coursera MOOC on ANNs (Downloaded before deletion)
Not a fan of videos, but this is held in very high regard amongst almost everybody. Would this still be worthwhile going over, or would CS231 cover most of what is discussed in this series?

Hugo Larochelle's [course on ANNs](http://info.usherbrooke.ca/hlarochelle/neural_networks/content.html)
People are saying that this should replace Hinton's class on ANNs, but it seems a bit scattered, and there's only a few assignments.

Koeller's Coursera MOOC on Probabilistic Graphical Models (Downloaded)
Held in high regard, as with Hinton's, but honestly, I'd rather learn this stuff from a textbook if possible.

Stanford's [Deep Learning Tutorial](http://deeplearning.stanford.edu/tutorial)
This looks like it would be pretty good, were it about ten times its current size...

**TL;DR:**
I've looked through about a billion web pages looking for a new learning resource after finishing ISL. I'd like something that introduces the underlying theory, doesn't lose sight of applications, and is not that out of date. Also, I'm not fond of feeling like I'm just doing Data Mining (Sorry I keep putting it like this, and I don't want to offend anyone by it. I just don't know how else to phrase it...). Limited to about 3 or 4 hours a day this summer to work on this, so one resource is preferred, or two if they're both around 400/500 pages each.
",17,16
122,2016-7-6,2016,7,6,1,4rdmpu,"Robots Need ""Common Sense"" A.I. to Work in Our Uncertain World",https://www.reddit.com/r/MachineLearning/comments/4rdmpu/robots_need_common_sense_ai_to_work_in_our/,reworksophie,1467737409,,0,1
123,2016-7-6,2016,7,6,2,4rdulk,Building a Twitter bot with PHP and Machine Learning,https://www.reddit.com/r/MachineLearning/comments/4rdulk/building_a_twitter_bot_with_php_and_machine/,wildcodegowrong,1467740021,,0,0
124,2016-7-6,2016,7,6,3,4re69c,Three Impactful Machine Learning Topics at ICML 2016,https://www.reddit.com/r/MachineLearning/comments/4re69c/three_impactful_machine_learning_topics_at_icml/,Init_ai,1467743974,,0,25
125,2016-7-6,2016,7,6,4,4rebdo,The DataFrame is Apache Spark's new central API. Is mapreduce (MR) programming with RDDs an antique style now? Or do you expect MR to still prove useful in some apps?,https://www.reddit.com/r/MachineLearning/comments/4rebdo/the_dataframe_is_apache_sparks_new_central_api_is/,datasciguy-aaay,1467745658,,1,1
126,2016-7-6,2016,7,6,4,4reds9,Two Minute Papers - Image Colorization With Deep Learning and Classification,https://www.reddit.com/r/MachineLearning/comments/4reds9/two_minute_papers_image_colorization_with_deep/,nagasgura,1467746493,,2,12
127,2016-7-6,2016,7,6,4,4regq8,Googles DeepMind AI to use 1 million NHS eye scans to spot diseases earlier,https://www.reddit.com/r/MachineLearning/comments/4regq8/googles_deepmind_ai_to_use_1_million_nhs_eye/,dharma-1,1467747461,,59,211
128,2016-7-6,2016,7,6,5,4renb8,AI: Awkwardly Intelligent - A comedy discussing the possible future of artificial intelligence.,https://www.reddit.com/r/MachineLearning/comments/4renb8/ai_awkwardly_intelligent_a_comedy_discussing_the/,the_next_seth,1467749663,,1,0
129,2016-7-6,2016,7,6,5,4reril,Coursera ML Specialization or Udacity Machine Learning Engineer Nanodegree ?,https://www.reddit.com/r/MachineLearning/comments/4reril/coursera_ml_specialization_or_udacity_machine/,yeyemelocotom,1467751095,"Hi, guys! Which one should I choose: Coursera ML Specialization or Udacity Machine Learning Engineer Nanodegree?",13,8
130,2016-7-6,2016,7,6,7,4rfbfl,Model development and experimental workflow?,https://www.reddit.com/r/MachineLearning/comments/4rfbfl/model_development_and_experimental_workflow/,bionerd2,1467758276,"Hi guys,

I was wondering if you guys would be willing to share your workflows in developing models, once you have designed an architecture/modified an algorithm to use for your research. For most of my stuff [read: the stuff that can be done in Python], I use Jupyter notebooks for development and at some point (when I get decent results/know model is working) migrate to python scripts (.py files) for final software development and usually some sort of shell wrapper script. Is that what most of you do, or are there other recommendations out there?",2,1
131,2016-7-6,2016,7,6,7,4rfe4h,For Well Thou Know'st: Live stream of building a recurrent font classification model,https://www.reddit.com/r/MachineLearning/comments/4rfe4h/for_well_thou_knowst_live_stream_of_building_a/,vanboxel,1467759285,,0,0
132,2016-7-6,2016,7,6,7,4rfegq,"$2,250,000 Algorithmic Trading Contest - Use Machine Learning in MATLAB or Python",https://www.reddit.com/r/MachineLearning/comments/4rfegq/2250000_algorithmic_trading_contest_use_machine/,denb95,1467759402,"Wanted to share the Quantiacs platform for anyone interested in applying machine learning to financial data. We have a free open-source toolbox that lets you develop and backtest in Python and MATLAB with support for libraries like SciPy and TensorFlow. 

We've found that the most talented quants on our platform have no background in finance and use machine learning techniques. Check out this Stanford research paper that used our platform: [Algorithmic Trading of Futures via Machine Learning](http://cs229.stanford.edu/proj2014/David%20Montague,%20Algorithmic%20Trading%20of%20Futures%20via%20Machine%20Learning.pdf)

If you want to get some background knowledge in before the competition begins, check out our aggregated [reading list](http://www.quantiacs.com/Data/Quantiacs%20Reading%20List.pdf) of books, videos, courses, and trading forums.

Once youre ready to make money as a quant, you can join the latest Quantiacs automated trading contest, with a total of $2,250,000 in investments available: [Can you compete with the best quants?](https://quantiacs.com/Competitions/Open-Q6-Competition.aspx)",5,0
133,2016-7-6,2016,7,6,7,4rfesq,What is the hardest part of data for machine learning?,https://www.reddit.com/r/MachineLearning/comments/4rfesq/what_is_the_hardest_part_of_data_for_machine/,n00bzor,1467759527,"I've been thinking about this problem in regards to machine learning: Is it hard for people to build data sets? Are the open source data sets that are out there good enough? And in general how are people building these datasets? (all in regards to computer vision)

I feel like the open source image datasets that are labeled, aren't good enough to use in some kind of business. If you want to go into the machine learning/object recognition business, you need really high quality datasets. I'm just wondering if this is a good assumption to make or not. ",3,1
134,2016-7-6,2016,7,6,8,4rffi3,Abstention. We need a learning algorithm to identify its own actual areas of undertraining.,https://www.reddit.com/r/MachineLearning/comments/4rffi3/abstention_we_need_a_learning_algorithm_to/,datasciguy-aaay,1467759790,"https://www.youtube.com/watch?v=FU6T6EAEG0s&amp;index=42&amp;list=PLNtMya54qvOH6YAVFigzoXb4iIzl0cvgd

@22m48s: Abstentions: Sometimes a classifier should not make a prediction; instead it should say I dont know for example when the query feature vector is far away from the training set features.

This is an open research area.

Source:  Professor Robert Tibshirani, 2015.

Today, learning algorithms cannot even do abstention correctly.  Even if a learning algorithm can eventually be formulated to identify its own actual areas of undertraining, can it then even notify the human to take over control quickly enough, given the time it takes for a human to go from one train of thought to a completely different train of thought?

Consider a self-driving car scenario. There was a car crash in Florida which happened some months ago but only became reported a few days ago. The apparent misprediction failure was saddening because a human life was lost.

Consider a cancer surgery scenario as in the video of Prof. Tibshirani.

What will happen when the ML prediction that the fitted model produces is a high probability classification, but it's actually wrong. The professor's model didn't know that it should have abstained from making the prediction in the first place.

Second, supposing even if we eventually get our ML algorithms to abstain, how can we hand back control to the human in these cases quickly enough -- will the human even be ready to take back control quickly enough?

Let's say our best accuracy is only ever going to asymptote at 99.999% (five nines) because we cannot get all the training data that is needed.  If that's the case then are self-driving cars actually an impossibility due to human limitations for taking back the controls fast enough?",11,2
135,2016-7-6,2016,7,6,9,4rfws2,"What are the best lecture series, films, shows, interviews, etc. to watch and learn more about Machine Learning from?",https://www.reddit.com/r/MachineLearning/comments/4rfws2/what_are_the_best_lecture_series_films_shows/,apmoney,1467766179,,3,0
136,2016-7-6,2016,7,6,10,4rg63l,Distribution prediction problem,https://www.reddit.com/r/MachineLearning/comments/4rg63l/distribution_prediction_problem/,AceOfBase32,1467769634,"I'm currently looking for techniques to solve the following problem:
I'm given a set of systems that behave similarly, each with the same number of input variables and each with a single output variable. I need to predict the probability distribution of the output variables, which will change depending on the input variables and will also have covariance with the outputs of the other systems. Training data is available, so a supervised learning method could be used.

My first idea was to try to use something such as a neural network to simply predict the average output value. Then, I could measure the variance and covariance for particular systems and just fit a Gaussian distribution. Does anyone have thoughts on methods that could be more accurate? ",1,0
137,2016-7-6,2016,7,6,10,4rg6b4,LSTM Encoder Decoder for Multi-variate Anomaly Detection,https://www.reddit.com/r/MachineLearning/comments/4rg6b4/lstm_encoder_decoder_for_multivariate_anomaly/,Jxieeducation,1467769716,,0,2
138,2016-7-6,2016,7,6,12,4rgnah,Best Computing Option for Tensorflow Neural Nets,https://www.reddit.com/r/MachineLearning/comments/4rgnah/best_computing_option_for_tensorflow_neural_nets/,danielcanadia,1467777065,I create various neural network models with Tensorflow. I need something to run the models on in a cost efficient way. I tried Google Cloud Computing but I feel that my models still train at a snail's pace. Any suggestions? Not use Tensorflow? Another web service? Get a physical computer?,9,0
139,2016-7-6,2016,7,6,13,4rgrla,Actor/Critic RL with OpenAI Gym and Keras,https://www.reddit.com/r/MachineLearning/comments/4rgrla/actorcritic_rl_with_openai_gym_and_keras/,gregatragenet,1467779058,,3,38
140,2016-7-6,2016,7,6,13,4rguhu,Question about what supervised learning approach to use.,https://www.reddit.com/r/MachineLearning/comments/4rguhu/question_about_what_supervised_learning_approach/,[deleted],1467780416,[deleted],2,0
141,2016-7-6,2016,7,6,15,4rh5ae,Tech News Today United State New Military Technology,https://www.reddit.com/r/MachineLearning/comments/4rh5ae/tech_news_today_united_state_new_military/,besstreetfightvideos,1467785792,,0,1
142,2016-7-6,2016,7,6,15,4rh8t8,How Machine Learning Amplifies Inequality in Society,https://www.reddit.com/r/MachineLearning/comments/4rh8t8/how_machine_learning_amplifies_inequality_in/,icelem,1467787689,,18,13
143,2016-7-6,2016,7,6,15,4rh94z,Exploiting machine learning in cybersecurity,https://www.reddit.com/r/MachineLearning/comments/4rh94z/exploiting_machine_learning_in_cybersecurity/,galapag0,1467787863,,0,0
144,2016-7-6,2016,7,6,16,4rhd9u,Screw Air Compressor last test before send out.---AEOMACHINE AEOM-16061...,https://www.reddit.com/r/MachineLearning/comments/4rhd9u/screw_air_compressor_last_test_before_send/,AEOMACHINE,1467790275,,1,0
145,2016-7-6,2016,7,6,17,4rhk6r,TensorFlow question - softmax with loss,https://www.reddit.com/r/MachineLearning/comments/4rhk6r/tensorflow_question_softmax_with_loss/,snapillar,1467794492,"Hi all,
In Caffe, there is a layer named 'softmax_loss_layer' which can be used for spatial comparison between label mask and predicted map such as segmentation tasks. 
For example, if the label size is 1x120x160 with label 0 or 1 for each pixel, and the predicted one is 2x120x160, the loss is summation of negative logarithm of each label (0 for first channel, 1 for second channel in this case..). 
How can I implement this in tensorflow?
Is there softmax function similar to this?

Thank you.",1,2
146,2016-7-6,2016,7,6,17,4rhl6o,Neural networks - Increasing the learning rate on loss function saturation,https://www.reddit.com/r/MachineLearning/comments/4rhl6o/neural_networks_increasing_the_learning_rate_on/,BananaCode,1467795105,"

I'm currently reading about neural networks, specifically how loss functions saturation can cause problems. During my studies, I was curious if one could remedy the problem during training of neural networks by detecting saturation (such as comparing the gradients to the previous training iteration) and just increase the learning rate in a proportional manner so that learning still can be done effectively. Unfortunately, my (admittately limited) research on google has not come up with any method using this approach.

Is there a reason why this is not being done?
",2,0
147,2016-7-6,2016,7,6,19,4rhwr3,Is Gaussian Process useful in context large-scale machine learning?,https://www.reddit.com/r/MachineLearning/comments/4rhwr3/is_gaussian_process_useful_in_context_largescale/,huyhcmut,1467801807,Yoshua bengio said on quora that Gaussian Process is just useful for small dataset about(10002000). Is there any great-steps to improve this?,5,2
148,2016-7-6,2016,7,6,19,4rhxld,A neural network language modeling package developed on top of Theano,https://www.reddit.com/r/MachineLearning/comments/4rhxld/a_neural_network_language_modeling_package/,senarvi,1467802305,,2,1
149,2016-7-6,2016,7,6,20,4ri4fw,The challenges with word embddings,https://www.reddit.com/r/MachineLearning/comments/4ri4fw/the_challenges_with_word_embddings/,MikeWally,1467806122,,7,15
150,2016-7-6,2016,7,6,21,4riaus,List of deep learning startups,https://www.reddit.com/r/MachineLearning/comments/4riaus/list_of_deep_learning_startups/,JohnnyCantRemember,1467809275,Where can I find a list of a deep learning/ML startups that are hiring?,4,0
151,2016-7-6,2016,7,6,21,4rib0a,Group Sparse Regularization for Deep Neural Networks,https://www.reddit.com/r/MachineLearning/comments/4rib0a/group_sparse_regularization_for_deep_neural/,scardax88,1467809345,,0,11
152,2016-7-6,2016,7,6,22,4rifpv,HCC backend Implementation for Torch7,https://www.reddit.com/r/MachineLearning/comments/4rifpv/hcc_backend_implementation_for_torch7/,guardianhelm,1467811373,,3,2
153,2016-7-6,2016,7,6,22,4rih6g,Intuition of 1X1 convolutions as final layer of a convnet instead of FC (and possible pseudo/theano code) please ?,https://www.reddit.com/r/MachineLearning/comments/4rih6g/intuition_of_1x1_convolutions_as_final_layer_of_a/,muktabh,1467811928,"Hi,
I am not able to understand how a 1X1 convolution can replace a FC Layer in the top layer of a ConvNet. Can anyone please help ?",13,24
154,2016-7-6,2016,7,6,22,4rikw8,Who consistently uses batch normalization?,https://www.reddit.com/r/MachineLearning/comments/4rikw8/who_consistently_uses_batch_normalization/,siblbombs,1467813402,"Batch normalization seems to be one of those tricks that's rather obvious in hindsight and works well, most CNN papers I'm reading are including it by default now, but it feels like a pain to actually use. 

My biggest issues with it (coming from a pure theano standpoint, no additional libraries) is that it's a pain to manage all the conditional computations everywhere to determine when to use the batch/population statistics, as well as the additional overhead for calculating the batch statistics can start to add up (all though BN should still be net-positive in terms of wallclock time). Persisting all the population stats for each BN transform also makes the model more of a pain to save/load, but that could be more a problem of how I save the shared variables. 

Theano has a .bn function in the API now, but from looking at it it doesn't seem to handle either of my pain points from above. For people who are doing lots of stuff that BN should help accelerate, do you actually use it?",56,43
155,2016-7-6,2016,7,6,23,4rimdf,Quantifying and Reducing Stereotypes in Word Embeddings [1606.06121],https://www.reddit.com/r/MachineLearning/comments/4rimdf/quantifying_and_reducing_stereotypes_in_word/,pmigdal,1467813919,,3,5
156,2016-7-6,2016,7,6,23,4riosk,machine question,https://www.reddit.com/r/MachineLearning/comments/4riosk/machine_question/,[deleted],1467814781,[deleted],3,0
157,2016-7-7,2016,7,7,0,4rj2al,*[Survey] to study to how people use mobile food apps (to improve our new Machine learning integrated cross platform food app)*,https://www.reddit.com/r/MachineLearning/comments/4rj2al/survey_to_study_to_how_people_use_mobile_food/,pcp_7,1467819451,[removed],0,0
158,2016-7-7,2016,7,7,1,4rjciy,Google develops practical plan for addressing threat of unintended AI consequences,https://www.reddit.com/r/MachineLearning/comments/4rjciy/google_develops_practical_plan_for_addressing/,jonfla,1467822800,,0,0
159,2016-7-7,2016,7,7,1,4rjdy3,How important Rocstories dataset (a corpora of short stories with a test set) might be for AI-community?,https://www.reddit.com/r/MachineLearning/comments/4rjdy3/how_important_rocstories_dataset_a_corpora_of/,curiosity_monster,1467823268,"The dataset: http://cs.rochester.edu/nlp/rocstories/ (50K of short stories and a test set to them)
The paper: http://arxiv.org/pdf/1604.01696v1.pdf

When I found it, my first impression was that it's a next important benchmark for AI-researchers on par with Visual Genome and upcoming Labirynth of Deep Mind. It appeared months ago, but it didn't seem to get any reaction. At least no articles in arxiv based on it and no discussion here. 

Does it mean it's not so significant as it seems at a first glance? What do you think about it? ",6,1
160,2016-7-7,2016,7,7,1,4rjdyb,Is there a list of standard notation?,https://www.reddit.com/r/MachineLearning/comments/4rjdyb/is_there_a_list_of_standard_notation/,GuyHasNoUsername,1467823270,What symbol should I use for the net input function sum(wx+b)?,7,11
161,2016-7-7,2016,7,7,1,4rjfcp,Have there been any improvements on Batch Normalization?,https://www.reddit.com/r/MachineLearning/comments/4rjfcp/have_there_been_any_improvements_on_batch/,PM_ME_YOUR_GRADIENTS,1467823696,I tried to parse through all the citations the Szegedy paper has gotten but it's &gt;300 and most of them are _using_ BN as opposed to improving it. Any help?,10,16
162,2016-7-7,2016,7,7,2,4rjp7j,Question on learning rate reduction (CNN),https://www.reddit.com/r/MachineLearning/comments/4rjp7j/question_on_learning_rate_reduction_cnn/,[deleted],1467826574,[removed],0,1
163,2016-7-7,2016,7,7,3,4rjx5p,machinery question?,https://www.reddit.com/r/MachineLearning/comments/4rjx5p/machinery_question/,[deleted],1467828988,[deleted],3,1
164,2016-7-7,2016,7,7,3,4rjzbm,Study Material for picking up Machine Learning,https://www.reddit.com/r/MachineLearning/comments/4rjzbm/study_material_for_picking_up_machine_learning/,ancientmtk,1467829726,"Hi guys, I am a developer by trade and I am interested in getting into ML. I've started taking Andrew Ng's Coursera class, and while concepts aren't that difficult for me as I have taken linear algebra/calculus/basic stats in college, I feel I am moving too slowly and can't grasp the whole picture. I think the issue I have is that I can only watch the videos a little bit at a time and thus can't bring all my learning together into one piece.

I'm wondering, does anyone know of any good books/lecture notes that cover the basic concepts and not dive full fledge into the math theories?
",5,4
165,2016-7-7,2016,7,7,4,4rke9u,A Geometric Proof of the Perceptron Convergence Theorem,https://www.reddit.com/r/MachineLearning/comments/4rke9u/a_geometric_proof_of_the_perceptron_convergence/,tgyatso,1467834664,,0,2
166,2016-7-7,2016,7,7,5,4rkotr,Maximizing the norm of sum of vectors,https://www.reddit.com/r/MachineLearning/comments/4rkotr/maximizing_the_norm_of_sum_of_vectors/,wdlearn,1467838190,"Suppose we have a set of vectors and we want to find a subset of these vectors (hopefully with less than 'c' vectors, but this can be omitted if it can make things any easier) such that the resulting sum of these vectors have the largest norm. Does anyone know of a good way to tackle this problem? This looks like a NP-hard problem to me, but I wonder if anyone knows a relaxed optimization problem formulation or any heuristics that work well enough.

Thank you all!

  ",10,5
167,2016-7-7,2016,7,7,6,4rkr63,The Theorem Every Data Scientist Should Know (Cross-Post),https://www.reddit.com/r/MachineLearning/comments/4rkr63/the_theorem_every_data_scientist_should_know/,nickhould,1467838994,,2,0
168,2016-7-7,2016,7,7,6,4rksmg,"Working Implementation of F-GAN ""Variational Divergence"" Paper?",https://www.reddit.com/r/MachineLearning/comments/4rksmg/working_implementation_of_fgan_variational/,alexmlamb,1467839495,"Does anyone have an implementation, preferably in Theano?  ",0,3
169,2016-7-7,2016,7,7,7,4rl4u4,Learn to code Basic Dynamic BiDirectional LSTM with Tensorflow,https://www.reddit.com/r/MachineLearning/comments/4rl4u4/learn_to_code_basic_dynamic_bidirectional_lstm/,[deleted],1467843912,[deleted],0,0
170,2016-7-7,2016,7,7,8,4rlcjl,Learning to code Python with a LSTM (Lasagne),https://www.reddit.com/r/MachineLearning/comments/4rlcjl/learning_to_code_python_with_a_lstm_lasagne/,nano_72,1467846723,,15,58
171,2016-7-7,2016,7,7,8,4rlj6x,Trying to find some good papers on Image Segmentation?,https://www.reddit.com/r/MachineLearning/comments/4rlj6x/trying_to_find_some_good_papers_on_image/,transhumanist_,1467849140,"Hello everyone,

I am starting a little project that relies on a database consisting of a 1000s of 512 x 512 grayscale images + black and white (0s and 1s) ground truth masks (segmentations) of features of each grayscale image. 

I am trying to implement a CNN from scratch that uses this database to segment new images alike the grayscale ones, but I am unsure on how to do it. Would training the CNN by enlarging the dataset with small patches of the gray/bw images be a good start? I am thinking this could take the network capacity to ""understand"" what it is segmenting, so I thought too about just training it with it receiving the entire image and outputting the entire segmentation mask, but I am not so sure that would work so well. What are your thoughts?

Also, what are some good papers on this kind of segmentation? 

Cheers!",3,0
172,2016-7-7,2016,7,7,9,4rlp3s,Bootcamp worth it even if I already know basics of ML but can't get a job?,https://www.reddit.com/r/MachineLearning/comments/4rlp3s/bootcamp_worth_it_even_if_i_already_know_basics/,[deleted],1467851427,[deleted],7,1
173,2016-7-7,2016,7,7,12,4rmeql,Problems with You Only Look Once: Real-Time Object Detection,https://www.reddit.com/r/MachineLearning/comments/4rmeql/problems_with_you_only_look_once_realtime_object/,Greendogo,1467861034,"So, I've been training some data on the YOLO network; the data is the keyboard and computer mouse synsets from ImageNet.

After I've trained for 40,000 iterations (batch = 1, subdivisions = 1, in yolo.cfg file), my network will always classify every input image regardless of what it is actually of, as a keyboard with a large bounding box roughly centered on the image (if there's a keyboard on the image, it isn't centered over it).  Also, it's always 43% confident of this single object detection.

So I suppose my question is this: what am I doing wrong that's causing my YOLO training to output the same exact thing regardless of my test input?

I'm so confused...",3,2
174,2016-7-7,2016,7,7,12,4rmff0,An 'einsteinian' thought experiment in Deep Q learning &amp; Causal embedding (with 'complex' lemma and kittens),https://www.reddit.com/r/MachineLearning/comments/4rmff0/an_einsteinian_thought_experiment_in_deep_q/,unibrain,1467861305,[removed],0,1
175,2016-7-7,2016,7,7,13,4rmprc,[1606.03813] Towards an integration of deep learning and neuroscience,https://www.reddit.com/r/MachineLearning/comments/4rmprc/160603813_towards_an_integration_of_deep_learning/,evc123,1467865502,,14,28
176,2016-7-7,2016,7,7,15,4rn3yh,Deep Learning Desktop Computer Question (noobie),https://www.reddit.com/r/MachineLearning/comments/4rn3yh/deep_learning_desktop_computer_question_noobie/,[deleted],1467872088,[deleted],0,0
177,2016-7-7,2016,7,7,15,4rn6dq,Shakeout: A New Regularized Deep Neural Network Training Scheme,https://www.reddit.com/r/MachineLearning/comments/4rn6dq/shakeout_a_new_regularized_deep_neural_network/,brockl33,1467873351,,5,4
178,2016-7-7,2016,7,7,17,4rnfc9,Link to Computer Vision News - July,https://www.reddit.com/r/MachineLearning/comments/4rnfc9/link_to_computer_vision_news_july/,Gletta,1467878436,"July issue of Computer Vision News, including a report from CVPR. Free subscription at page 33.

http://www.rsipvision.com/ComputerVisionNews-2016July/",0,0
179,2016-7-7,2016,7,7,18,4rnora,"3 Reasons Why People, Not Robots, Are Key to Data Science",https://www.reddit.com/r/MachineLearning/comments/4rnora/3_reasons_why_people_not_robots_are_key_to_data/,_joermungandr_,1467883861,,0,1
180,2016-7-7,2016,7,7,20,4ro13h,A tool to integrate Emotion AI on your website,https://www.reddit.com/r/MachineLearning/comments/4ro13h/a_tool_to_integrate_emotion_ai_on_your_website/,vikasr111,1467890292,,4,1
181,2016-7-7,2016,7,7,22,4rooxw,Would You Survive the Titanic? A Guide to Machine Learning in Python,https://www.reddit.com/r/MachineLearning/comments/4rooxw/would_you_survive_the_titanic_a_guide_to_machine/,mrborgen86,1467899979,,0,0
182,2016-7-7,2016,7,7,23,4ropph,[WebCast] Jeff Dean - Large-Scale Deep Learning with TensorFlow for Building Intelligent Systems,https://www.reddit.com/r/MachineLearning/comments/4ropph/webcast_jeff_dean_largescale_deep_learning_with/,[deleted],1467900260,[deleted],0,0
183,2016-7-7,2016,7,7,23,4roumy,Machine Learning Powered Web Scraping Through Scrapely,https://www.reddit.com/r/MachineLearning/comments/4roumy/machine_learning_powered_web_scraping_through/,stummj,1467901972,,16,86
184,2016-7-7,2016,7,7,23,4rov8m,Use Genetic Algo for random forest,https://www.reddit.com/r/MachineLearning/comments/4rov8m/use_genetic_algo_for_random_forest/,brookm291,1467902150,"Just wondering if there is any attempt to use Genetic Algo to recombine simple CART trees for random forest ?

Using genetic recombination of previous features to generate
new trees and new rules.



",1,0
185,2016-7-7,2016,7,7,23,4rovbf,Tool for labeling cars/pedestrians/road signs on images,https://www.reddit.com/r/MachineLearning/comments/4rovbf/tool_for_labeling_carspedestriansroad_signs_on/,n00bzor,1467902174,"Hello,

I'm pretty new to this ML community, just want to be up-front. I'm working on a project with some friends and we've built a tool to draw boxes on images (right now for cars/pedestrians/street signs), can indicate which direction the object is facing, and how much of the object is hidden. We provide the coordinates of those boxes and the annotations in a JSON format.

Weve built this using some algorithms but primarily it's a layer that sits on top of mechanical turk. We really want to get feedback on UI and the output (in regards to if it is useful to build machine learning models for object recognition). 

We'd like to do some label/boxing work for you at a flat rate per image with the intent of talking to you and getting your feedback on the quality of the work, the price range, the output format and the UI. If you are interested please let me know by sending me a message directly. I can answer general questions on here if you are interested. As well as talk to other use cases that would be interesting for you.",2,0
186,2016-7-7,2016,7,7,23,4rovj7,"Trying to generate training data for extractive summarization ,Is there a better way?",https://www.reddit.com/r/MachineLearning/comments/4rovj7/trying_to_generate_training_data_for_extractive/,its_liquid,1467902248,"So i have abstractive summary and the full text of an article,i am trying to map the abstractive summary to the sentences in the full text and hence generating extractive summary for the same , so that i can train my naive bayes classifier.

 I plan to use [Corticol.io Semantic Similarity](http://www.cortical.io/demos/similarity-explorer/) and add the best matching sentences to the extractive summary . Is this the right way to do this, or is there a better way?

or where can i find a dataset for extractive summarization?",0,1
187,2016-7-7,2016,7,7,23,4royov,Graphics card for high-end rendering AND machine learning?,https://www.reddit.com/r/MachineLearning/comments/4royov/graphics_card_for_highend_rendering_and_machine/,is_it_fun,1467903284,"Got a question: I need a machine that will render like a beast and do well at ML too. A flexible workstation. So the graphics card matters. Which graphics card will do both? I get the feeling I have to pick one. As in, a Quadro will render very well but not be great for ML. Am I understanding this right?",2,1
188,2016-7-8,2016,7,8,0,4rp11d,What US graduate programs are good for deep learning?,https://www.reddit.com/r/MachineLearning/comments/4rp11d/what_us_graduate_programs_are_good_for_deep/,songload,1467904019,"I am interested in pursuing a PhD or Masters that will allow me to focus on deep learning/neural network approaches to machine learning with some crossover into psychology. I have two bachelors in Computer Science and Psychology and almost went into AI research after college but decided to go into a related industry (computer games) instead. After some years in industry I am now looking towards applying to a PhD or Masters program in Fall 2017 and I was looking for help in narrowing my school search.

A lot of the information I see about PhD programs is a few years out of date because it appears that many of the professors have moved into industry (http://bvlc.eecs.berkeley.edu/ as an example), so if anyone has recent information I would love to see it. I am specifically interested in neural networks due to my interest in psychology and my experience in writing complicated high performance systems. I do not have much interest in anatomy so machine learning seems like a better fit for me than computational neuroscience, but cognitive science might be another option. I am based in the US and wouldn't necessarily need a grant.

Anyone have suggestions or thoughts? Thanks!",5,1
189,2016-7-8,2016,7,8,0,4rp3sa,Alternative loss functions for Classification tasks?,https://www.reddit.com/r/MachineLearning/comments/4rp3sa/alternative_loss_functions_for_classification/,illiterate_gorillas,1467904888,"I'm working on a classification task with LSTMs.

I was wondering if there were any other cost functions I could use other than the standard ones in tensorflow  softmax_cross_entropy_with_logits

any other things out there That I could try ??",2,1
190,2016-7-8,2016,7,8,0,4rp4en,Super mario bros. machine learning stream (MarI/O),https://www.reddit.com/r/MachineLearning/comments/4rp4en/super_mario_bros_machine_learning_stream_mario/,mikseri123321,1467905076,,0,1
191,2016-7-8,2016,7,8,0,4rp5om,Fun with performance tuning on Torch - and an Intel Knights Landing!,https://www.reddit.com/r/MachineLearning/comments/4rp5om/fun_with_performance_tuning_on_torch_and_an_intel/,borkenit,1467905450,,0,3
192,2016-7-8,2016,7,8,0,4rp7r0,[1607.01759] Bag of Tricks for Efficient Text Classification,https://www.reddit.com/r/MachineLearning/comments/4rp7r0/160701759_bag_of_tricks_for_efficient_text/,cesarsalgado,1467906083,,18,14
193,2016-7-8,2016,7,8,0,4rp9lc,What are some good neuroscience books for AI researchers get inspiration from?,https://www.reddit.com/r/MachineLearning/comments/4rp9lc/what_are_some_good_neuroscience_books_for_ai/,andrewbarto28,1467906671,"I would like to understand more about how the brain works and know what are some promising theories. I know we currently don't know much about it, but I would like to understand where we stand now. I don't want to read a medicine oriented book that talks about Alzheimer's disease. I want to read one that gives insights for AI researchers.

Obs: I have already seen ""Towards an integration of deep learning and neuroscience"" and some Yoshua Bengio's theory papers, so please don't cite those.",11,6
194,2016-7-8,2016,7,8,0,4rpaqi,Need Funding for your project? Need a Job? Lets talk!,https://www.reddit.com/r/MachineLearning/comments/4rpaqi/need_funding_for_your_project_need_a_job_lets_talk/,stoopidcomputer,1467907029,"Hi everyone,

I lurk on this sub and a number of somewhat-related subs, and i'm quite sure there is a lot of great ideas and skilled people looking for a chance to shine.

I've recently begun building a lab and started building teams that focus on applying ""AI"" technologies to new and old problems/processes - And now i want to offer myself, my capital and my teams to help you tackle the problems and projects that you're interested in. (I don't want to 'advertise' my company here - so if you're curious, just send me a PM)

I'm just really excited about the next few years in machine learning, computer vision, statistics, augmented reality, chat bots, etc - and really want to help some people and ideas grow. 

If there is anyone reading this with a great project idea, talent, or just looking for a place to start working - I want to offer myself as a sounding board/potential investor/employer/incubator/whatever you need. 

Thank you for your time!


",0,2
195,2016-7-8,2016,7,8,1,4rpfhq,I want to fund some cool ideas in ML/CV/etc - Where should i post?,https://www.reddit.com/r/MachineLearning/comments/4rpfhq/i_want_to_fund_some_cool_ideas_in_mlcvetc_where/,stoopidcomputer,1467908507,"I'm sure there are lots of people on reddit with some cool ideas - and i'm looking to help incubate or finance some of them. 

Are there any places you guys would suggest posting (to connect with some of these people) that wouldn't break any rules?

Thanks!",0,2
196,2016-7-8,2016,7,8,1,4rpgpo,"BLAS choice, GPU training?",https://www.reddit.com/r/MachineLearning/comments/4rpgpo/blas_choice_gpu_training/,dimmtree,1467908887,"Does the BLAS choice for Caffe matter if using GPU training? For instance can I just use ATLAS, or should I use OpenBLAS? In my case I have an i7-3970x and 2xTitanX.",4,0
197,2016-7-8,2016,7,8,1,4rph7s,Human Intelligence Vs Artificial Intelligence | ApplyBigAnalytics: Data Engineers Mentor,https://www.reddit.com/r/MachineLearning/comments/4rph7s/human_intelligence_vs_artificial_intelligence/,ApplyBigAnalytics,1467909035,,1,1
198,2016-7-8,2016,7,8,1,4rpibt,Evolving Swimming Soft-Bodied Creatures,https://www.reddit.com/r/MachineLearning/comments/4rpibt/evolving_swimming_softbodied_creatures/,erkaman,1467909378,,13,59
199,2016-7-8,2016,7,8,2,4rpo6t,Dataset with rich customer purchase history?,https://www.reddit.com/r/MachineLearning/comments/4rpo6t/dataset_with_rich_customer_purchase_history/,logrech,1467911166,Was wondering if anyone knows of any publicly available datasets that have rich information on customer purchase history? Ideally it would have customer attributes with which to identify individual customers as well as what products they bought. ,4,0
200,2016-7-8,2016,7,8,2,4rpqys,"NVIDIA CEO downplays challenge from Googles AI chip: ""it is only for inferencing""",https://www.reddit.com/r/MachineLearning/comments/4rpqys/nvidia_ceo_downplays_challenge_from_googles_ai/,[deleted],1467912023,[deleted],0,0
201,2016-7-8,2016,7,8,2,4rpuey,Theano Implementation of LambProp?,https://www.reddit.com/r/MachineLearning/comments/4rpuey/theano_implementation_of_lambprop/,alexmlamb,1467913092,"Does anyone have an implementation of LambProp, preferably in Theano?  

The arxiv paper isn't out yet but I think that enough details have been leaked that a strong PhD student should be able to reproduce it.  ",7,2
202,2016-7-8,2016,7,8,2,4rpx8g,On the importance of democratizing Artificial Intelligence,https://www.reddit.com/r/MachineLearning/comments/4rpx8g/on_the_importance_of_democratizing_artificial/,fchollet,1467913984,,13,32
203,2016-7-8,2016,7,8,3,4rpz30,"Nvidia Says GeForce GTX 1060 Will Outperform GTX 980, Founders Edition Will Cost $299",https://www.reddit.com/r/MachineLearning/comments/4rpz30/nvidia_says_geforce_gtx_1060_will_outperform_gtx/,Rich700000000000,1467914540,,0,0
204,2016-7-8,2016,7,8,4,4rqh3k,Mycroft.ai: An open source/open hardware AI platform,https://www.reddit.com/r/MachineLearning/comments/4rqh3k/mycroftai_an_open_sourceopen_hardware_ai_platform/,andyandy16,1467920214,,2,0
205,2016-7-8,2016,7,8,5,4rqqbd,"Microsoft's spam detection is so crappy, it labels emails sent from its own domain as spam.",https://www.reddit.com/r/MachineLearning/comments/4rqqbd/microsofts_spam_detection_is_so_crappy_it_labels/,[deleted],1467923231,[deleted],3,0
206,2016-7-8,2016,7,8,5,4rqr4f,What are some papers on current state of the art machine translation?,https://www.reddit.com/r/MachineLearning/comments/4rqr4f/what_are_some_papers_on_current_state_of_the_art/,Lajamerr_Mittesdine,1467923504,"What are some papers that are state of the art machine learning techniques for machine translation?

The papers can be about translating any languages. 

But if you know any specifically focusing on translating from Japanese, Chinese, Korean to English and vice versa, please post.",3,4
207,2016-7-8,2016,7,8,5,4rqs8c,"Renowned Hungarian Scientist, Inventor Of The ""Klmn filter"" Rudolf Klmn Dies Aged 86",https://www.reddit.com/r/MachineLearning/comments/4rqs8c/renowned_hungarian_scientist_inventor_of_the/,aleph__one,1467923881,,23,264
208,2016-7-8,2016,7,8,5,4rqvdm,"""Why Should I Trust You?"": Explaining the Predictions of Any Classifier",https://www.reddit.com/r/MachineLearning/comments/4rqvdm/why_should_i_trust_you_explaining_the_predictions/,crowwork,1467924940,,8,8
209,2016-7-8,2016,7,8,6,4rr2yi,Predictive Analytics ecosystem,https://www.reddit.com/r/MachineLearning/comments/4rr2yi/predictive_analytics_ecosystem/,get4cast,1467927506,,0,0
210,2016-7-8,2016,7,8,6,4rr47u,Neural Machine Translation by LISA,https://www.reddit.com/r/MachineLearning/comments/4rr47u/neural_machine_translation_by_lisa/,theflofly,1467927935,,6,3
211,2016-7-8,2016,7,8,7,4rrcjm,Comparisons of Text Classification Algorithms,https://www.reddit.com/r/MachineLearning/comments/4rrcjm/comparisons_of_text_classification_algorithms/,learndis,1467930923,"I have a tricky binary text classification problem where I am attempting to classify paragraphs of text with the highest possible accuracy. I know there have been many recent advances in this space with word embeddings, paragraph embeddings, RNNs such as LSTM, GRU, etc... and I am looking for surveys of the recent space or comparisons of these methods on various datasets (preferably including a baseline such as bag-of-words + SVM or logistic regression).


For my specific problem, I have approximately 10,000 labeled training examples and 300,000 unlabeled examples. Is it obvious which approach I should expect to perform best? ",3,6
212,2016-7-8,2016,7,8,7,4rrg5g,What is the best machine learning algorithm for predicting lotto ticket numbers?,https://www.reddit.com/r/MachineLearning/comments/4rrg5g/what_is_the_best_machine_learning_algorithm_for/,sigmoidp,1467932245,[removed],14,0
213,2016-7-8,2016,7,8,8,4rrnkf,visualize bottlenecks in TensorFlow,https://www.reddit.com/r/MachineLearning/comments/4rrnkf/visualize_bottlenecks_in_tensorflow/,[deleted],1467934787,[deleted],0,1
214,2016-7-8,2016,7,8,11,4rsaer,How the hell do you measure accuracy in the TensorFlow Playground??,https://www.reddit.com/r/MachineLearning/comments/4rsaer/how_the_hell_do_you_measure_accuracy_in_the/,zbplot,1467943434,"I'm using the playground to experiment with hyperparameters and am about to pull my hair out. 


Is there a way to observe the accuracy of your network in the [TensorFlow Playground?](http://playground.tensorflow.org) I've noted how little the test loss and training loss have to do with accuracy....there must be an acccuracy metric somewhere though, right? 


Surely the best way isn't to count the dots? Please don't make me count the dots.",0,0
215,2016-7-8,2016,7,8,12,4rskln,"""How to train an artificial neural network to play Diablo 2 using visual input?"" A stack overflow question posted in 2011, then updated in 2012, 2015 and again in June 2016 with links to various technological breakthroughs in the field.",https://www.reddit.com/r/MachineLearning/comments/4rskln/how_to_train_an_artificial_neural_network_to_play/,ggrieves,1467947285,,12,287
216,2016-7-8,2016,7,8,12,4rsqoa,How ISIS Uses Twitter,https://www.reddit.com/r/MachineLearning/comments/4rsqoa/how_isis_uses_twitter/,[deleted],1467949674,,0,6
217,2016-7-8,2016,7,8,15,4rt8xu,training an auto encoder to predict within correct domain in Keras?,https://www.reddit.com/r/MachineLearning/comments/4rt8xu/training_an_auto_encoder_to_predict_within/,bionerd2,1467957843,"I'm training an auto encoder in Keras/Theano. The input is a vector of integers (in fact they are categorical, so each element of any input example vector is something in { 0, 1, 2 }). The problem is the Keras decoder is predicting floats rather than integers. Is there a way to make Keras predict ints? ",10,2
218,2016-7-8,2016,7,8,15,4rt9x0,Intermediate Generated Samples while training Generative Neural Nets,https://www.reddit.com/r/MachineLearning/comments/4rt9x0/intermediate_generated_samples_while_training/,sidakpal,1467958320,"Hi,
I am working on project that involves generative neural nets. I have trained them (DCGAN Architecture) for some while, but I still don't see any image like Cifar10 objects( as I am using that dataset). Can you guys please share what kind of intermediate images and the loss function plots you get during the training? 

Thank you so much!",1,1
219,2016-7-8,2016,7,8,15,4rtehy,Neural Variational Inference: Classical Theory,https://www.reddit.com/r/MachineLearning/comments/4rtehy/neural_variational_inference_classical_theory/,barmaley_exe,1467960648,,2,8
220,2016-7-8,2016,7,8,16,4rtl9y,Machine learning algorithm cheat sheet,https://www.reddit.com/r/MachineLearning/comments/4rtl9y/machine_learning_algorithm_cheat_sheet/,mrdanibudapest,1467964303,,7,7
221,2016-7-8,2016,7,8,17,4rtmiv,[meta] should r/ml have stronger moderation?,https://www.reddit.com/r/MachineLearning/comments/4rtmiv/meta_should_rml_have_stronger_moderation/,Latent_space,1467965033,"qud.
after seeing [this embarrassing comment section](https://www.reddit.com/comments/4rpx8g),  maybe this subreddit should have a discussion about whether this is a free-for-all forum where you have to wade through the (offensive and sometimes emotionally taxing) noise,  or if it should be more curated in the way r/science and other serious subreddits are. 

what this entails. 
- submissions possibly curated. or maybe at least tagged (research,  industry, meta, and fluff?) 
- comments curated for a very,  very similar set of rules that r/science has 

if you want this to be a better venue for meaningful discussion,  these things should be seriously considered.",56,11
222,2016-7-8,2016,7,8,18,4rtuh7,How to handle one ML Model by User ?,https://www.reddit.com/r/MachineLearning/comments/4rtuh7/how_to_handle_one_ml_model_by_user/,marcoflint,1467969784,"Hello fellow data people, 
I am developing an application that I hope I could present you very soon based on Django. 
I would like to be able to compute one ML for each of my users to recommand him ways to optimize his publications and auto-tagging. It seems obvious that the algorithm should adapt itself to each of the user (How ? Retraining ?) and how do you manage a pool of ML algorithms (store where ? how to monitor ?)

I guess my needs are very similar to recommender systems adapted to each users or to spam filters which learns your preferences and so on. 

Thank you in advance for your answers, Best regards from France, Jonathan",1,0
223,2016-7-8,2016,7,8,18,4rtw92,Need testers for AI chatbot,https://www.reddit.com/r/MachineLearning/comments/4rtw92/need_testers_for_ai_chatbot/,anghamithrowaway,1467970811,"Throwaway because work.

Hi everyone, I'm working on a project at Anghami (music streaming service based in Lebanon) which is based on wit.ai's NLP. Basically the goal is that people can text/speak to the bot and it should help you with app problems, answer music requests and a bunch of other stuff.

My biggest issue is that I can't keep coming up with stuff to talk to it about, so I was hoping the people of reddit could help me out and ask for music whether by artists/album/song/lyrics. Fuck with it, ask for jokes, call it a bitch, just push it to the limits.

It's linked to facebook just to make it easy to test for now. You can find it at:
https://facebook.com/anghamitestbot
https://messenger.com/t/anghamitestbot

It probably won't answer until I set you up as a tester, but just send it hey and I'll reply as soon as you're active

Thanks &lt;3",1,2
224,2016-7-8,2016,7,8,20,4ru85d,"This Week in Machine Learning, 8 July 2016: brewing beer, predicting blindness, helping farms, evaluating water usage, and more.",https://www.reddit.com/r/MachineLearning/comments/4ru85d/this_week_in_machine_learning_8_july_2016_brewing/,DavidAJoyner,1467977560,,0,0
225,2016-7-8,2016,7,8,21,4rugiu,Possible applications of Machine Learning in sports domain,https://www.reddit.com/r/MachineLearning/comments/4rugiu/possible_applications_of_machine_learning_in/,satwik_,1467981610,[removed],6,3
226,2016-7-8,2016,7,8,22,4rukgn,Millions of SE Asian jobs may be lost to automation in next two decades: ILO,https://www.reddit.com/r/MachineLearning/comments/4rukgn/millions_of_se_asian_jobs_may_be_lost_to/,juvchan,1467983277,,0,1
227,2016-7-8,2016,7,8,22,4rulc6,The challenges behind parsing &amp; matching CVs and jobs and how machine learning comes to rescue!,https://www.reddit.com/r/MachineLearning/comments/4rulc6/the_challenges_behind_parsing_matching_cvs_and/,grumpybusinesscat,1467983632,,1,3
228,2016-7-8,2016,7,8,22,4rupvk,"LSTM, Attention Mechanism Resources",https://www.reddit.com/r/MachineLearning/comments/4rupvk/lstm_attention_mechanism_resources/,chaser999,1467985376,"Hey I'm new to Deep Learning and have a good understanding of Neural Networks, could you provide me some good resources to learn LSTM and Attention Mechanism in general Machine Learning or for NLP.",3,2
229,2016-7-8,2016,7,8,22,4ruql1,Plastic Profile and Laser Engraving in the UK,https://www.reddit.com/r/MachineLearning/comments/4ruql1/plastic_profile_and_laser_engraving_in_the_uk/,woodside_villa,1467985642,,0,1
230,2016-7-8,2016,7,8,23,4ruy3i,Predicting User Preferences in Python using Alternating Least Squares,https://www.reddit.com/r/MachineLearning/comments/4ruy3i/predicting_user_preferences_in_python_using/,DrLegend,1467988443,,0,2
231,2016-7-9,2016,7,9,0,4rv4sf,Bag of Tricks for Efficient Text Classification - FB,https://www.reddit.com/r/MachineLearning/comments/4rv4sf/bag_of_tricks_for_efficient_text_classification_fb/,gwulfs,1467990812,,6,24
232,2016-7-9,2016,7,9,0,4rv8oy,A good list of courses to learn machine learning online,https://www.reddit.com/r/MachineLearning/comments/4rv8oy/a_good_list_of_courses_to_learn_machine_learning/,[deleted],1467992132,[deleted],0,11
233,2016-7-9,2016,7,9,0,4rvacb,Dropout Keep Probability as a Learned Parameter,https://www.reddit.com/r/MachineLearning/comments/4rvacb/dropout_keep_probability_as_a_learned_parameter/,LeavesBreathe,1467992686,"Hey Guys,

Have been doing alot of experimenting with zoneout and dropout in LSTMs and recently I have been wondering:

*Why do we not set dropout keep probability as a learned parameter?*

The issue is that for different datasets, different dropout keep probabilities are most optimal. You simply can't *guess* the right dropout keep probability. 

Furthermore, we could have the dropout keep probability a *separate learnable parameter for each layer*

Finally, to really ensure that the keep probability doesn't go to zero, we could implement a scalar dropout regularizer that weighs the amount of dropout versus the amount of dropout should be used:

    Total Loss = Loss + Dropout Regularizer * Dropout For Layer

Just wondering why no one has done this! Thanks",11,3
234,2016-7-9,2016,7,9,1,4rvh64,LinkedIn group for machine learning and AI Applications,https://www.reddit.com/r/MachineLearning/comments/4rvh64/linkedin_group_for_machine_learning_and_ai/,iamtheinvestor,1467994891,,2,0
235,2016-7-9,2016,7,9,1,4rvmmx,IBM Fronts Money for Deep Learning Devs via Cognitive Cup Hackathon Thing,https://www.reddit.com/r/MachineLearning/comments/4rvmmx/ibm_fronts_money_for_deep_learning_devs_via/,[deleted],1467996600,[deleted],0,0
236,2016-7-9,2016,7,9,4,4rwcql,"It's ML, not magic: simple questions you should ask to help reduce AI hype",https://www.reddit.com/r/MachineLearning/comments/4rwcql/its_ml_not_magic_simple_questions_you_should_ask/,smerity,1468004846,,17,121
237,2016-7-9,2016,7,9,4,4rwdx0,Renaming our company - Dato is now Turi,https://www.reddit.com/r/MachineLearning/comments/4rwdx0/renaming_our_company_dato_is_now_turi/,alexmlamb,1468005241,,0,0
238,2016-7-9,2016,7,9,4,4rwiyn,"16 Free Machine Learning Books. Direct download, no email required.",https://www.reddit.com/r/MachineLearning/comments/4rwiyn/16_free_machine_learning_books_direct_download_no/,get4cast,1468006881,,5,0
239,2016-7-9,2016,7,9,5,4rwmwm,Train a ML model to rapidly solve quantum mechanics problems.,https://www.reddit.com/r/MachineLearning/comments/4rwmwm/train_a_ml_model_to_rapidly_solve_quantum/,hudsmith,1468008173,"It occurred to me that it may be possible for ML algorithms to model the mapping from a quantum mechanical potential to the eigenvalues of the Hamiltonian for the quantum mechanical system. Plus, for a sufficiently simple model, the prediction speed may be faster than solving Schrodinger's equation outright.

Any comments/criticisms about the basic idea?

I've started working on it: https://github.com/dhudsmith/qmnnet

UPDATE (July 26, 2016):
You can now run through the entire process of training a neural net to solve quantum mechanics problems right in your browser. You can view, modify, and run the code as you see fit. This is made possible by using [jupyter](http://jupyter.org/) + [binder](http://mybinder.org/). 

To access the interactive notebook:

1. Go to https://github.com/dhudsmith/qmnnet
2. Click the `launch|binder` badge at the top of README.md. That will take you to an interactive jupyter session
3. Open `qmnnet.ipynb`

Let me know if it gives you any problems.",12,6
240,2016-7-9,2016,7,9,5,4rwqqs,Using Machine Learning to predict deaths on Game of Thrones,https://www.reddit.com/r/MachineLearning/comments/4rwqqs/using_machine_learning_to_predict_deaths_on_game/,Sig_Luna,1468009429,,2,0
241,2016-7-9,2016,7,9,6,4rx6eh,Another Neural Style app,https://www.reddit.com/r/MachineLearning/comments/4rx6eh/another_neural_style_app/,dharma-1,1468014770,"http://www.thestar.com.my/tech/tech-news/2016/06/24/russian-photo-app-prisma-makes-splash-woos-investors/

http://prisma-ai.com/

It's pretty fast, wonder if it runs on the phone. No credits to the  the algorithm it ripped off, of course.",6,6
242,2016-7-9,2016,7,9,7,4rx909,Is it wise to learn TensorFlow before scikit learn?,https://www.reddit.com/r/MachineLearning/comments/4rx909/is_it_wise_to_learn_tensorflow_before_scikit_learn/,redditusernamed,1468015698,[removed],6,1
243,2016-7-9,2016,7,9,7,4rxbvl,Anyone able to run Tensorflow with 1070/1080 on Ubuntu 16.04/15.10/15.04?,https://www.reddit.com/r/MachineLearning/comments/4rxbvl/anyone_able_to_run_tensorflow_with_10701080_on/,ancientmtk,1468016743,"I've been tearing my hair out for the last 3 days and I just can't get it to work.

I recently got a GTX 1070 and I went to a fresh install with:
* Ubuntu 16.04/15.10/15.04
* PPA drivers nvidia-367
* Cuda 7.5

Somehow every step of the way I couldn't get CUDA to detect my GPU despite following various instructions 
http://askubuntu.com/questions/693145/installing-cuda-7-5-toolkit-on-ubuntu-15-10
https://www.pugetsystems.com/labs/hpc/GTX-1080-CUDA-performance-on-Linux-Ubuntu-16-04-preliminary-results-nbody-and-NAMD-803/

Has anyone been able to get 1070/1080 running CUDA and tensorflow correctly and can guide me thru this painful process?

(I can't do ubuntu 14.04 as it doesn't register my wifi card, which then i can't do anything)

Plz welp~",17,2
244,2016-7-9,2016,7,9,8,4rxk5h,Translating Matlab Code to Python,https://www.reddit.com/r/MachineLearning/comments/4rxk5h/translating_matlab_code_to_python/,yxsilentxy,1468019749,[removed],8,2
245,2016-7-9,2016,7,9,8,4rxns6,Did you use a learning algorithm implementation designed for sparse data? Tell us about it.,https://www.reddit.com/r/MachineLearning/comments/4rxns6/did_you_use_a_learning_algorithm_implementation/,datasciguy-aaay,1468021148,"I use R and Spark and python. Did you originally use one of the typical ML packages available with something like these platforms, and then find out you needed to switch to some kind of sparse-supporting specialty software?

Did you use a whole different package, or, did you use an add-in for sparse computations in a traditional platform like (not limited to) R or python.",8,13
246,2016-7-9,2016,7,9,9,4rxtws,Who are the best DL researchers to follow on Twitter?,https://www.reddit.com/r/MachineLearning/comments/4rxtws/who_are_the_best_dl_researchers_to_follow_on/,Jxieeducation,1468023500,Thanks!,28,13
247,2016-7-9,2016,7,9,9,4rxv37,How to Best tweak hyperparametera for RNN accuracy?,https://www.reddit.com/r/MachineLearning/comments/4rxv37/how_to_best_tweak_hyperparametera_for_rnn_accuracy/,[deleted],1468023960,"Hey guys,

I am currently building a CNN some text data. My dataset seems to work out fine since I get the same/slightly better accuracy compared to the well known IMDB dataset. The goal is basically sentiment classification of short paragraphs (binary classes that is).

However, when I train 5 epochs of the network I get like 51% accuracy. I tried tweaking some stuff (embedding dimensions, learning rate, Algo, amount of layers, training set size, vocab size etc.) but the only thing that ramps up my validation accuracy is basically number of epochs.

For 15 epochs for example I get approximately 90% accuracy, but I have the fear that this might just be overfitting the network (although I do use dropout).

Basically I am just jiggling stuff around not really knowing what I am doing thus far.

How would a sane person go about tweaking the network in a scientific manner? How do you guys do it? Any tips regarding 

* how many epochs is enough
* which algorithm to use
* tips for regularization
* dimensions for embedding/layers/etc
* general procedure to ramp up the classification accuracy without over fitting?

Any help is appreciated guys!",5,2
248,2016-7-9,2016,7,9,9,4rxvt5,Suggestion on algorithm to classify hand-written alphabet on computer,https://www.reddit.com/r/MachineLearning/comments/4rxvt5/suggestion_on_algorithm_to_classify_handwritten/,xbradhawkx,1468024248,[removed],1,9
249,2016-7-9,2016,7,9,9,4rxygo,When to stop iteration of neural network training?,https://www.reddit.com/r/MachineLearning/comments/4rxygo/when_to_stop_iteration_of_neural_network_training/,tuming1990,1468025337,"On validation set, Loss starts to drop or no lower loss for several iterations, or accuracy starts to drop or no higher accuracy for several iterations?

Which one is better and why?

Thanks",5,0
250,2016-7-9,2016,7,9,10,4ry6hq,I was training a CNN and this huge accuracy drop happened (see link). Any idea why?,https://www.reddit.com/r/MachineLearning/comments/4ry6hq/i_was_training_a_cnn_and_this_huge_accuracy_drop/,ppff01,1468028787,,3,6
251,2016-7-9,2016,7,9,11,4ryf8s,A comprehensive list of Machine Learning courses available online,https://www.reddit.com/r/MachineLearning/comments/4ryf8s/a_comprehensive_list_of_machine_learning_courses/,[deleted],1468032653,[deleted],0,1
252,2016-7-9,2016,7,9,12,4ryn2l,This Robot Can Sound Design Your Movie,https://www.reddit.com/r/MachineLearning/comments/4ryn2l/this_robot_can_sound_design_your_movie/,[deleted],1468036184,[deleted],0,1
253,2016-7-9,2016,7,9,12,4ryn5g,This Robot Can Sound Design Your Movie,https://www.reddit.com/r/MachineLearning/comments/4ryn5g/this_robot_can_sound_design_your_movie/,j_lyf,1468036220,,0,0
254,2016-7-9,2016,7,9,13,4ryrlk,Building Machine Learning Estimator in TensorFlow,https://www.reddit.com/r/MachineLearning/comments/4ryrlk/building_machine_learning_estimator_in_tensorflow/,terrytangyuan,1468038242,,0,20
255,2016-7-9,2016,7,9,14,4rz1er,How to make plastercine packaging,https://www.reddit.com/r/MachineLearning/comments/4rz1er/how_to_make_plastercine_packaging/,mixmachinery,1468043092,,1,1
256,2016-7-9,2016,7,9,16,4rzbfp,Python ML: Logit Beta Weights,https://www.reddit.com/r/MachineLearning/comments/4rzbfp/python_ml_logit_beta_weights/,sbagroy986,1468048685,"Hey guys. I'm using python to build a logistic regression classifier and I need to get the beta weights from it. I've looked into sklearn and statsmodel documentation, but I haven't been able to find any functions which give us the weights. I know for a fact that statsmodel does have an end-point that provides these features/weights. Can anyone help me out with this?",1,1
257,2016-7-9,2016,7,9,17,4rzj7h,Can convnets learn random noise?,https://www.reddit.com/r/MachineLearning/comments/4rzj7h/can_convnets_learn_random_noise/,[deleted],1468053649,[deleted],0,1
258,2016-7-9,2016,7,9,18,4rzky1,Can CNN learn to recognize fixed shapes containing random noise?,https://www.reddit.com/r/MachineLearning/comments/4rzky1/can_cnn_learn_to_recognize_fixed_shapes/,robobub,1468054846,"I was curious as to whether convnets can learn consistent repeated shapes filled with non-coherent random noise?  

E.g. a square of fixed size that is filled with completely random noise within it, which moves around an image.  

My initial thoughts were of course it could, but now I can't help but view it from the perspective of a linear filter/classifier [(where kaparthy explains adversarial examples in cs231n)](https://youtu.be/FnU6h2dQt8g?list=PLlJy-eBtNFt6EuMxFYRiNRS07MCWN5UIA&amp;t=3727), and am struggling to think of how it could learn it.  

Sure, it could learn small little random noise 3x3 filters, but how would the later layers learn to aggregate all that random noise coherently, if its truly random?  Only a few of those random 3x3 filters will have large values.  I suppose it could basically just look for large responses anywhere, but that seems unsatisfactory.  

It could look for the edges of the square vs the background, but what if the edges are feathered? ",16,26
259,2016-7-9,2016,7,9,18,4rznss,NLP (C)NNs pre-trained on text?,https://www.reddit.com/r/MachineLearning/comments/4rznss/nlp_cnns_pretrained_on_text/,lispbaron,1468056853,I'm looking for (C)NN pre-trained on English text. That should save me longer training time. I'm going to do surgery to combine that with my NN (free features!),1,4
260,2016-7-9,2016,7,9,22,4s08ee,Best way to organize image data to train DNNs,https://www.reddit.com/r/MachineLearning/comments/4s08ee/best_way_to_organize_image_data_to_train_dnns/,zaega,1468069792,"I want to use torch to train DNNs to classify images, but I'm not sure what would be the best way to organize the data. At the moment I have around 100,000 images but this value will increase over time and I hope to reach more than 1,000,000 images.

Since many available datasets organize images as files, my first approach would be to store the images as files and organize the metadata of the images in a database (e.g. MySQL).

1. Is it efficient to train DNNs with torch directly on image files or is there any faster way?
2. Would you suggest any other approach that has advantages over this? (more efficient, scalable, ...)

Thanks!",5,1
261,2016-7-10,2016,7,10,0,4s0ukv,Kaggle's Digit Recognizer Testing Help,https://www.reddit.com/r/MachineLearning/comments/4s0ukv/kaggles_digit_recognizer_testing_help/,[deleted],1468079550,[removed],1,0
262,2016-7-10,2016,7,10,0,4s0vcp,Neural Variational Inference: Stochastic Variational Inference,https://www.reddit.com/r/MachineLearning/comments/4s0vcp/neural_variational_inference_stochastic/,[deleted],1468079857,[deleted],0,1
263,2016-7-10,2016,7,10,0,4s0vlc,Neural Variational Inference: Stochastic Variational Inference,https://www.reddit.com/r/MachineLearning/comments/4s0vlc/neural_variational_inference_stochastic/,barmaley_exe,1468079954,,0,7
264,2016-7-10,2016,7,10,1,4s0vp8,Question regarding normalization propagation,https://www.reddit.com/r/MachineLearning/comments/4s0vp8/question_regarding_normalization_propagation/,Blammar,1468080000,"In the paper http://arxiv.org/pdf/1603.01431v5.pdf, I'm thinking that the gamma sub i values (see equations 9 and 11) are actually constants (1/1.21) rather than variables that need to be initialized and then learned.

Is that other people's reading also?",2,2
265,2016-7-10,2016,7,10,1,4s107x,Microsoft's Minecraft AI platform mod (Project Malmo) now available on Github,https://www.reddit.com/r/MachineLearning/comments/4s107x/microsofts_minecraft_ai_platform_mod_project/,bunch_of_miscreants,1468081756,,7,57
266,2016-7-10,2016,7,10,1,4s1429,Using ML to recognize cats that visit my mother,https://www.reddit.com/r/MachineLearning/comments/4s1429/using_ml_to_recognize_cats_that_visit_my_mother/,[deleted],1468083156,[removed],1,1
267,2016-7-10,2016,7,10,1,4s1458,Why is Cross Entropy so popular?,https://www.reddit.com/r/MachineLearning/comments/4s1458/why_is_cross_entropy_so_popular/,danielcanadia,1468083184,"Hi everyone, I have worked with neural networks a lot lately. I have recently been stumped a little about why a lot of people tend to use cross entropy for optimization.

Cross Entropy is:
Sum of P(x)log(Q(x)) where P is the correct probability and Q is the amount generated by model.

But lets say you have two classes and all P(x) are either 0 or 1 (for the example's simplicity). If P(x) = 0, then the cross entropy for a case will always be 0, regardless of whether Q(x) is near 1 (which should be reduced) or near 0 (correctly). Wouldn't using a function such as log (abs (P (x)-Q(x))) make more sense? Maybe I'm over thinking this, but I would really appreciate if someone could clarify.",12,26
268,2016-7-10,2016,7,10,2,4s171h,"Growing Pains of /r/ML, more active moderation?",https://www.reddit.com/r/MachineLearning/comments/4s171h/growing_pains_of_rml_more_active_moderation/,[deleted],1468084221,[deleted],0,1
269,2016-7-10,2016,7,10,2,4s175l,"Growing pains of /r/MachineLearning, more active moderation?",https://www.reddit.com/r/MachineLearning/comments/4s175l/growing_pains_of_rmachinelearning_more_active/,olaf_nij,1468084262,"I've decided to make a new more visible thread to discuss this issue. Here's the reference thread for more background:

https://www.reddit.com/r/MachineLearning/comments/4rtmiv/meta_should_rml_have_stronger_moderation/

I personally try to stay away from strong moderation because I'm aware of the personal biases I might be imposing on the community. But I find myself having to reconsider as the number of posts per day has been growing steadily.

As a researcher myself, I prefer the subreddit to be more focused on research/science than other things. You'll notice this in the AMAs I organize. But I know plenty of those in industry who find /r/MachineLearning useful to them because it's not a pure science subreddit. Unlike other fields, a great deal of the success in machine learning is because of contributions from industry. So I personally think we should not adopt a format like /r/science.

In an ideal world, I would like more participation from the community in terms of upvoting and downvoting. As a moderator, that gives me a better idea of what to do with a comment or post.

But in the absence of that, should we come up with very explicit rules of what is allowed? The issue is we can always find an exception. For example, no blogspam? That would force us to remove some very great posts but also allow us to remove many bad ones which are vaguely self-promoting and advertising. It's hard not to be subjective.

So I wanted to open the floor to discussion on the moderation issue.

P.S&gt; I'm all for making moderation a machine learning problem if someone can prove to me machines won't be biased :) 

**Potential baselines for post removal based on the discussion so far:**

* ""is obviously targeted at otherwise uninformed laypeople""
* belongs in /r/Futurology 
* lacks technical detail
* easily googled
* personal attacks, name-calling, insults

**Additional Features to the Subreddit**

* Labeled posts (posts must have labels like Question, News, Research, Discussion)",109,87
270,2016-7-10,2016,7,10,2,4s1ciq,What are some good ways of removing outliers from a dataset?,https://www.reddit.com/r/MachineLearning/comments/4s1ciq/what_are_some_good_ways_of_removing_outliers_from/,frogsplash911,1468086180,[removed],12,2
271,2016-7-10,2016,7,10,5,4s285l,NVIDIA Jetson TX1 Promotion for Machine Learning Algorithms,https://www.reddit.com/r/MachineLearning/comments/4s285l/nvidia_jetson_tx1_promotion_for_machine_learning/,[deleted],1468097863,[deleted],0,0
272,2016-7-10,2016,7,10,6,4s2cun,What are the current options for automatic hyperparameter fitting?,https://www.reddit.com/r/MachineLearning/comments/4s2cun/what_are_the_current_options_for_automatic/,piesdesparramaos,1468099645,[removed],7,1
273,2016-7-10,2016,7,10,7,4s2iub,Chasing Cats,https://www.reddit.com/r/MachineLearning/comments/4s2iub/chasing_cats/,fully_connected,1468101887,,13,99
274,2016-7-10,2016,7,10,8,4s2xqm,Machine Learning - WAYR (What Are You Reading) - Week 2,https://www.reddit.com/r/MachineLearning/comments/4s2xqm/machine_learning_wayr_what_are_you_reading_week_2/,Deinos_Mousike,1468107765,"This is a place to share machine learning research papers, journals, and articles that you're reading this week. If it relates to what you're researching, by all means elaborate and give us your insight, otherwise it could just be an interesting paper you've read.

Preferably you should link the arxiv page (not the PDF, you can easily access the PDF from the summary page but not the other way around) or any other pertinent links.

Besides that, there are no rules, have fun.",15,43
275,2016-7-10,2016,7,10,8,4s2y1w,Comparing Torch/Torchnet?,https://www.reddit.com/r/MachineLearning/comments/4s2y1w/comparing_torchtorchnet/,dataislyfe,1468107898,"Hey guys,

I was wondering generally what the use cases were for Torch versus Torchnet? It seems like Torchnet is similar to Keras and its sklearn api? I'm wondering what people's experience with Torchnet has been (it is relatively new, I guess)....Also are there any other wrapper frameworks around Torch7? ",0,1
276,2016-7-10,2016,7,10,13,4s41e1,Visualizing Convolutions in Tensorflow Mnist Tutorial,https://www.reddit.com/r/MachineLearning/comments/4s41e1/visualizing_convolutions_in_tensorflow_mnist/,[deleted],1468125616,[removed],0,0
277,2016-7-10,2016,7,10,15,4s4dvo,Am I doing it correctly?,https://www.reddit.com/r/MachineLearning/comments/4s4dvo/am_i_doing_it_correctly/,nadiah2016,1468132161,[removed],0,1
278,2016-7-10,2016,7,10,17,4s4nh8,BIG DATA MADE EASY!!,https://www.reddit.com/r/MachineLearning/comments/4s4nh8/big_data_made_easy/,andalib_ansari,1468138223,[removed],0,1
279,2016-7-10,2016,7,10,19,4s4x6v,In which situation in reinforcement learning is the cross entropy method useful?,https://www.reddit.com/r/MachineLearning/comments/4s4x6v/in_which_situation_in_reinforcement_learning_is/,[deleted],1468144991,[deleted],3,1
280,2016-7-10,2016,7,10,20,4s5362,Algorithm for words related to selected word,https://www.reddit.com/r/MachineLearning/comments/4s5362/algorithm_for_words_related_to_selected_word/,urifeigin,1468148903,"Hi,
I'm looking for a related word algorithm.
nltk.similar finds words used in the same context of the selected word.
I'm would like to find words which relate in the content to the selected word.

for example:
""John went to see Amy. Amy sat at the table""

Submitting ""Table"" will result in ""Amy"".
If possible than changing the threshold, ""table"" will return ""Amy"" and ""John"".

I'm new to ML. I use NLTK as a playground.
Thanks!
",2,0
281,2016-7-10,2016,7,10,21,4s5a11,Any library to tell if an image is that of a bird?,https://www.reddit.com/r/MachineLearning/comments/4s5a11/any_library_to_tell_if_an_image_is_that_of_a_bird/,badtemperedpeanut,1468153415,[removed],1,0
282,2016-7-10,2016,7,10,21,4s5bt6,Machine Learning Algorithm Spots Depression in Speech Patterns,https://www.reddit.com/r/MachineLearning/comments/4s5bt6/machine_learning_algorithm_spots_depression_in/,oliveione,1468154477,,22,173
283,2016-7-10,2016,7,10,21,4s5c2c,best CCA implementation?,https://www.reddit.com/r/MachineLearning/comments/4s5c2c/best_cca_implementation/,lioru,1468154609,[removed],0,0
284,2016-7-10,2016,7,10,22,4s5h6d,I am making a machine learning library with Python. How can it be improved?,https://www.reddit.com/r/MachineLearning/comments/4s5h6d/i_am_making_a_machine_learning_library_with/,[deleted],1468157380,[deleted],28,49
285,2016-7-10,2016,7,10,23,4s5pos,Find REAL GIRL FOR SEX in your area with 100% guranteed result (check img description),https://www.reddit.com/r/MachineLearning/comments/4s5pos/find_real_girl_for_sex_in_your_area_with_100/,demmax2004,1468161582,,0,0
286,2016-7-11,2016,7,11,0,4s5tzs,"Anywhere I can find ""daily conversation"" examples?",https://www.reddit.com/r/MachineLearning/comments/4s5tzs/anywhere_i_can_find_daily_conversation_examples/,KHRZ,1468163559,[removed],1,0
287,2016-7-11,2016,7,11,0,4s5ysg,Harry Potter: Written by Artificial Intelligence,https://www.reddit.com/r/MachineLearning/comments/4s5ysg/harry_potter_written_by_artificial_intelligence/,yourbasicgeek,1468165585,,0,0
288,2016-7-11,2016,7,11,1,4s65bt,What are some interesting insights I can attempt to gather from an e-commerce store's sales data set?,https://www.reddit.com/r/MachineLearning/comments/4s65bt/what_are_some_interesting_insights_i_can_attempt/,[deleted],1468168253,[removed],1,0
289,2016-7-11,2016,7,11,1,4s66mt,This Week in ML &amp; AI Podcast - 7/8/16,https://www.reddit.com/r/MachineLearning/comments/4s66mt/this_week_in_ml_ai_podcast_7816/,sbc1906,1468168724,"This week's show is at =&gt; https://twimlai.com/8

Summary:

* Societal and economic implications of AI from latest White House workshop

* A BS meter for AI, and beer brewed by AI

* Business: More acquisitions and fundraising. One VC's investment thesis on bots. Microsoft's AI strategy.

* Research: ML/AI for diagnosing eye disease, Alzheimers. Predator robots.

* Tech: IPython 5.0 release, Skype Bot Framework updated, Microsoft Project Malmo goes open source.

* Projects: Teaching an AI to write Python code. Retrieval-based models for chatbots in TensorFlow.

 

BONUS: A couple of weeks ago I mentioned the forthcoming O'Reilly book, Mastering Feature Engineering, which is available now as part of their Early Access program. The folks at O'Reilly reached out and offered FREE access to the ebook to all TWiML listeners. Link in the show notes.",0,0
290,2016-7-11,2016,7,11,1,4s67f4,Looking for convolutional neural network tutorial,https://www.reddit.com/r/MachineLearning/comments/4s67f4/looking_for_convolutional_neural_network_tutorial/,nate1421m,1468169036,[removed],2,0
291,2016-7-11,2016,7,11,3,4s6sye,Benchmarks for 'long-term memory'?,https://www.reddit.com/r/MachineLearning/comments/4s6sye/benchmarks_for_longterm_memory/,athitham,1468176940,"Hey guys. I'm planning to do some research on how various deep learning architectures handle long-term memory. From what I've heard, this is still an ongoing problem that hasn't been solved so I thought it would be a good area for research.

I'm wondering where would be the best place to start.
I think as a first step, it would be quite useful to find out if there are any good benchmarks out there to assess the long-term memory of a network.

Thanks.",4,10
292,2016-7-11,2016,7,11,4,4s6ylz,End to end way of addressing catastrophic inference in neural networks?,https://www.reddit.com/r/MachineLearning/comments/4s6ylz/end_to_end_way_of_addressing_catastrophic/,[deleted],1468179070,[deleted],0,0
293,2016-7-11,2016,7,11,4,4s71vh,This paper studies interactive teaching strategies for identifying when a student can benefit from teacher-advice in a reinforcement learning framework.,https://www.reddit.com/r/MachineLearning/comments/4s71vh/this_paper_studies_interactive_teaching/,downtownslim,1468180287,,0,0
294,2016-7-11,2016,7,11,5,4s77my,"Looking for specialized object recognition in live video of any curved loop, where some parts may be missing",https://www.reddit.com/r/MachineLearning/comments/4s77my/looking_for_specialized_object_recognition_in/,BenRayfield,1468182473,"I'm using webcam and canvas in html to see a ""bendable loop"" game controller (a cut of extension cord taped into a loop) and ajax it to a local server running the game.

In this video, points move toward whatever red thing moves near them, but of course some things other than the object are also recognized like my hand holding the loop.
https://www.youtube.com/watch?v=rgIkcQN2jn4

Should I use a stateful model where points observed recently move toward where they see the local part of object move (and somehow spread them along its perimeter)?

Or should I use a stateless model where each video frame finds the best fit loop path in an image?",0,0
295,2016-7-11,2016,7,11,5,4s7878,Exponential runtime increase in the degree of the polynomial kernel using SVC?,https://www.reddit.com/r/MachineLearning/comments/4s7878/exponential_runtime_increase_in_the_degree_of_the/,bagelorder,1468182678,"I am using sklearn SVC with polynomial kernel. For degrees &lt;= 5 my laptop is able to train the model (though the increase in the runtime looks very exponential), degree 5 taking about 2.5 minutes. And for degree 10 he gives up then (well, I stopped his thinking session after an hour). 

Calculating the scalar products (kernel values) of a polynomial degree is nearly the same problem for each degree as I understand it ((&lt;x,y&gt; + 1)^d) - where does this runtime increase then come from? Is there anything I can do about it?",3,1
296,2016-7-11,2016,7,11,6,4s7hsr,"Computer Vision - Facts, Mind experiments and some amazing observations by an AI researcher",https://www.reddit.com/r/MachineLearning/comments/4s7hsr/computer_vision_facts_mind_experiments_and_some/,nounshoun,1468186318,,0,1
297,2016-7-11,2016,7,11,6,4s7knw,Generative Adversarial Networks - Fresh ML #2,https://www.reddit.com/r/MachineLearning/comments/4s7knw/generative_adversarial_networks_fresh_ml_2/,llSourcell,1468187390,,15,11
298,2016-7-11,2016,7,11,7,4s7t7t,Evolution vs backprop as a form of parameter compression - lessons learned from the ALife community,https://www.reddit.com/r/MachineLearning/comments/4s7t7t/evolution_vs_backprop_as_a_form_of_parameter/,NichG,1468190591,,9,28
299,2016-7-11,2016,7,11,11,4s8mma,Seq2seq gradient descent manually?,https://www.reddit.com/r/MachineLearning/comments/4s8mma/seq2seq_gradient_descent_manually/,rescue11,1468202725,"Would it be possible to build a seq2seq machine translation model w/ attention from the ground up, without automatic differentiation? 

I don't see anything about gradient update rules in the seq2seq or rnn-enc-dec papers, is this because it's impossible to make these models without automatic differentiation or is it merely a matter of convenience?

If it is possible, then what would the gradient chain look like? 
Would it be like 

output word embedding -&gt; decoder Rnnlm(GRU) -&gt; attention mlp -&gt; encoder RNN(GRU) -&gt; input word embedding ?

",3,3
300,2016-7-11,2016,7,11,12,4s8ya4,Difficulty Replicating Weight Normalization In TensorFlow (Code Provided),https://www.reddit.com/r/MachineLearning/comments/4s8ya4/difficulty_replicating_weight_normalization_in/,LeavesBreathe,1468207646,"Hey Guys,

I've been working on a TensorFlow Implementation of weight normalization:

http://arxiv.org/abs/1602.07868

I've been looking at the author's provided theano code:

https://github.com/TimSalimans/weight_norm/blob/master/nn.py#L247

I feel that there's something crucially I do not understand. The paper states that the weights are reparameterized as: 
	W = (g/||v||) * v

Yet in there implementation, when they calculate the Euclidean norm, they do NOT sum on all axis. Rather, it appears that g is a vector (not a scalar), and then sum on all but one axis when calculating the Euclidean Norm. 

Does this not go against their in equation 2? Definitely don't want to shoot down the authors or anything, but I feel like I'm missing something. The learnable parameter g should be a single scalar number correct? It should NOT be a vector. And consequently, the Euclidean norm should be summed on **all** axis. 

I've provided my code below:

	def apply_weight_normalization(weight, scope = 'weight_norm'):
	  g_shape = [1]
	  with tf.variable_scope(scope):
	    weight_euclidean_norm =calc_euclidean_norm(weight, reduction_indices = 'all')
	    g = tf.get_variable('g_scalar', shape = g_shape, initializer = tf.constant_initializer(1.0))
	    reparameterized_weight = (g/weight_euclidean_norm) * weight
	  return reparameterized_weight


	def calc_euclidean_norm(tensor, reduction_indices = 1, scope = 'calc_euclidean_norm'):
	  '''reduction_indices can be a list if more reduction is needed'''
	  if reduction_indices == 'all': reduction_indices = None
	  with tf.variable_scope(scope):
	    return tf.sqrt(tf.reduce_sum(tf.square(tensor), reduction_indices =  reduction_indices))


So far I have found in a RNN seq2seq setting, weight normalization has a negligible effect, but perhaps I'm implementing it incorrectly. ",9,0
301,2016-7-11,2016,7,11,13,4s99v4,We made an open source tool that lets you monitor your deep learning experiments in real-time,https://www.reddit.com/r/MachineLearning/comments/4s99v4/we_made_an_open_source_tool_that_lets_you_monitor/,adalikesapples,1468213020,,0,2
302,2016-7-11,2016,7,11,17,4s9uqc,The different types of dispersion machine,https://www.reddit.com/r/MachineLearning/comments/4s9uqc/the_different_types_of_dispersion_machine/,mixmachinery,1468224229,,1,1
303,2016-7-11,2016,7,11,17,4s9yag,Natural Language Processing for Machine Learning Tasks via REST API,https://www.reddit.com/r/MachineLearning/comments/4s9yag/natural_language_processing_for_machine_learning/,[deleted],1468226499,[deleted],0,1
304,2016-7-11,2016,7,11,18,4sa31o,[1603.04259] Item2Vec: Neural Item Embedding for Collaborative Filtering,https://www.reddit.com/r/MachineLearning/comments/4sa31o/160304259_item2vec_neural_item_embedding_for/,pmigdal,1468229482,,20,24
305,2016-7-11,2016,7,11,18,4sa5pd,"[Question] GTX 980 vs GTX 1080, actual performance difference in practice?",https://www.reddit.com/r/MachineLearning/comments/4sa5pd/question_gtx_980_vs_gtx_1080_actual_performance/,carlthome,1468230992,My wallet is wondering how much faster the GTX 1080 is in a real-world situation for training RNNs. Does anyone have any benchmarks  (seconds per epoch) they could share? Perhaps for CNNs?,18,20
306,2016-7-11,2016,7,11,19,4sabaa,London company uses artificial intelligence to brew the perfect beer,https://www.reddit.com/r/MachineLearning/comments/4sabaa/london_company_uses_artificial_intelligence_to/,[deleted],1468234003,[deleted],0,0
307,2016-7-11,2016,7,11,20,4saisa,Workflow: What to do in training times?,https://www.reddit.com/r/MachineLearning/comments/4saisa/workflow_what_to_do_in_training_times/,bagelorder,1468237872,"As one sees from the question I am a total beginner in applying ML. I am just starting with applying some techniques to MNIST and small I can make the error. Maybe this is due to me being so inexperienced: But at the moment I just think of some different approaches I could try, then let my computer train and compare the results I get. 

But the training time can get very big very quickly (16gb ram + intel i5) and most of the time I spend waiting.

Is this normal? Does one just think of some new model, then let it train, wait for some hours and then check the results and based on that again twist the parameters/try a new preprocessing/modelling approach?",8,0
308,2016-7-11,2016,7,11,22,4saw05,Machine Learning Theory: What if we descend to the computer's level instead of trying to bring it to ours?,https://www.reddit.com/r/MachineLearning/comments/4saw05/machine_learning_theory_what_if_we_descend_to_the/,[deleted],1468243428,[deleted],2,0
309,2016-7-11,2016,7,11,22,4sazt7,FBLearner Flow ICML Slides,https://www.reddit.com/r/MachineLearning/comments/4sazt7/fblearner_flow_icml_slides/,[deleted],1468244911,[deleted],0,0
310,2016-7-11,2016,7,11,22,4sb0kt,Per-instance cost-aware learning?,https://www.reddit.com/r/MachineLearning/comments/4sb0kt/perinstance_costaware_learning/,falgo12,1468245208,"I have a situation where the misclassification cost depends on the instance, i.e. on the independent variables. In my training set I have for each instance the independent variables plus a vector of costs for each possible class, where the cost for the correct class is 0 and the cost for incorrect classes can be any value &gt; 0.

It is easy to see how those costs could be used directly for a number of machine learning algorithms, but I could not find a tool which would directly support this. For example Weka only seems to support a global misclassification cost matrix, but not per-instance costs. I think the same is true for SciKit-Learn.

Is there any way to make this work with Weka or SciKit learn or are there other tools / frameworks where this can be done out of the box?

Is there a standard way to represent a training set fro such a scenario in an external file, e.g. in a format similar to ARFF format?

",2,2
311,2016-7-11,2016,7,11,22,4sb0m9,Weights changing during training,https://www.reddit.com/r/MachineLearning/comments/4sb0m9/weights_changing_during_training/,alrojo,1468245221,"How do I interpret weights/bias changing over time?

I.e while training a simple machine translation model with GRUs and attention, my the weights and bias of my decoder layer looks something like this in over time in [Tensorboard](http://i.imgur.com/igGfdnw.png)

The darkest blue color corresponds to 0.5 std. dev. the lightest blue color corresponds to 4.5 std. dev. and nuances are inbetween 0.5 and 4.5.

In my example I notice that the variance of all weights (prefix `W_` or `U_`) tends to stay around the same std. dev. (with the 4.5 std.dev. increasing over time, but it is so far out I don't think it matters). I assume this is what is wanted when training neural networks.

However, some of the bias (prefix `b_`) have a significant increase in variance over the course of the training.
What does this mean?",6,2
312,2016-7-11,2016,7,11,23,4sb2yu,Idea: Trebek as speech data source,https://www.reddit.com/r/MachineLearning/comments/4sb2yu/idea_trebek_as_speech_data_source/,[deleted],1468246122,[deleted],5,0
313,2016-7-11,2016,7,11,23,4sb511,How to improve text generation with LSTMs,https://www.reddit.com/r/MachineLearning/comments/4sb511/how_to_improve_text_generation_with_lstms/,hyperion_agent3011,1468246898,"Hi all,

I've been using keras recently to try to do text generation with twitter data. I've followed the example tutorial online, and have now modified my code to use stateful LSTM layers as opposed to a sliding window method, however, I still am not seeing great results. Had a few questions that I was hoping to get help with.

I guess I also have a few questions regarding workflow in general and any help would be greatly appreciated!

1. Is there a way to add a classifier onto the output of my text generation that can indicate whether a sentence is grammatically correct or not? I've seen that this might be possible with the Stanford sentence parser, but not quite sure.

2. Other than a grid search for hyper parameter tuning, how do I improve my network architecture? There's so many variables to tune (number of layers, layer size, dropout, etc.), and yet it seems like no good framework to tune them.

3. What's a good measure of knowing you've hit a bound on the lowest loss your model is probably going to get. I can see from the loss on each batch that after a while the loss basically hits an asymptote, are there any standards as to when to do early stopping?

4. If anyone is familiar with the stateful LSTM in keras, is there an intuitive explanation for the way you need to batch inputs when using stateful LSTM? I've seen the examples on how to batch the data properly, but I'm lost as to why that is ""proper"".

5. I have about 14,000 tweets right now (small for sure), and each is about 80 characters long. I've been using a batch size of 256, so 54 batches per epoch. I've been training the networks on GCP, with an instance that has high-memory. Currently training for abouth 100 epochs, takes 4-5 hours. How much faster would this be on a desktop with a GPU?

6. Related to the above, with the training times being so long, it's hard to iterate and make changes to see if I can improve the text I'm generating, I guess I can run more examples at once, but any other thoughts here?
",11,3
314,2016-7-12,2016,7,12,0,4sbc34,LSTM converging too slow,https://www.reddit.com/r/MachineLearning/comments/4sbc34/lstm_converging_too_slow/,theendofallend,1468249334,[removed],0,0
315,2016-7-12,2016,7,12,0,4sbcjg,"Is it demonstratedly possible to produce and use Transfer Learning contributions for deep neural nets, based on using different data, different hardware, different NN training and decoding packages for the TL and the final application?",https://www.reddit.com/r/MachineLearning/comments/4sbcjg/is_it_demonstratedly_possible_to_produce_and_use/,datasciguy-aaay,1468249468,"Can it be possible -- that is has anyone demonstrated with real data and real code -- to employ transfer learning by borrowing some existing pretrained deep NN lower layers trained on other bigger data and other better computer hardware on other DNN software package, and then finish the training by freezing the premade lower layers and only making application specific final layers and perform decoding (prediction) on my application specific smaller dataset on my computer hardware on my DNN software package?  It would be crucial for success on my medical cancer image dataset for cancer detection classification problem.  This is because my dataset has too few examples for successful deep DNN on its own, as it only has 10^2 examples in the entire dataset. DNN success requires much more examples than exist within my application. 

Transfer learning offers a new hope for applications like my cancer image dataset with only 10^2 examples in existence.  I hope someone has pretrained lower layers using image data that has at least some or any positive transfer learning possibility for contributing accuracy to my application. I hope the others have trained on much more data, and on much better different hardware, than the data and hardware that I have.  

Say for example they have several banks of GPU but I only have 16 CPU cores total, 0 GPU cores, on which to train final two layers using application-specific cancer image data and on which to decode (make predictions) on new cancer image data.",10,0
316,2016-7-12,2016,7,12,0,4sbehi,Which is the best Machine Learning algorithm for predicting current trends?,https://www.reddit.com/r/MachineLearning/comments/4sbehi/which_is_the_best_machine_learning_algorithm_for/,divya_chopra,1468250089,[removed],2,1
317,2016-7-12,2016,7,12,0,4sbmb3,"Research of Eamonn Keogh, Professor at UCR: Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images",https://www.reddit.com/r/MachineLearning/comments/4sbmb3/research_of_eamonn_keogh_professor_at_ucr_deep/,get4cast,1468252549,,0,0
318,2016-7-12,2016,7,12,1,4sbohz,Machine learning using observable factors and various percentages; association.,https://www.reddit.com/r/MachineLearning/comments/4sbohz/machine_learning_using_observable_factors_and/,Zephandrypus,1468253236,[removed],2,0
319,2016-7-12,2016,7,12,1,4sbrqn,Maximizing expected norm of sum of vectors,https://www.reddit.com/r/MachineLearning/comments/4sbrqn/maximizing_expected_norm_of_sum_of_vectors/,wdlearn,1468254250,"Hi all,

Suppose you have a set of vectors v1, ... vn that can take two values each with certain probabilities. In other words,

&amp;nbsp;

v_i = v_i1 with probability p_i1 
        v_i2 with probability p_i2

&amp;nbsp;

Now, I want to take a subset (of max size m) of those vectors such that the expected norm of the resulting sum vector is maximized. Do you guys have any idea how to do this effectively? For now, I'm resorting to a beam-search approach but computing the expected norm itself is taking too much time since I'm pretty much brute-forcing to compute the expectation.

Thanks!",7,0
320,2016-7-12,2016,7,12,1,4sbuj8,Exponential Distance Summation - What's this called?,https://www.reddit.com/r/MachineLearning/comments/4sbuj8/exponential_distance_summation_whats_this_called/,machine_animal,1468255126,"Hey guys and gals,

I was playing around with classifying points in R^1. I generate two sets of data that overlap some with the +1 points biased in the positive direction and -1 points biased in the negative direction. I then created a classifyer that took a new point z and classified as follows: h(z)=sign(sum(y*exp(-alpha*(x-z)^(2)))). Basically I am counting the number of positive examples versus the number of negative examples weighted by their distance from z in the vicinity of z. Alpha is a parameter that I usually set to one. From this you can derive a decision threshold T by creating a sequence over the domain and calculating the hypothesis at each z and then finding the point at which the negatives go to positives. The new rule is h(z)=+1 if z&gt;T and h(z)=-1 if z&lt;=T.

The classifyer has been working very well both on toy data and on real data. I'm just wondering if you have seen this somewhere.

There is a natural extension to higher dimensional space where h(z)=sign(sum(y*exp(-alpha*norm(x-z)^(2)))). The drawback is in this case you couldn't come up with a simple decision rule like the 1d case. You'd have to keep around the data like in the case of knn.

Anyways, has anyone heard of this?",1,1
321,2016-7-12,2016,7,12,2,4sca7k,"ImageNet was used as source of unsupervised learning for medical image analysis, which improved the ability to detect inner ear infections in medical images.",https://www.reddit.com/r/MachineLearning/comments/4sca7k/imagenet_was_used_as_source_of_unsupervised/,datasciguy-aaay,1468259816,,11,1
322,2016-7-12,2016,7,12,3,4scatl,eBay acquires predictive analytics startup SalesPredict to boost its machine learning | VentureBeat | Commerce,https://www.reddit.com/r/MachineLearning/comments/4scatl/ebay_acquires_predictive_analytics_startup/,shugert,1468260010,,0,2
323,2016-7-12,2016,7,12,3,4scdrp,RSS Feed for new Kaggle Competitions?,https://www.reddit.com/r/MachineLearning/comments/4scdrp/rss_feed_for_new_kaggle_competitions/,orange_robot338,1468260914,Does anyone know where I can obtain an RSS Feed of new Kaggle competitions?,3,0
324,2016-7-12,2016,7,12,3,4scg1u,Accord.Net: Looking for a Bug that Could Help Machines Conquer Humankind,https://www.reddit.com/r/MachineLearning/comments/4scg1u/accordnet_looking_for_a_bug_that_could_help/,Resistor510,1468261593,,0,0
325,2016-7-12,2016,7,12,3,4schoh,Make Your Own Neural Network: IPython Neural Networks on a Raspberry Pi Zero,https://www.reddit.com/r/MachineLearning/comments/4schoh/make_your_own_neural_network_ipython_neural/,hyperqube12,1468262068,,34,175
326,2016-7-12,2016,7,12,4,4sco6u,"Deeplearning4j 4.0 Release Notes: Cudnn wrapper, multi-GPUs, Spark",https://www.reddit.com/r/MachineLearning/comments/4sco6u/deeplearning4j_40_release_notes_cudnn_wrapper/,vonnik,1468264060,,0,1
327,2016-7-12,2016,7,12,4,4scprp,List of 6 Free Deep Learning Courses,https://www.reddit.com/r/MachineLearning/comments/4scprp/list_of_6_free_deep_learning_courses/,jasonb,1468264552,,0,3
328,2016-7-12,2016,7,12,4,4scvi0,Curious example of Deep Learning from macOS Sierra,https://www.reddit.com/r/MachineLearning/comments/4scvi0/curious_example_of_deep_learning_from_macos_sierra/,TonyDavenportUK,1468266375,"Having installed the public beta of macOS Sierra, I am staggered by this - it's not image recognition of a photo that finds a building in it, or something like that.

I have some 27,000 photos and videos in my library - I was curious to see if it would find images of birds - so that's what I typed in to the search box. While I was doing that it made two suggestions - birds and birdhouse. Out of curiosity I clicked the birdhouse one. There are two things in there - a photo and a video.

The photo is understandable - it's a picture of a birdhouse in a shop. Any text search also would find ""birdhouse"".

The video, however, is a nest that my friend's two birds made in a cupboard. How did Apple know that it was a birdhouse?!

There are no birds in the video - no bird toys to recognise, and no mention of birds in the audio track. I didn't take any images of her birds that day (and had not done for some months) and none following that video. The only thing I can think is that at that location when I have taken photos in the past, I usually take images of birds.

Here's a link to the stuff: https://www.icloud.com/sharedalbum/#B0f5idkMw2xrpU

Any answers or theories would be welcome!",8,21
329,2016-7-12,2016,7,12,5,4sd0z9,"EU regulations on algorithmic decision-making and a ""right to explanation""",https://www.reddit.com/r/MachineLearning/comments/4sd0z9/eu_regulations_on_algorithmic_decisionmaking_and/,tangerinemike,1468268129,,38,13
330,2016-7-12,2016,7,12,6,4sde0a,Coordinates in a Neural Network,https://www.reddit.com/r/MachineLearning/comments/4sde0a/coordinates_in_a_neural_network/,Nater5000,1468272485,"Hey everybody, 

I've been recently working on a little project to create a simple neural network to price houses in my area. I have access to a lot of buying/selling information via a MLS, so that shouldn't be an issue. 

Now, there are certainly a number of ways to handle location of a home and how that effects it's value, but im curious to know how this is normally/effectively done. 

Naively, I could use a simple classification of discrete areas to do this (like by Township, zip code, school district, etc), but I wanted to get more detailed and give the NN the actual coordinates of a home (or something equivalent) to get as much precision as possible. 

Getting the coordinate information is easy via various Geoapis that are available, such as Google's. And then normalizing this data is also straight forward since the area I'm considering is well defined.

Naively, again, I figured just feeding these coordinates into a simple feed forward system would allow for it to find patterns in pricing and location. But I'm not convinced this would work well. Since location is dependent on both the latitude and longitude of a home, it seems like applying weights to each separately would have unwanted side affects. I mean, a location at some (x0, y0) may be more valuable than a location at (x0, y1), but im afraid the network may simply figure that that sharing the same x coordinate (or longitude) means they are both valuable (or not). 

Obviously there are other ways of handling this (such as giving it distance to some epicenter(s) or something along these lines), but I'd rather keep the data as non-derivative as possible.

Is this approach valid/wise? Or are there better ways of handling location in a NN? I can't seem to any information regarding this, but I suppose I also don't even know what terms might be involved in this case. 

What do you guys think? Is there any papers involving this kind of thing?",9,1
331,2016-7-12,2016,7,12,7,4sdllg,Language translation by feeding an AI with two translations of the same text. Is it possible?,https://www.reddit.com/r/MachineLearning/comments/4sdllg/language_translation_by_feeding_an_ai_with_two/,b_r0m,1468275129,[removed],1,0
332,2016-7-12,2016,7,12,7,4sdrvs,Deepmind learns to play Montezuma's Revenge,https://www.reddit.com/r/MachineLearning/comments/4sdrvs/deepmind_learns_to_play_montezumas_revenge/,sigmoidp,1468277407,,17,25
333,2016-7-12,2016,7,12,8,4sdvof,"Switched GPUS, now having cuDNN errors.",https://www.reddit.com/r/MachineLearning/comments/4sdvof/switched_gpus_now_having_cudnn_errors/,dimmtree,1468278769,[removed],0,0
334,2016-7-12,2016,7,12,8,4se00z,Markov chatbot based on 500k reddit comments,https://www.reddit.com/r/MachineLearning/comments/4se00z/markov_chatbot_based_on_500k_reddit_comments/,biomimic,1468280410,,12,3
335,2016-7-12,2016,7,12,11,4seph8,one strip razor barbed wire machine,https://www.reddit.com/r/MachineLearning/comments/4seph8/one_strip_razor_barbed_wire_machine/,susanwang333999,1468290081,,0,1
336,2016-7-12,2016,7,12,11,4sepv2,Having a hard time reading machine learning textbooks.,https://www.reddit.com/r/MachineLearning/comments/4sepv2/having_a_hard_time_reading_machine_learning/,thefocusoffear,1468290223,[removed],1,0
337,2016-7-12,2016,7,12,11,4sesjb,"Course on kadenze.com on ""Creative Applications of Deep Learning with TensorFlow""",https://www.reddit.com/r/MachineLearning/comments/4sesjb/course_on_kadenzecom_on_creative_applications_of/,pkmital,1468291248,,6,28
338,2016-7-12,2016,7,12,11,4sesmm,Comparing the Top Five Computer Vision APIs,https://www.reddit.com/r/MachineLearning/comments/4sesmm/comparing_the_top_five_computer_vision_apis/,goberoi,1468291278,,0,1
339,2016-7-12,2016,7,12,13,4sf49o,[1607.03085] Recurrent Memory Array Structures,https://www.reddit.com/r/MachineLearning/comments/4sf49o/160703085_recurrent_memory_array_structures/,kmrocki,1468296292,,10,20
340,2016-7-12,2016,7,12,13,4sf6d1,Why Machine Learning Beginners Shouldn't Avoid the Math,https://www.reddit.com/r/MachineLearning/comments/4sf6d1/why_machine_learning_beginners_shouldnt_avoid_the/,[deleted],1468297238,[deleted],0,1
341,2016-7-12,2016,7,12,14,4sfez2,"Google's Machine Learning Buy, Alteryx Updates: Big Data Roundup",https://www.reddit.com/r/MachineLearning/comments/4sfez2/googles_machine_learning_buy_alteryx_updates_big/,coonsdeacon,1468301270,,0,0
342,2016-7-12,2016,7,12,16,4sfvil,Named entity recognition using NLTK in python,https://www.reddit.com/r/MachineLearning/comments/4sfvil/named_entity_recognition_using_nltk_in_python/,vaibhavs10,1468309910,"Hey!

I am looking for a way to train the NLTK chunker using my own text, 
For e.g., If I want to get all the car manufacturers name out of a text, It won't be possible from the existing corpus.
can someone point me to the right direction?",3,1
343,2016-7-12,2016,7,12,17,4sfya6,Wire Straightening and Cutting Machines Manufacturer,https://www.reddit.com/r/MachineLearning/comments/4sfya6/wire_straightening_and_cutting_machines/,logosbysan,1468311519,,0,1
344,2016-7-12,2016,7,12,17,4sg086,Welding Electrode Plant,https://www.reddit.com/r/MachineLearning/comments/4sg086/welding_electrode_plant/,logosbysan,1468312634,,0,1
345,2016-7-12,2016,7,12,17,4sg1rj,"Newsletter: 5 weeks of AI tech news, research papers and VC investments/exits",https://www.reddit.com/r/MachineLearning/comments/4sg1rj/newsletter_5_weeks_of_ai_tech_news_research/,nb410,1468313583,,0,1
346,2016-7-12,2016,7,12,18,4sg775,Unpooling operation in Tensorflow?,https://www.reddit.com/r/MachineLearning/comments/4sg775/unpooling_operation_in_tensorflow/,Nole_Ksum,1468316754,"Has anybody managed to implement the Unpooling operation in Tensorflow? 

I've tried two ways, which unfortunately do not work due to non existing gradient operations. My first try was using the gradient of the max pool operation and then multiplying by the input to the max pooling op. However the gradient for max pooling does itself not have a defined gradient in Tensorflow. 

My second attempt tried to use the max_pool_with_argmax function and then using scatter_update in combination with the argmax indices to get the unpooling, which works for the forward pass, but again scatter_update does not have a gradient defined. I can't think of another way to unpool and I really need this operation to build a Convolutional Autoencoder. I would be very grateful for any answers.


    ",0,1
347,2016-7-12,2016,7,12,18,4sg7b0,[1607.01759] Bag of Tricks for Efficient Text Classification,https://www.reddit.com/r/MachineLearning/comments/4sg7b0/160701759_bag_of_tricks_for_efficient_text/,[deleted],1468316821,[deleted],4,0
348,2016-7-12,2016,7,12,18,4sg7su,Welding Electrode Machinery,https://www.reddit.com/r/MachineLearning/comments/4sg7su/welding_electrode_machinery/,logosbysan,1468317128,,0,1
349,2016-7-12,2016,7,12,18,4sg8hc,Unpooling operation in Tensorflow?,https://www.reddit.com/r/MachineLearning/comments/4sg8hc/unpooling_operation_in_tensorflow/,TheInvisibleHand89,1468317500,"Has anybody managed to implement the Unpooling operation in Tensorflow?

I've tried two ways, which unfortunately do not work due to non existing gradient operations. My first try was using the gradient of the max pool operation and then multiplying by the input to the max pooling op. However the gradient for max pooling does itself not have a defined gradient in Tensorflow.

My second attempt tried to use the max_pool_with_argmax function and then using scatter_update in combination with the argmax indices to get the unpooling, which works for the forward pass, but again scatter_update does not have a gradient defined. 

I can't think of another way to unpool and I really need this operation to build a Convolutional Autoencoder. I would be very grateful for any answers.",3,3
350,2016-7-12,2016,7,12,19,4sge4e,Online Certification Course On Machine Learning and Natural Language Processing,https://www.reddit.com/r/MachineLearning/comments/4sge4e/online_certification_course_on_machine_learning/,[deleted],1468320676,[deleted],0,1
351,2016-7-12,2016,7,12,21,4sgnqk,relationship neural networks and algorithms,https://www.reddit.com/r/MachineLearning/comments/4sgnqk/relationship_neural_networks_and_algorithms/,sjap,1468325373,"Question about theory: Can a trained neural network be converted into traditional algorithmic code? 

So I have a multi-layer network that does classification. Can this network's performance be transformed into a traditional line-by-line program (10 int x=3; 20 print x; 30 stop)?

Forget for a moment why you would want to to this, the question is whether it is possible in principle. 

If there is any literature on this topic I would be delighted to have some pointers.",5,2
352,2016-7-12,2016,7,12,21,4sgsn9,Drawing CNN architectures.,https://www.reddit.com/r/MachineLearning/comments/4sgsn9/drawing_cnn_architectures/,FreeWildbahn,1468327565,"Hi, i'm currently writing a a small document with latex. It will include a plot of a CNN architecture. 
Currently I'm wondering what is the best way to create this plot with minimal effort. I found a pretty good tikz example: http://www.texample.net/tikz/examples/neural-network/

But maybe there is a better solution? What are you using?",8,6
353,2016-7-12,2016,7,12,22,4sgw2e,New Deep Neural Network paradigm - tiny scale General AI demonstration,https://www.reddit.com/r/MachineLearning/comments/4sgw2e/new_deep_neural_network_paradigm_tiny_scale/,informationflow,1468328948,,0,1
354,2016-7-12,2016,7,12,22,4sgwr4,[1607.01668] Tensor Decomposition for Signal Processing and Machine Learning,https://www.reddit.com/r/MachineLearning/comments/4sgwr4/160701668_tensor_decomposition_for_signal/,bdamos,1468329221,,11,54
355,2016-7-12,2016,7,12,22,4sgyut,Autonomous GoPiGo driven by Retrained Inception 3 on Tensorflow,https://www.reddit.com/r/MachineLearning/comments/4sgyut/autonomous_gopigo_driven_by_retrained_inception_3/,[deleted],1468330048,[deleted],0,1
356,2016-7-12,2016,7,12,22,4sh1r7,The Mathematics of Machine Learning - Data Science Africa,https://www.reddit.com/r/MachineLearning/comments/4sh1r7/the_mathematics_of_machine_learning_data_science/,[deleted],1468331124,[deleted],0,1
357,2016-7-12,2016,7,12,22,4sh21j,The Multiworld Testing Decision Service,https://www.reddit.com/r/MachineLearning/comments/4sh21j/the_multiworld_testing_decision_service/,cesarsalgado,1468331242,,0,5
358,2016-7-12,2016,7,12,22,4sh3dr,A Yahoo-Flickr Grand Challenge on Tag and Caption Prediction!,https://www.reddit.com/r/MachineLearning/comments/4sh3dr/a_yahooflickr_grand_challenge_on_tag_and_caption/,rgoliax,1468331729,,3,1
359,2016-7-12,2016,7,12,22,4sh3gj,https://github.com/zxzhijia/GoPiGo-Driven-by-Tensorflow,https://www.reddit.com/r/MachineLearning/comments/4sh3gj/httpsgithubcomzxzhijiagopigodrivenbytensorflow/,[deleted],1468331753,[deleted],0,1
360,2016-7-12,2016,7,12,23,4sh4s4,The Mathematics of Machine Learning by Wale Akinfaderin,https://www.reddit.com/r/MachineLearning/comments/4sh4s4/the_mathematics_of_machine_learning_by_wale/,admiral93,1468332254,,0,1
361,2016-7-12,2016,7,12,23,4sh5cn,Autonomous Robot Driven by Tensorflow CNN,https://www.reddit.com/r/MachineLearning/comments/4sh5cn/autonomous_robot_driven_by_tensorflow_cnn/,zxzhijia,1468332438,,6,29
362,2016-7-12,2016,7,12,23,4sh7ce,Learning Theano vs TensorFlow,https://www.reddit.com/r/MachineLearning/comments/4sh7ce/learning_theano_vs_tensorflow/,centau1,1468333108,"I'm pretty new to neural networks and I want to learn one of the low level DL packages and I need to learn them fast. Which one of Theano or Tensorflow is faster to learn and use?

I see that many people just port Theano solutions to TensorFlow. So, my guess is that they should be more or less similar. Which one do you recommend that I spend time learning first?",10,3
363,2016-7-12,2016,7,12,23,4sh8gn,Ranking a set of classifiers based on metrics with differing units,https://www.reddit.com/r/MachineLearning/comments/4sh8gn/ranking_a_set_of_classifiers_based_on_metrics/,zerribert,1468333499,"Note: I posted this question to [stackoverflow](http://stats.stackexchange.com/questions/223359/ranking-a-set-of-classifiers-based-on-metrics-with-differing-units) as well. 


**Background**


I'm working on a system that trains an arbitrarily large amount of classifiers (e.g. Support Vector Machines, k-Neighbors Classifiers, Neural Networks, Decision Trees, ...) on the same training set and collects a bunch of performance metrics for each model. Now, most of these are your standard run-of-the-mill metrics like precision, recall, overall accuracy and all that, but some are more complex (or should I say ""different""?), for example:


* Runtime- and memory complexity of the learning algorithm
* Number of performed preprocessing/dimensionality reduction steps
* The absolute amount of time it took to train the model on the training set
* The average amount of time it took to classify a single data point from the test set


**My goal**


I want to find a good way of ranking these models based on user-specified weights for a subset of the aforementioned performance metrics. Simplified example:

    Model 1:
        * Precision: 92%
        * no. of preprocessing steps: 2

    Model 2
        * Precision: 89%
        * no. of preprocessing steps: 1

    Model 3
        * Precision: 72%
        * no. of preprocessing steps: 2


If a user's goal was to find the model that was least ""complex"" while still achieving reasonable precision, they would likely assign a higher weight to the ""no. of preprocessing steps"" attribute and see which model gets ranked highest (probably model 2, but it really depends on the concrete values of the weights of course).


So, in short, I am faced with a so-called Multiple-criteria decision-making (MCDM) problem, and I need to solve it. I've looked into several [MCDM methods](https://en.wikipedia.org/wiki/Multiple-criteria_decision_analysis#MCDM_methods) and figured it would be a good idea to start off simple by ranking the models using the weighted sum of all their metrics. But I'm afraid it's not that straightforward...


**Problems**


The first problem I've been facing here is that the metrics have different meanings in terms of which values are actually ""better"" than others. In the above example, higher accuracy (""more is better"") with less preprocessing steps (""less is better"") would form a desirable combination. But how should the ranking algorithm know that less is better in terms of the number of preprocessing steps? One obvious way to deal with this would be to simply negate the values for all metrics where less is better. That way the general assumption ""more is better"" holds for all metrics in question no matter what, ultimately causing the weighted average to be higher for more desirable models. Does this approach have any caveats? I guess not (please let me know if it does)...
Now, on to the ""real"" problem:


The metrics come in different units, and some of the metrics are theoretically unbounded (e.g. the number of preprocessing steps), so it becomes sort of hard to scale them to a fixed range. I figured, however, that since I'm only trying to find the best model relative to the other models in the set (as opposed to finding a ""universal"" score/rank) I could simply scale all metrics to [0,1] by diving each metric by the sum of all of its occurrences in the set. That way the ""more is better"" assumption still holds, but now all metrics will also have the same unit (%). I don't know why, but to me this sort of feels like the kind of solution that seems plausible on the surface but can backfire in some cases, but I don't really know why. I'm not that great at math or stats so I lack the mathematical expertise (or at least intuition) to come up with a reason or a concrete example where this may be the case.
I'd be really glad if anyone could help me assess the solutions I came up with for both problems.


Note: I've already looked into MCDM methods that aim to eliminate the need for a common unit among the metrics, namely the [weighted product model](https://en.wikipedia.org/wiki/Weighted_product_model). Here, however, all the metrics need to be greater than or equal to one. Again, I could just scale all metrics to the range [1,2] or similar, but the question whether or not that's a good idea still stands.",3,3
364,2016-7-12,2016,7,12,23,4sh98t,Artificial Neural Networks  Part 1: The XOr Problem,https://www.reddit.com/r/MachineLearning/comments/4sh98t/artificial_neural_networks_part_1_the_xor_problem/,[deleted],1468333767,[deleted],0,1
365,2016-7-12,2016,7,12,23,4shci9,Hadoopin' with Louis,https://www.reddit.com/r/MachineLearning/comments/4shci9/hadoopin_with_louis/,[deleted],1468334860,[deleted],2,0
366,2016-7-13,2016,7,13,0,4shg5p,How to build a simple walking robot with NN?,https://www.reddit.com/r/MachineLearning/comments/4shg5p/how_to_build_a_simple_walking_robot_with_nn/,Simon_Ger,1468336043,[removed],4,0
367,2016-7-13,2016,7,13,0,4shm39,From not working to neural networking: the resurrection of AI,https://www.reddit.com/r/MachineLearning/comments/4shm39/from_not_working_to_neural_networking_the/,jonfla,1468337888,,0,0
368,2016-7-13,2016,7,13,1,4shr71,"(Call for Abstracts) Action and Anticipation for Visual Learning Workshop, ECCV, Amsterdam, October 2016 (repost after schedule update - July 25 deadline)",https://www.reddit.com/r/MachineLearning/comments/4shr71/call_for_abstracts_action_and_anticipation_for/,dineshjayaraman,1468339418,,0,3
369,2016-7-13,2016,7,13,2,4siany,Manufacturers must learn to behave more like tech firms - interesting read,https://www.reddit.com/r/MachineLearning/comments/4siany/manufacturers_must_learn_to_behave_more_like_tech/,[deleted],1468345245,[deleted],0,0
370,2016-7-13,2016,7,13,3,4sigtr,Dask and Scikit-Learn for Model Parallelism,https://www.reddit.com/r/MachineLearning/comments/4sigtr/dask_and_scikitlearn_for_model_parallelism/,smerity,1468347125,,0,18
371,2016-7-13,2016,7,13,4,4siwss,[Question] Change text based on command nlp,https://www.reddit.com/r/MachineLearning/comments/4siwss/question_change_text_based_on_command_nlp/,cvikasreddy,1468352114,[removed],3,0
372,2016-7-13,2016,7,13,5,4sj7be,R Question on neuralnet,https://www.reddit.com/r/MachineLearning/comments/4sj7be/r_question_on_neuralnet/,[deleted],1468355512,[deleted],2,0
373,2016-7-13,2016,7,13,5,4sjari,"CNN models that are pre-trained using computer vision databases (e.g., Imagenet) are useful in medical image applications, despite the signi cant di erences in image appearance.",https://www.reddit.com/r/MachineLearning/comments/4sjari/cnn_models_that_are_pretrained_using_computer/,datasciguy-aaay,1468356620,,7,8
374,2016-7-13,2016,7,13,7,4sjmvx,"Building a Convolutional RNN in Keras, livestream",https://www.reddit.com/r/MachineLearning/comments/4sjmvx/building_a_convolutional_rnn_in_keras_livestream/,vanboxel,1468360807,,2,2
375,2016-7-13,2016,7,13,8,4sk6fx,keras SIMPLE RNN help?,https://www.reddit.com/r/MachineLearning/comments/4sk6fx/keras_simple_rnn_help/,maximus12793,1468367985,[removed],0,0
376,2016-7-13,2016,7,13,9,4sk7p4,Feature Engineering: Data scientist's Secret Sauce !,https://www.reddit.com/r/MachineLearning/comments/4sk7p4/feature_engineering_data_scientists_secret_sauce/,abdsc,1468368466,,0,1
377,2016-7-13,2016,7,13,13,4slff2,[1607.03474] Recurrent Highway Networks,https://www.reddit.com/r/MachineLearning/comments/4slff2/160703474_recurrent_highway_networks/,downtownslim,1468385992,,24,53
378,2016-7-13,2016,7,13,14,4slh37,10 Papers from ICML and CVPR,https://www.reddit.com/r/MachineLearning/comments/4slh37/10_papers_from_icml_and_cvpr/,frustrated_lunatic,1468386750,,3,45
379,2016-7-13,2016,7,13,16,4slw44,An Introduction to Model-Based Machine Learning,https://www.reddit.com/r/MachineLearning/comments/4slw44/an_introduction_to_modelbased_machine_learning/,Emaasit,1468394453,,0,3
380,2016-7-13,2016,7,13,17,4sm50m,[Request] Good sources to learn Fuzzy - Association Rule Mining,https://www.reddit.com/r/MachineLearning/comments/4sm50m/request_good_sources_to_learn_fuzzy_association/,[deleted],1468399344,[deleted],0,0
381,2016-7-13,2016,7,13,18,4sm998,[1607.03316] Separating Answers from Queries for Neural Reading Comprehension  SOTA on DeepMind's and Facebook's cloze-style Q&amp;A tasks,https://www.reddit.com/r/MachineLearning/comments/4sm998/160703316_separating_answers_from_queries_for/,_rockt,1468401610,,2,2
382,2016-7-13,2016,7,13,18,4smbca,Google Announces New Machine Learning Center,https://www.reddit.com/r/MachineLearning/comments/4smbca/google_announces_new_machine_learning_center/,51zero,1468402708,,2,3
383,2016-7-13,2016,7,13,19,4smg6b,Where to get data,https://www.reddit.com/r/MachineLearning/comments/4smg6b/where_to_get_data/,flavouredesign,1468405281,[removed],2,0
384,2016-7-13,2016,7,13,21,4smvl3,A tool for finding + launching the cheapest AWS GPU spot instances for ML.,https://www.reddit.com/r/MachineLearning/comments/4smvl3/a_tool_for_finding_launching_the_cheapest_aws_gpu/,Jakobovski,1468412692,,24,33
385,2016-7-13,2016,7,13,22,4sn1pd,Data set of simple projects written in C,https://www.reddit.com/r/MachineLearning/comments/4sn1pd/data_set_of_simple_projects_written_in_c/,dorlevy,1468415187,"Hello,

In my research I need a lot of source code samples, can anyone point me to a good data set?


Downloading many projects from github is not a good option for me since I have specific criteria the data should meet:

1- The code was written by a human (was not automatically generated).

2- The programming language can be compiled (preferably C language but can be any similar language that compiles).

3- The projects should be simple, i.e. have a small number of relatively short functions. A good example is a program that computes the gcd() of two numbers and is built of a main() function plus a gcd() function.

for example cs course homework solutions submitted by students will be great.

Thanks!",4,0
386,2016-7-13,2016,7,13,22,4sn5jp,Not sure if I'm splitting my data set correctly,https://www.reddit.com/r/MachineLearning/comments/4sn5jp/not_sure_if_im_splitting_my_data_set_correctly/,lostwhitewalker,1468416683,"I downloaded a data set from this [site](http://ai.stanford.edu/~btaskar/ocr/). It has the following fields: 

*id: each letter is assigned a unique integer id

*letter: a-z

*next_id: id for next letter in the word, -1 if last letter

*word_id: each word is assigned a unique integer id (not used)

*position: position of letter in the word (not used)

*fold: 0-9 -- cross-validation fold

*p_0_0...p_0_7....p_15_7: 0/1 -- value of pixel in row i, column j

*this is the code I wrote to split the data.

    def load_data ():
    df = pd.read_csv('C:\\Users\\small\\Desktop\\DataSet\\main_dataset.csv', header = 0)
    training, test = train_test_split(df, test_size = 0.80)
    print(training)

this is the result I get back. I am only showing five rows to make it easier to read.

              id letter  next_id  word_id  position  fold  p_0_0  p_0_1  p_0_2    
    21273  21274      g    21275     2946         3     5      0      0      1   
    13980  13981      v    13982     1841         2     8      0      0      0   
    34209  34210      e    34211     4891         8     6      0      0      0   
    24282  24283      j    24284     3382         3     9      0      0      0   
    40934  40935      t    40936     5631         9     1      0      0      0   

Clearly it is not splitting the data properly.  How do I split this data set properly, so I can use it to train neural network. I am using [this](http://neuralnetworksanddeeplearning.com/chap1.html) tutorial to create my neural network.  The tutorial has the following code to create training data set. Note this tutorial is for digit, uses the dataset from [this site] (http://yann.lecun.com/exdb/mnist/). I am creating a letter recognition.

    def load_mnist():
    f = gzip.open('C:\\Users\\small\\Desktop\\DataSet\\mnist.pkl.gz', 'rb')
    training_data, validation_data, test_data = cPickle.load(f, encoding='latin1')
    f.close()
    print(training_data)

When I print training_data, I get the following output:

    (array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],
       [ 0.,  0.,  0., ...,  0.,  0.,  0.],
       [ 0.,  0.,  0., ...,  0.,  0.,  0.],
       ..., 
       [ 0.,  0.,  0., ...,  0.,  0.,  0.],
       [ 0.,  0.,  0., ...,  0.,  0.,  0.],
       [ 0.,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32), array([5, 0, 4, ..., 8, 4, 8], dtype=int64))

As you can see the training data is not this same as mine. 
",1,0
387,2016-7-13,2016,7,13,22,4sn6r6,[1606.06737v2] Critical Behavior from Deep Dynamics: A Hidden Dimension in Natural Language (theoretical result on why Markov chains don't work as well as LSTM's),https://www.reddit.com/r/MachineLearning/comments/4sn6r6/160606737v2_critical_behavior_from_deep_dynamics/,dunnowhattoputhere,1468417135,,14,80
388,2016-7-13,2016,7,13,23,4snfp8,iOS framework for on-device automated speech recognition,https://www.reddit.com/r/MachineLearning/comments/4snfp8/ios_framework_for_ondevice_automated_speech/,OgnjenTodic,1468420198,[removed],0,1
389,2016-7-13,2016,7,13,23,4snfvn,Find a location of an acoustic signal in another big wav file,https://www.reddit.com/r/MachineLearning/comments/4snfvn/find_a_location_of_an_acoustic_signal_in_another/,JustARandomNoob165,1468420257,"Hello,

I am trying to locate a short waveform(~1-3sec) in another big wav file. What kind of approach would be the best to start? I know it should be there(the short fragments were cut from the movie file). I am trying now to compare the pattern with the source file using mean squared error over MFCCs in a sliding window manner. But currently the results are noisy.",13,2
390,2016-7-13,2016,7,13,23,4snkfj,Machine Learning Courses for Beginners,https://www.reddit.com/r/MachineLearning/comments/4snkfj/machine_learning_courses_for_beginners/,[deleted],1468421725,[deleted],0,1
391,2016-7-14,2016,7,14,0,4snu2a,char-RNN experiments: generating text imitating multiple authors,https://www.reddit.com/r/MachineLearning/comments/4snu2a/charrnn_experiments_generating_text_imitating/,gwern,1468424915,,2,9
392,2016-7-14,2016,7,14,1,4snzpj,"TensorFlow 0.9 AMI with Keras, cuDNN 5, and 30-40% faster",https://www.reddit.com/r/MachineLearning/comments/4snzpj/tensorflow_09_ami_with_keras_cudnn_5_and_3040/,mtweak,1468426797,,0,2
393,2016-7-14,2016,7,14,1,4so03p,Big Sur: A Closer Look at Facebook's Machine Learning Hardware,https://www.reddit.com/r/MachineLearning/comments/4so03p/big_sur_a_closer_look_at_facebooks_machine/,1SockChuck,1468426914,,0,1
394,2016-7-14,2016,7,14,2,4so7ty,Data sets from stack overflow,https://www.reddit.com/r/MachineLearning/comments/4so7ty/data_sets_from_stack_overflow/,3eyedravens,1468429390,,1,12
395,2016-7-14,2016,7,14,2,4sodka,2 chatbots are training one another by having a conversation - based on markov + word2vec,https://www.reddit.com/r/MachineLearning/comments/4sodka/2_chatbots_are_training_one_another_by_having_a/,biomimic,1468431179,,26,39
396,2016-7-14,2016,7,14,3,4sojs6,What is Intelligence?,https://www.reddit.com/r/MachineLearning/comments/4sojs6/what_is_intelligence/,numenta,1468433186,,10,0
397,2016-7-14,2016,7,14,3,4soqwj,What is the difference between using an unconstrained optimization solver like octave's fminunc and doing gradient descent?,https://www.reddit.com/r/MachineLearning/comments/4soqwj/what_is_the_difference_between_using_an/,[deleted],1468435435,[removed],2,1
398,2016-7-14,2016,7,14,4,4sowxm,Ask ML: What are some of the best algorithms for predictive sales lead scoring and prioritization?,https://www.reddit.com/r/MachineLearning/comments/4sowxm/ask_ml_what_are_some_of_the_best_algorithms_for/,Pipvault,1468437370,[removed],2,0
399,2016-7-14,2016,7,14,4,4sox8q,Dumb question on ISLR chapter six labs,https://www.reddit.com/r/MachineLearning/comments/4sox8q/dumb_question_on_islr_chapter_six_labs/,RowdyWalrus,1468437469,"I've been really enjoying the ISLR textbook so far, and I'm currently working my way through chapter six. I realize that I am very confused by the process used in lab 3 of this chapter. 

First, they use the PCR function's cross validation option and the entire training data set to calculate the optimal number of principle components. Great! All set (I thought...)

Next, they ""perform PCR on the training data and evaluate its test set performance."" I'm confused because I thought that cross-validation (which they did first) is basically a better version of doing exactly this! To make me even more confused, they go on to say they that with the training/test set approach, they get the ""lowest cross-validation error"" when 7 components are used. It seems like they are using a validation set together with cross validation?

I'm sure I am missing something very obvious, so I apologize if this is a very silly question. Also, I understand that this may not be the best forum for my question - let me know if this should be posted on stack overflow or something. 

Thanks!


textbook: http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Fourth%20Printing.pdf lab in question is on page 256-258
",7,0
400,2016-7-14,2016,7,14,5,4sp71m,Women are far more likely to be Machine Learning Developers than men,https://www.reddit.com/r/MachineLearning/comments/4sp71m/women_are_far_more_likely_to_be_machine_learning/,logicsattva,1468440525,,10,0
401,2016-7-14,2016,7,14,5,4spgku,"If you havent been able to regularly keep track of important developments in deep learning, you might want to bookmark this page",https://www.reddit.com/r/MachineLearning/comments/4spgku/if_you_havent_been_able_to_regularly_keep_track/,[deleted],1468443592,[deleted],0,1
402,2016-7-14,2016,7,14,6,4spii3,"Hand-curated summarization of Deep Learning breakthroughs,people and companies. Be up to date",https://www.reddit.com/r/MachineLearning/comments/4spii3/handcurated_summarization_of_deep_learning/,[deleted],1468444238,[deleted],0,1
403,2016-7-14,2016,7,14,6,4spqj7,"Philosophical question about how to interpret loss functions, sigmoid function, etc.",https://www.reddit.com/r/MachineLearning/comments/4spqj7/philosophical_question_about_how_to_interpret/,RockyMcNuts,1468447075,"I've taken the Andrew Ng class, the Stanford Statistical Learning class, have a couple of models in production in R, sklearn, tensorflow that work pretty well, and I have a question that always bothers me a little.

What is the relationship between the sigmoid, the cross-entropy function, the sample, and the probability from the 'population distribution'.

In statistics, you can say something like, if the data follows the assumptions of linear regression, the prediction will agree with the population distribution, subject to a prediction error which will follow a normal distribution described by the standard error.

If I create some random data and apply a logistic regression or e.g. a neural network classifier with one hidden layer, minimize the cross-entropy function, and the output for an observation with a vector of predictors is say 0.8, what would be the similar statement? 

i.e. if the data follows X assumptions, the actual probabilities in a sample will match the predicted probability give or take X. What is that 0.8, or the cross-entropy loss function telling me?

I sort of feel like machine learning is 'street-fighting statistics' and the point is to use what works and use cross-validation and test sets to estimate the real-world performance, and not worry about proofs and closed-form solutions and error estimates like statistics does. 

But I wonder about things like, if I have a 1-hidden layer NN, what forms of classification boundaries will it do well on, what's the relationship between number of data points, the form of the classification boundary and how big/how many layers should I use, are those sigmoids and cross-entropy functions arbitrary, or do they have a clear interpretation when the data follows certain assumptions.

hope that makes sense. If there are any references that talk about how to interpret the output and loss function, how to select your model based on what you may know about the underlying data, besides trial and error and cross-validation that would be really helpful!",5,0
404,2016-7-14,2016,7,14,7,4spvz6,Automation and Anxiety: The Impact on Jobs,https://www.reddit.com/r/MachineLearning/comments/4spvz6/automation_and_anxiety_the_impact_on_jobs/,ogglethorpe,1468449006,,0,0
405,2016-7-14,2016,7,14,7,4spwda,How would you determine if someone is skilled at ML?,https://www.reddit.com/r/MachineLearning/comments/4spwda/how_would_you_determine_if_someone_is_skilled_at/,Pieranha,1468449155,"I'm helping a friend find good candidates within ML for some positions that are opening up with his startup. In that context I would love some input on the following:

1. What makes a good ML practitioner?
2. How would you spot those characteristics based on a resume and a brief interview session?

Note that these positions are more application-oriented than research-oriented.",59,50
406,2016-7-14,2016,7,14,7,4spy68,Technology Is Only Making Social Skills More Important,https://www.reddit.com/r/MachineLearning/comments/4spy68/technology_is_only_making_social_skills_more/,ogglethorpe,1468449805,,0,0
407,2016-7-14,2016,7,14,10,4sqrrm,Tuning Torch on CPUs,https://www.reddit.com/r/MachineLearning/comments/4sqrrm/tuning_torch_on_cpus/,nickl,1468460941,,0,5
408,2016-7-14,2016,7,14,11,4sqwvf,Introduction to Deep Learning for Image Recognition : SciPy US 2016 Tutorial,https://www.reddit.com/r/MachineLearning/comments/4sqwvf/introduction_to_deep_learning_for_image/,rousemaga,1468462902,,0,7
409,2016-7-14,2016,7,14,11,4sr1wf,"how to sample ""partitioning arrangement"" and ""parameter"" in Dirichlet process?",https://www.reddit.com/r/MachineLearning/comments/4sr1wf/how_to_sample_partitioning_arrangement_and/,[deleted],1468464830,[deleted],0,0
410,2016-7-14,2016,7,14,12,4sr3fl,nine strip razor wire machine,https://www.reddit.com/r/MachineLearning/comments/4sr3fl/nine_strip_razor_wire_machine/,susanwang333999,1468465435,,0,1
411,2016-7-14,2016,7,14,12,4srajy,Movers and Shakers in Deep Learning month by month,https://www.reddit.com/r/MachineLearning/comments/4srajy/movers_and_shakers_in_deep_learning_month_by_month/,[deleted],1468468397,[deleted],0,2
412,2016-7-14,2016,7,14,13,4srbrc,ELI5: What are fc6-fc7 layers?,https://www.reddit.com/r/MachineLearning/comments/4srbrc/eli5_what_are_fc6fc7_layers/,de_spot,1468468907,[removed],0,0
413,2016-7-14,2016,7,14,13,4srd25,Problem set and solution for CS229 stanford 2015 or spring 2016?,https://www.reddit.com/r/MachineLearning/comments/4srd25/problem_set_and_solution_for_cs229_stanford_2015/,huyhcmut,1468469466,I'm searching for the latest version(fall 2015 or spring 2016) of problem sets and solutions for them in the course CS229: machine learning. The professor did not public these material on the web page course . http://cs229.stanford.edu/materials.html. I want to ask anyone else had taken this class this term or fall2015 term to give me the materials( problem and solution). Thank you very much :D :D,2,15
414,2016-7-14,2016,7,14,16,4ss2np,How would you hide an easter egg in a neural network?,https://www.reddit.com/r/MachineLearning/comments/4ss2np/how_would_you_hide_an_easter_egg_in_a_neural/,wanrh,1468481789,,11,1
415,2016-7-14,2016,7,14,17,4ss88u,Bayesian machine learning,https://www.reddit.com/r/MachineLearning/comments/4ss88u/bayesian_machine_learning/,pmigdal,1468484986,,0,10
416,2016-7-14,2016,7,14,17,4ss8cw,Collaborative effort to compile summaries of must read papers in deep learning,https://www.reddit.com/r/MachineLearning/comments/4ss8cw/collaborative_effort_to_compile_summaries_of_must/,perceptron01,1468485049,,2,1
417,2016-7-14,2016,7,14,19,4ssk6u,Is it true that R programming is dying?,https://www.reddit.com/r/MachineLearning/comments/4ssk6u/is_it_true_that_r_programming_is_dying/,rajshre,1468491448,,32,1
418,2016-7-14,2016,7,14,19,4ssm4z,"Benchmarks for popular CNN models (TITAN X vs 1080, VGG vs ResNet, CuDNN...)",https://www.reddit.com/r/MachineLearning/comments/4ssm4z/benchmarks_for_popular_cnn_models_titan_x_vs_1080/,malleus17,1468492523,,27,109
419,2016-7-14,2016,7,14,20,4ssux2,What is the best algorithm/library to use to detect vehicles in an image/video?,https://www.reddit.com/r/MachineLearning/comments/4ssux2/what_is_the_best_algorithmlibrary_to_use_to/,last6118,1468497173,[removed],0,0
420,2016-7-14,2016,7,14,20,4ssv27,Mathematicians don't know matrix differential calculus,https://www.reddit.com/r/MachineLearning/comments/4ssv27/mathematicians_dont_know_matrix_differential/,Kiuhnm,1468497248,"I wrote two tutorials about [matrix differential calculus](https://github.com/mtomassoli/papers/blob/master/matrixcalculus.pdf) and [backprop](https://github.com/mtomassoli/papers/blob/master/backprop.pdf), which I submitted to this very subreddit some time ago.

A few weeks ago I needed to do a derivation and I realized I had forgot some important rules about trace, vectorization and the kronecker operator, which brought me to the realization that, maybe, I didn't know matrix calculus as well as I thought I did. One doesn't forget so easily things which understands at a deep level.

So, I decided to dig deeper and, finally, came up with a tensor generalization of matrix differential calculus which is much simpler, at least conceptually. It doesn't use vectorization but works directly with tensors. Low-level manipulations are done on graphs (which have *nothing* to do with backprop). While one can do computations directly using this tensor calculus, this should lead to a more comfortable use of the classic calculus where every classic rule is seen as a notational shortcut. If you forget a rule, you can just draw a graph and *see* the rule almost right away. Sometimes, you need to combine many rules to massage an expression. A graph lets you see the final expression more easily.

Anyway, I came across some dilemmas here and there which, for one, made me appreciate the power of differentials. Mathematicians usually prefer to think in terms of derivatives but when you start using tensors, derivatives become inefficient and it's better to work with differentials directly. While derivatives are always combined the same way, differentials are combined by substitution therefore the way you combine them differs from case to case.

I post some of my ""realizations"" and dilemmas on /r/math (I deleted the post) and I, surprisingly, was *completely* misunderstood. The top-voted answer consisted in a summary of Calculus I and II. Basically, I was retaught the concepts of derivative, partial derivative, gradient and Jacobian. The other answers were all off track as well.

I read many papers which claim that Matrix Differential Calculus is not well understood by statisticians and other practitioners. Mathematicians are not helping since they don't seem interested in this topic at all.

I just wanted to let you know what's the situation (but maybe I was the only one unaware of it) and arguing that questions about matrix/tensor differential calculus are more on-topic here than on /r/math or other similar forums. Of course, if one doesn't know what's a Jacobian, he/she should ask on /r/math.",13,0
421,2016-7-14,2016,7,14,21,4st095,How to recognize the several similar objects on the image?,https://www.reddit.com/r/MachineLearning/comments/4st095/how_to_recognize_the_several_similar_objects_on/,kulsemig,1468499497,"I have a lot of photos. Each photo contains many similar items. An example on the photo. I need to find the boundaries of each such item. For now I'm searching rectangular boxes of goods. At first I find the logos on the photos, then for each logo I'm trying to find the boundary box which contain that logo. I'm using the Hough Trasform algorigthm for searching and selection lines.

My code (using OpenCV) currently allocates about 20% of the boxes. Data (recognized boxes) is not a lot that does not allow normal to teach statistical algorithms (eg neural networks). Tell me, please, how to select the boxes, especially with regard to their similarity.
http://imgur.com/2CssYhL",0,0
422,2016-7-14,2016,7,14,21,4st0yd,Web app that uses deep learning to automatically colorize black and white photos,https://www.reddit.com/r/MachineLearning/comments/4st0yd/web_app_that_uses_deep_learning_to_automatically/,rhiever,1468499775,,0,2
423,2016-7-14,2016,7,14,21,4st41f,Are there any good tutorials for LSTM Neural Nets?,https://www.reddit.com/r/MachineLearning/comments/4st41f/are_there_any_good_tutorials_for_lstm_neural_nets/,[deleted],1468501048,[removed],0,2
424,2016-7-14,2016,7,14,22,4st8e1,2016 report from cs224d (deep learning for nlp),https://www.reddit.com/r/MachineLearning/comments/4st8e1/2016_report_from_cs224d_deep_learning_for_nlp/,evc123,1468502695,,1,4
425,2016-7-14,2016,7,14,23,4stfvw,Temporal Contrastive Learning for Representation Learning and Inference in Latent Variable Models,https://www.reddit.com/r/MachineLearning/comments/4stfvw/temporal_contrastive_learning_for_representation/,fhuszar,1468505495,,2,25
426,2016-7-14,2016,7,14,23,4stg9i,Short demo for live video processing by fast style transfer neural network,https://www.reddit.com/r/MachineLearning/comments/4stg9i/short_demo_for_live_video_processing_by_fast/,mystarcat,1468505623,,1,1
427,2016-7-14,2016,7,14,23,4stjjq,Thinking of applying for Georgia Tech's ML degree,https://www.reddit.com/r/MachineLearning/comments/4stjjq/thinking_of_applying_for_georgia_techs_ml_degree/,[deleted],1468506739,[deleted],0,0
428,2016-7-15,2016,7,15,0,4stwj6,My new post on KDnuggets| Top Machine Learning MOOCs and Online Lectures: A comprehensive survey,https://www.reddit.com/r/MachineLearning/comments/4stwj6/my_new_post_on_kdnuggets_top_machine_learning/,pulkit_pulks,1468510942,http://www.kdnuggets.com/2016/07/top-machine-learning-moocs-online-lectures.html,0,0
429,2016-7-15,2016,7,15,0,4stxvt,Does Boston Dynamics use machine learning?,https://www.reddit.com/r/MachineLearning/comments/4stxvt/does_boston_dynamics_use_machine_learning/,juniorrojas,1468511367,"There are a bunch of impressive videos of Boston Dynamics' robots, but it's very difficult to find scientific publications of their work and it's not obvious to me whether or not they're using some kind of machine learning for their robots. I've seen different comments, some say they do use machine learning and others say they do not. Does anybody know of a reliable source that talks about this?",13,9
430,2016-7-15,2016,7,15,0,4stzsu,Exploding gradients in LSTM/GRU,https://www.reddit.com/r/MachineLearning/comments/4stzsu/exploding_gradients_in_lstmgru/,alrojo,1468511936,[removed],1,0
431,2016-7-15,2016,7,15,1,4su0mw,"Machine learning with Vowpal Wabbit, C++ and Python",https://www.reddit.com/r/MachineLearning/comments/4su0mw/machine_learning_with_vowpal_wabbit_c_and_python/,tschellenbach,1468512210,,2,8
432,2016-7-15,2016,7,15,1,4su18e,Normalized Advantage Functions (NAF) in TensorFlow,https://www.reddit.com/r/MachineLearning/comments/4su18e/normalized_advantage_functions_naf_in_tensorflow/,carpedm20,1468512393,,0,9
433,2016-7-15,2016,7,15,1,4su1os,[Fluff article] Using Machine Learning and Big Data to Find True Love on OKCupid: Full Text in Comments,https://www.reddit.com/r/MachineLearning/comments/4su1os/fluff_article_using_machine_learning_and_big_data/,crowbahr,1468512539,,2,0
434,2016-7-15,2016,7,15,1,4su4hy,Social Deep Dreamz,https://www.reddit.com/r/MachineLearning/comments/4su4hy/social_deep_dreamz/,Hidden_dreamz,1468513426,,0,0
435,2016-7-15,2016,7,15,1,4su5fh,Deep Q Learning for Beginners in 200 lines of python code to play Flappy Bird with Keras https://yanpanlau.github.io/2016/07/10/FlappyBird-Keras.html,https://www.reddit.com/r/MachineLearning/comments/4su5fh/deep_q_learning_for_beginners_in_200_lines_of/,[deleted],1468513733,[removed],0,1
436,2016-7-15,2016,7,15,1,4su5p2,Germany enlists machine learning to boost renewables revolution,https://www.reddit.com/r/MachineLearning/comments/4su5p2/germany_enlists_machine_learning_to_boost/,dharma-1,1468513821,,0,0
437,2016-7-15,2016,7,15,1,4su93w,Deep Q Learning for Beginners in 200 lines of python code to play Flappy Bird with Keras,https://www.reddit.com/r/MachineLearning/comments/4su93w/deep_q_learning_for_beginners_in_200_lines_of/,yanpanlau,1468514908,,3,14
438,2016-7-15,2016,7,15,2,4suf7x,Splitting data for cross validation,https://www.reddit.com/r/MachineLearning/comments/4suf7x/splitting_data_for_cross_validation/,heimson,1468516826,"When I started learning ML I was told that when optimizing model parameters it is considered good practice to split data into three parts: one for training, one for fitting a parameter, and one for testing. This is because model shouldn't be tested on data with which its parameters were fitted. Yet in scikit-learn there are methods dedicated to splitting data into two sets, training and testing, and no straightforward way to split into three sets. Why is it so? Is the method I described not so popular after all? Or do I misunderstand something? What is the recommended way to perform cross validation?",5,0
439,2016-7-15,2016,7,15,2,4sufjm,Helpful Instructions to Lubricate Machine Bearings,https://www.reddit.com/r/MachineLearning/comments/4sufjm/helpful_instructions_to_lubricate_machine_bearings/,jackerfrinandis,1468516924,,0,1
440,2016-7-15,2016,7,15,2,4sujyp,8bit.ai provides Free Image Recognition API on Mashape and from www.8bit.ai,https://www.reddit.com/r/MachineLearning/comments/4sujyp/8bitai_provides_free_image_recognition_api_on/,erogol,1468518266,,0,0
441,2016-7-15,2016,7,15,3,4sun6e,[MOOC] Computational Probability and Inference (edX/MIT),https://www.reddit.com/r/MachineLearning/comments/4sun6e/mooc_computational_probability_and_inference/,chalupapa,1468519208,,8,57
442,2016-7-15,2016,7,15,3,4sup1w,PokemonGoBot chatbot is here to help you beat gyms. [Beta],https://www.reddit.com/r/MachineLearning/comments/4sup1w/pokemongobot_chatbot_is_here_to_help_you_beat/,inuishan,1468519764,"http://m.me/pokemonGoBot
The PokemonGo Bot helps you Catch 'Em All.
For now, say 'Hi' and it will help you fight gym battles.
You will soon be able to find locations of awesome Pokemons, Gyms and Pokestops around you. There will also be special tips and tricks to CatchEmAll. Like the Facebook page PokemonGoBot to get updated on release of new features. Feedback and suggestions welcome. :)

[Screenshot Album](http://imgur.com/a/aQcy2)

[tl;dr Screenshot](http://i.imgur.com/rQfa2cw.png?1)
",1,0
443,2016-7-15,2016,7,15,4,4suz5e,Classification playground with NEAT,https://www.reddit.com/r/MachineLearning/comments/4suz5e/classification_playground_with_neat/,julian88888888,1468522842,,4,8
444,2016-7-15,2016,7,15,4,4sv537,Statistics on Neural Network,https://www.reddit.com/r/MachineLearning/comments/4sv537/statistics_on_neural_network/,danielcanadia,1468524623,"I have a quick question on cross-entropy once more.

Let's say I have 50 ""types"" of y-labels, where each y-label is a 100-size probability vector. After training the model (by minimizing cross-entropy), when I test it, I get an 100-size output for each test case. If I want to predict which ""type"" of y-input it most resembles, could I take the 50 types of y-input vectors and calculate cross-entropy with the output? Then take one with smallest cross entropy as most likely ""type"". I don't use types as vector cells mainly because each type overlaps other types at various degrees (dictated by 100-size probability vectors). There's a bit more to it but I don't want to go into it too much.",1,0
445,2016-7-15,2016,7,15,4,4sv53j,Train LSTM with k-hot vectors instead of 1-hot,https://www.reddit.com/r/MachineLearning/comments/4sv53j/train_lstm_with_khot_vectors_instead_of_1hot/,anonDogeLover,1468524625,Is it possible to use the vanilla LSTM setup with crossentropy loss for predicting k-hot instead of 1-hot vectors?,11,2
446,2016-7-15,2016,7,15,5,4svh32,Graph Mining applied to Neural Networks,https://www.reddit.com/r/MachineLearning/comments/4svh32/graph_mining_applied_to_neural_networks/,geomtry,1468528623,"I am looking for some fun papers to read on graph mining artificial neural networks. I am aware this has been done plentifully for biological ones.

Since the structure of a feed forward network is extremely specific, looking at the connections between nodes will probably be pretty plain. Instead, I am interested in are detection of ""new ties"" and ""removed ties"" during the training of a neural network. 

The simplest way would be to look at the change in edge weight between two nodes. This is just a matter of storing the results of backpropogation.

Another way would be to measure a change in the tendency for two connected nodes to activate together, even if they are not connected. 

Feel free to rip this apart if there's a much simpler method for accomplishing the same thing, or if certain theoretical expectations make this analysis redundant.",0,1
447,2016-7-15,2016,7,15,5,4svji5,Implementation of NIPS'15 Paper E2C with random orthogonal weight initialization,https://www.reddit.com/r/MachineLearning/comments/4svji5/implementation_of_nips15_paper_e2c_with_random/,_murphys_law_,1468529458,,0,5
448,2016-7-15,2016,7,15,5,4svke0,How to plot high dimensional data before proceeding with an algorithm?,https://www.reddit.com/r/MachineLearning/comments/4svke0/how_to_plot_high_dimensional_data_before/,[deleted],1468529745,[deleted],3,0
449,2016-7-15,2016,7,15,6,4svlwe,Kaggle user's badge autogenerator,https://www.reddit.com/r/MachineLearning/comments/4svlwe/kaggle_users_badge_autogenerator/,Littus,1468530245,,1,1
450,2016-7-15,2016,7,15,6,4svo3w,Specificity for multiple-class classification,https://www.reddit.com/r/MachineLearning/comments/4svo3w/specificity_for_multipleclass_classification/,Bohemian90,1468530968,"Hello

have read the paper ""[A systematic analysis of performance measures for classification tasks](http://biome.tk/Reuters/2009%20-%20Sokolova%20and%20Lapalme%20-%20A%20systematic%20analysis%20of%20performance%20measures%20for%20.pdf)"" in which different statistics for binary classification as well as their extension to the multi-class setting isexplained.

They introduced micro- and macro-averaging for precision and recall. Unfortunately, they do not spend any word about specificity in the multi-class case.

Can I just use micro- and macro-averaging for specifcity in the same way as for recall and precision?",0,0
451,2016-7-15,2016,7,15,6,4svqfu,Questions about using GPUs in the cloud,https://www.reddit.com/r/MachineLearning/comments/4svqfu/questions_about_using_gpus_in_the_cloud/,threeshadows,1468531794,"I'm looking for advice on getting set up with GPUs in the cloud. Im having trouble sifting through the options (see  [some options](http://www.nvidia.com/object/gpu-cloud-computing-services.html) from last time someone asked a similar question here). I'm experienced in data science, but a total newbie to cloud environments. I think I am looking for:

* persistent data  Im working with about 50GB data, so I dont want to have to re-upload every time
* pay as you go compute time  I wont be running experiments full-time, so there might be weeks at a time when I am not computing
* multiple GPUs per instance  preferably fairly recent GPUs for handling big models
* I plan to work mostly off jupyter notebook  but also in bash

Does anyone have experience working in a similar setup? 
Do you generally work off of existing images, or docker containers, or what?
How do you monitor your jobs?
Any other pluses or minuses?

Thanks for any advice! Also, I know some folks prefer a desktop at home since it can pay off in the long run, but thats not viable for me right now.",6,3
452,2016-7-15,2016,7,15,6,4svveh,Could a GAN (Generative Adverserial Model) learn a model of how things move?,https://www.reddit.com/r/MachineLearning/comments/4svveh/could_a_gan_generative_adverserial_model_learn_a/,anthbell,1468533513,"GANs are quite impressive for modeling images.  It can learn how things like kitches look like.  When watching a demo of ""moving through the parameter space"" of kitchen images I couldn't help but wonder if it would be possible for a GAN to jointly model how things move as well as how things typically look.  I was thinking this may be possible with frames from youtube videos.  Does anyone know of any research like this or has any ideas if this is possible?",2,5
453,2016-7-15,2016,7,15,7,4svz9k,keras model stuck at 94.2% accuracy :( help?,https://www.reddit.com/r/MachineLearning/comments/4svz9k/keras_model_stuck_at_942_accuracy_help/,maximus12793,1468534908,"Basically I am doing the amazon employee access challenge on kaggle (https://www.kaggle.com/c/amazon-employee-access-challenge). And have trained a neural net in keras using the following.     

    cnn = keras.models.Sequential()
    cnn.add(keras.layers.Dense(128, input_shape=(9,)))
    cnn.add(keras.layers.Activation('sigmoid'))
    cnn.add(keras.layers.Dropout(0.25))
    cnn.add(keras.layers.Dense(256, activation='relu'))
    cnn.add(keras.layers.Dense(256))
    cnn.add(keras.layers.Activation('relu'))
    cnn.add(keras.layers.Dropout(0.25))
    cnn.add(keras.layers.Dense(2))
    cnn.add(keras.layers.Activation('softmax'))

    earlyStop = keras.callbacks.EarlyStopping(monitor='loss', patience=0, verbose=0, mode='auto')

    sgd = keras.optimizers.SGD(lr=0.003)
    cnn.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=[""accuracy""])
    cnn.fit(X_train, Y_train, batch_size=32, nb_epoch=10, show_accuracy=True, verbose=0, callbacks=[earlyStop])

no matter how many layers I add or dropout/learning rate/etc. I ALWAYS get 94.21% (same when I rebuilt the model in skflow using much less layers). Any idea whats going wrong here and why it plateaus after 3 epochs?

* http://imgur.com/hXHlI4N &lt;learning chart&gt;
* http://imgur.com/4t1LRvu &lt;easier to read layer diagram&gt;

**edit: update** I have done a kfold (split 5x) and have trained it with validation data and shuffling data. Learning rate has been lowered/increased, optimizer switched to adam/adadelta/sgd same results. Baseline: 94.21% (0.20%). ALSO: you guys were right. it assumes *everything* is a 1. Is there a way to fix this easily?",13,3
454,2016-7-15,2016,7,15,7,4sw1qh,Question about bucketing in Seq2Seq RNN models,https://www.reddit.com/r/MachineLearning/comments/4sw1qh/question_about_bucketing_in_seq2seq_rnn_models/,tensorq,1468535790,"Hi All,

I'm working on a sequence autoencoder in tensorflow. The input sequence does not have fixed length. One way to solve this is padding the sequence with special PAD symbols so that all sequences are as long as the longest sequence in the dataset. In this way, you artificially create fixed-length sequences. However, this is inefficient. One alternative is described in tensorflow's documentation here: https://www.tensorflow.org/versions/r0.9/tutorials/seq2seq/index.html (under ""Bucketing and padding"").



The summary is that instead of padding every sequence to the longest sequence's length, make buckets of different fixed lengths, and only pad the sequence as much as necessary to fit into one of the buckets. Then train a model for each bucket. Based on the language used in the documentation, it seems like each model (trained on a single bucket) is it's own thing that does not share parameters with any of the models trained on other buckets. This seems counterintuitive to me. Wouldn't it be better to share some parameters, for example the weights used to create word embeddings?


Does someone here know what is considered best practice? ",1,0
455,2016-7-15,2016,7,15,7,4sw5e3,Question about Bucketing and Padding in Sequence-to-Sequence RNN models,https://www.reddit.com/r/MachineLearning/comments/4sw5e3/question_about_bucketing_and_padding_in/,tensorq,1468537148,"Hi All,


I'm working on a sequence autoencoder in tensorflow. The input sequence does not have fixed length. One way to solve this is padding the sequence with special PAD symbols so that all sequences are as long as the longest sequence in the dataset. In this way, you artificially create fixed-length sequences. However, this is inefficient. One alternative is described in tensorflow's documentation.


The summary is that instead of padding every sequence to the longest sequence's length, make buckets of different fixed lengths, and only pad the sequence as much as necessary to fit into one of the buckets. Then train a model for each bucket. Based on the language used in the documentation, it seems like each model (trained on a single bucket) is it's own thing that does not share parameters with any of the models trained on other buckets. This seems counterintuitive to me. Wouldn't it be better to share some parameters, for example the weights used to create word embeddings?


Does someone here know what is considered best practice?",1,0
456,2016-7-15,2016,7,15,9,4swhvn,Games with Anti Cheat with Machine Learning.,https://www.reddit.com/r/MachineLearning/comments/4swhvn/games_with_anti_cheat_with_machine_learning/,NGTmeaty,1468541853,[removed],6,1
457,2016-7-15,2016,7,15,9,4swi2m,[Fluff] My experience with machine learning so far,https://www.reddit.com/r/MachineLearning/comments/4swi2m/fluff_my_experience_with_machine_learning_so_far/,Alathazor,1468541928,,1,1
458,2016-7-15,2016,7,15,9,4swi9u,Artificial Intelligence in Self Driving Cars  Part 1,https://www.reddit.com/r/MachineLearning/comments/4swi9u/artificial_intelligence_in_self_driving_cars_part/,felixthursday,1468542010,,1,0
459,2016-7-15,2016,7,15,10,4swt08,Finally there is an app to use neural style to process photos,https://www.reddit.com/r/MachineLearning/comments/4swt08/finally_there_is_an_app_to_use_neural_style_to/,wb14123,1468546505,,32,132
460,2016-7-15,2016,7,15,12,4sx7qv,"Mastering Scala Machine Learning, $20, eBook Deal of the Day (50% off - Expires Friday July 15, 6PM CST)",https://www.reddit.com/r/MachineLearning/comments/4sx7qv/mastering_scala_machine_learning_20_ebook_deal_of/,n1tw1t,1468552901,,0,1
461,2016-7-15,2016,7,15,13,4sxffn,Machine learning advice needed on classification,https://www.reddit.com/r/MachineLearning/comments/4sxffn/machine_learning_advice_needed_on_classification/,deluded_soul,1468556494,"I am a beginner in machine learning and need some advice on how to proceed regarding a classification project that I am trying to do as a learning exercise. So, I have a set of images of the same object which is moving in time. So, what I have done is co-registered these objects and obtained a field i.e. at each point in the image I have a 2D vector of how it moves across time.

Now, I also have some expert annotations about this object regarding its state whether it is functioning properly, how long it has been in service etc. Now, my hypothesis is that how this object moves is related to these state variables. I have a bit of data but not a whole lot and I would like to train some classification models and see if I can learn this mapping.

This is where I am not stuck. For example, if I want to start with something simple like say logistic regression. How can I begin to map this motion field (which is defined at every pixel and is a 2d vector) to these start variables which may be a 10-dimensional vector. 

Would be very grateful if someone can recommend a good starting point. Logistic regression is just an example and I would like to start from simpler to more complicated and flexible models. My issue is I cannot at the moment figure out how to relate these two sides.",3,0
462,2016-7-15,2016,7,15,14,4sxnpr,Evaluation measure for rank-based problem,https://www.reddit.com/r/MachineLearning/comments/4sxnpr/evaluation_measure_for_rankbased_problem/,swokulski,1468560742,"Problem I deal with seemed to be a regression problem at first. I was trying to predict continues positive value, but than realized that I don't care about exact value but about the order (which should match order of these continues positive values). Now I'm looking for a measure that would help me to assess my model as RMSE seems not to be best approach.

Example:

Test|Predicted|Test-rank|Predicted-rank 
:--|:--|:--|:--
0.1|0.2|1|2
0.4|0.5|3|3
0.3|0.1|2|1

",5,0
463,2016-7-15,2016,7,15,15,4sxr1c,Tensorflow Slim MNIST example,https://www.reddit.com/r/MachineLearning/comments/4sxr1c/tensorflow_slim_mnist_example/,leavesofclass,1468562574,,0,0
464,2016-7-15,2016,7,15,16,4sxxge,What are some thought provoking talks that could inspire research ideas?,https://www.reddit.com/r/MachineLearning/comments/4sxxge/what_are_some_thought_provoking_talks_that_could/,metacurse,1468566284,"Talks you thought were really inspiring by thought leaders relevant to Neural Networks, Artificial Intelligence, Machine Learning, Reinforcement Learning.

A good example is Hinton's talk on Capsules. ",9,0
465,2016-7-15,2016,7,15,16,4sxxtd,Machine Learning can enhance Marketing Automation,https://www.reddit.com/r/MachineLearning/comments/4sxxtd/machine_learning_can_enhance_marketing_automation/,nkg24,1468566486,,0,0
466,2016-7-15,2016,7,15,16,4sxyin,double twisted barbed wire machine,https://www.reddit.com/r/MachineLearning/comments/4sxyin/double_twisted_barbed_wire_machine/,susanwang333999,1468566882,,1,0
467,2016-7-15,2016,7,15,17,4sy69l,Improving matching results with learning-to-rank research,https://www.reddit.com/r/MachineLearning/comments/4sy69l/improving_matching_results_with_learningtorank/,grumpybusinesscat,1468571726,,0,3
468,2016-7-15,2016,7,15,18,4syd5z,Anesthesia Machine Market Share And Manufactures Analysis By Region 2016-2020,https://www.reddit.com/r/MachineLearning/comments/4syd5z/anesthesia_machine_market_share_and_manufactures/,sachinmathur01,1468576093,,0,0
469,2016-7-15,2016,7,15,19,4syfa8,"Data Science as Praxis: ""the process by which a theory, lesson, or skill is enacted, embodied, or realised.""",https://www.reddit.com/r/MachineLearning/comments/4syfa8/data_science_as_praxis_the_process_by_which_a/,DevFRus,1468577462,,0,0
470,2016-7-15,2016,7,15,19,4syjbr,"I'm interested in doing my final year undergrad project in machine learning, specifically an implementation of NEAT. How do I get started?",https://www.reddit.com/r/MachineLearning/comments/4syjbr/im_interested_in_doing_my_final_year_undergrad/,ZeppyFloyd,1468579919,"I just started my final year in my CSE degree. I'm very interested in machine learning as a field. I have to base my project on a research paper and I've chosen the original NEAT paper.
 
However, I don't know anything about neural networks, or how to go about to even starting making sense of the actual technical implementation of NEAT. I was thinking along the lines of my project being teaching a computer how to play a game or something like that.

I started reading around this sub and all of this and the jargon is pretty overwhelming. I am still choosing to believe that, like most subjects, it's actually not as complicated as it looks at first glance.

As for my technical skills, language is not an issue, I prefer to work with C++ or Java but I feel confident in learning other languages in a reasonable time. Competent with data structures and computer networks (is that relevant?). I have had statistics and related topics in my engineering math classes, but I don't feel very confident in my abilities.

 Is choosing to implement NEAT biting off more than I can chew? Should I go for something less ambitious? If not, where do I begin learning so I can make this happen at the end of almost a year from now. Thank you for any insight you can offer. Cheers.",13,0
471,2016-7-15,2016,7,15,20,4symsc,Are basic cs algorithms used in machine learning ?,https://www.reddit.com/r/MachineLearning/comments/4symsc/are_basic_cs_algorithms_used_in_machine_learning/,AyushExel,1468581924,[removed],0,1
472,2016-7-15,2016,7,15,21,4sywrq,Learn to build recurrent neural networks in tensorflow dynamically from raw equations.,https://www.reddit.com/r/MachineLearning/comments/4sywrq/learn_to_build_recurrent_neural_networks_in/,[deleted],1468586958,[deleted],2,11
473,2016-7-15,2016,7,15,22,4syz7r,Data Science Tools Survey,https://www.reddit.com/r/MachineLearning/comments/4syz7r/data_science_tools_survey/,rafal_hryciuk,1468588048,[removed],0,1
474,2016-7-15,2016,7,15,22,4sz7f3,Data Science Tools,https://www.reddit.com/r/MachineLearning/comments/4sz7f3/data_science_tools/,rafal_hryciuk,1468591106,[removed],0,1
475,2016-7-15,2016,7,15,23,4szcvg,Data Scientist Tools,https://www.reddit.com/r/MachineLearning/comments/4szcvg/data_scientist_tools/,rafal_hryciuk,1468593039,,0,2
476,2016-7-15,2016,7,15,23,4szgvp,Cross-Lingual Word Sense Disambiguation: How do you align parallel text?,https://www.reddit.com/r/MachineLearning/comments/4szgvp/crosslingual_word_sense_disambiguation_how_do_you/,pygirl,1468594443,[removed],0,1
477,2016-7-16,2016,7,16,1,4szwgd,"Retrofitting, Retraining, Combining Nets",https://www.reddit.com/r/MachineLearning/comments/4szwgd/retrofitting_retraining_combining_nets/,luseafur,1468599499,[removed],0,0
478,2016-7-16,2016,7,16,2,4t051c,The EU Bot  a machine learning classifier for the EU Referendum,https://www.reddit.com/r/MachineLearning/comments/4t051c/the_eu_bot_a_machine_learning_classifier_for_the/,[deleted],1468602321,[deleted],2,0
479,2016-7-16,2016,7,16,2,4t07zn,Mobile friendly deep convolutional neural networks,https://www.reddit.com/r/MachineLearning/comments/4t07zn/mobile_friendly_deep_convolutional_neural_networks/,malreddysid,1468603308,,0,2
480,2016-7-16,2016,7,16,2,4t08f5,Automatically colorize black and white photos with a deep convolutional neural net. An implementation of Colorful Image Colorization,https://www.reddit.com/r/MachineLearning/comments/4t08f5/automatically_colorize_black_and_white_photos/,felixthursday,1468603434,,43,210
481,2016-7-16,2016,7,16,3,4t0ga2,[1607.02789] Charagram: Embedding Words and Sentences via Character n-grams,https://www.reddit.com/r/MachineLearning/comments/4t0ga2/160702789_charagram_embedding_words_and_sentences/,gojomo,1468605978,,8,6
482,2016-7-16,2016,7,16,3,4t0kv1,Distributed TensorFlow - Design Patterns and Best Practices,https://www.reddit.com/r/MachineLearning/comments/4t0kv1/distributed_tensorflow_design_patterns_and_best/,__ai__,1468607425,,0,0
483,2016-7-16,2016,7,16,4,4t0zzx,Help Annotate Michael Nielsens Book on Deep Learning,https://www.reddit.com/r/MachineLearning/comments/4t0zzx/help_annotate_michael_nielsens_book_on_deep/,seojoeschmo,1468612434,,0,22
484,2016-7-16,2016,7,16,6,4t1fsh,Cant get CUDA to work on 5.0 compute compatible GPU. Any help?,https://www.reddit.com/r/MachineLearning/comments/4t1fsh/cant_get_cuda_to_work_on_50_compute_compatible/,iceman_121,1468617879,,0,0
485,2016-7-16,2016,7,16,6,4t1h4j,Generating Long-Term Structure in Songs and Stories,https://www.reddit.com/r/MachineLearning/comments/4t1h4j/generating_longterm_structure_in_songs_and_stories/,perceptron01,1468618368,,8,11
486,2016-7-16,2016,7,16,10,4t2dyo,Is it just me or is this Job post hopelessly optimistic?,https://www.reddit.com/r/MachineLearning/comments/4t2dyo/is_it_just_me_or_is_this_job_post_hopelessly/,htrp,1468631099,,4,1
487,2016-7-16,2016,7,16,20,4t48wn,classification of attributed trees,https://www.reddit.com/r/MachineLearning/comments/4t48wn/classification_of_attributed_trees/,crop_pm,1468669847,"Hello all, i am searching for methods to classify attributed trees. can you recommend me some literature or give any tip? Also, is there any work on feature extraction from trees?

Thanks",1,4
488,2016-7-16,2016,7,16,21,4t4bh7,Should We Be Rethinking Unsupervised Learning? Ilya and Roland think we should.,https://www.reddit.com/r/MachineLearning/comments/4t4bh7/should_we_be_rethinking_unsupervised_learning/,evc123,1468671322,,50,117
489,2016-7-16,2016,7,16,22,4t4kbu,Machine learning vs software engineering master degree,https://www.reddit.com/r/MachineLearning/comments/4t4kbu/machine_learning_vs_software_engineering_master/,Skarwild,1468676181,[removed],3,0
490,2016-7-17,2016,7,17,2,4t5lr4,[NLP] Is nltk easier/faster/better than openNLP?,https://www.reddit.com/r/MachineLearning/comments/4t5lr4/nlp_is_nltk_easierfasterbetter_than_opennlp/,nitheism,1468691141,"Hello, I am fairly new here and in the field and not sure if this post is for here, but recently I tried to implement a simple document classifier in openNLP, ofc I checked resources on the internet but when writting the code it turned out that most of the classes used in the examples and etc were deprecated. I found the new ones but they had parameters that I wasn't able to understand how to customize. Also a thing I don't like is the model based system and the lack of language support. I've never tried nltk but from tutorials and etc it seems like a more efficient tool? So I jumped over here to see what are your opinions on both tools but mostly nltk. Here is a list of things I am most interested to know:

* **Does nltk support more languages / easy to add and train model for new one?**
* **Does it have simple and understandable inteface ?**
* **How big community does it have?**",6,3
491,2016-7-17,2016,7,17,4,4t626w,Art Makina &amp; Drl Petrol Ortakl Oto Ykama Sistemleri Kurulumu,https://www.reddit.com/r/MachineLearning/comments/4t626w/art_makina_drl_petrol_ortakl_oto_ykama/,graficihad,1468697423,,0,1
492,2016-7-17,2016,7,17,4,4t65ec,Design Patterns for Real World Machine Learning Systems,https://www.reddit.com/r/MachineLearning/comments/4t65ec/design_patterns_for_real_world_machine_learning/,pacmanisfun,1468698647,,0,1
493,2016-7-17,2016,7,17,5,4t6fz1,[1607.02902] sk_p: a neural program corrector for MOOCs,https://www.reddit.com/r/MachineLearning/comments/4t6fz1/160702902_sk_p_a_neural_program_corrector_for/,smerity,1468702613,,4,8
494,2016-7-17,2016,7,17,7,4t6se8,Are any other statistics grads frustrated with the terminology?,https://www.reddit.com/r/MachineLearning/comments/4t6se8/are_any_other_statistics_grads_frustrated_with/,gongbak,1468707488,Why does the data science / machine learning community seems so eager to take tools statisticians have been using for decades and just rename them. Half of the stuff I get confused about turns to be things I know under a different terminology. ,61,58
495,2016-7-17,2016,7,17,7,4t6xle,Finding the dominant color in an image,https://www.reddit.com/r/MachineLearning/comments/4t6xle/finding_the_dominant_color_in_an_image/,utkarshsinha,1468709572,,0,3
496,2016-7-17,2016,7,17,10,4t7mqm,Machine Learning - WAYR (What Are You Reading) - Week 3,https://www.reddit.com/r/MachineLearning/comments/4t7mqm/machine_learning_wayr_what_are_you_reading_week_3/,Deinos_Mousike,1468720538,"This is a place to share machine learning research papers, journals, and articles that you're reading this week. If it relates to what you're researching, by all means elaborate and give us your insight, otherwise it could just be an interesting paper you've read.

Preferably you should link the arxiv page (not the PDF, you can easily access the PDF from the summary page but not the other way around) or any other pertinent links.

[Week 1](https://www.reddit.com/r/MachineLearning/comments/4qyjiq/machine_learning_wayr_what_are_you_reading_week_1/)

[Week 2](https://www.reddit.com/r/MachineLearning/comments/4s2xqm/machine_learning_wayr_what_are_you_reading_week_2/)

Here are some of the most upvoted links from last week with the user who found it:

[If you've ever wondered about why the skip-gram models make King - Male + Female = Queen, here's the paper which busts the myths of ""linear structure"" and explains what's really going on. It's obvious once you realize it in retrospect - /u/gabrielgoh](https://levyomer.files.wordpress.com/2014/04/linguistic-regularities-in-sparse-and-explicit-word-representations-conll-2014.pdf)

[If you're confused about what gradient boosting has to do with gradient descent, read this - /u/gabrielgoh](http://papers.nips.cc/paper/1766-boosting-algorithms-as-gradient-descent.pdf)

[Reinforcement Learning: An Introduction - Second edition - /u/LecJackS](https://www.dropbox.com/s/b3psxv2r0ccmf80/book2015oct.pdf)

Besides that, there are no rules, have fun.",21,103
497,2016-7-17,2016,7,17,11,4t7okz,Are parameters related to the dataset considered hyperparameters and can they be chosen using optimization techniques?,https://www.reddit.com/r/MachineLearning/comments/4t7okz/are_parameters_related_to_the_dataset_considered/,j_lyf,1468721423,"For example: 

- Time series sliding window duration/interval
- FFT length
- Smoothing/median filtering parameters.",7,0
498,2016-7-17,2016,7,17,14,4t8fkj,Which deep learning library should I use?,https://www.reddit.com/r/MachineLearning/comments/4t8fkj/which_deep_learning_library_should_i_use/,alek9,1468734686,[removed],11,0
499,2016-7-17,2016,7,17,14,4t8frq,I'm interested in pursuing a security-related Machine Learning project next semester for my research class - what are some good resources for possible topics/project ideas?,https://www.reddit.com/r/MachineLearning/comments/4t8frq/im_interested_in_pursuing_a_securityrelated/,[deleted],1468734787,[deleted],3,0
500,2016-7-17,2016,7,17,15,4t8h5b,Jump to deep learning,https://www.reddit.com/r/MachineLearning/comments/4t8h5b/jump_to_deep_learning/,[deleted],1468735589,[removed],3,0
501,2016-7-17,2016,7,17,15,4t8ibm,A Naive Bayes library built in Elixir :) (as a way to learn about the algorithm and the programming language),https://www.reddit.com/r/MachineLearning/comments/4t8ibm/a_naive_bayes_library_built_in_elixir_as_a_way_to/,fredwu,1468736295,,0,1
502,2016-7-17,2016,7,17,15,4t8mbf,Can you suggest high relevant and impacting papers on Reinforcement Learning?,https://www.reddit.com/r/MachineLearning/comments/4t8mbf/can_you_suggest_high_relevant_and_impacting/,stevofolife,1468738686,"I'm in my last year of computer science &amp; cognitive science and I've been gradually learning the terminology/concepts used in ML and Neuroscience by osmosis in various research and work experiences. For the sake of my own curiosity and interest, I really want to understand reinforcement learning from both ML and Neuro perspectives. 

Are there anyone out there specializing in RL and can suggest a list of high signal low noise RL-related papers ranging from the fundamentals to the current state of the art ones?

My goal is to establish a good fundamental understanding of how RL works and came from, so I can start reading the newest research in that area.",11,25
503,2016-7-17,2016,7,17,22,4t9nbf,"github.com/lferry007/LargeVis (authorial implementation of LargeVis, a t-SNE competitor)",https://www.reddit.com/r/MachineLearning/comments/4t9nbf/githubcomlferry007largevis_authorial/,dunnowhattoputhere,1468762476,,2,23
504,2016-7-18,2016,7,18,0,4ta6rb,How to Deal With Non-Equal Distribution in Neural Networks,https://www.reddit.com/r/MachineLearning/comments/4ta6rb/how_to_deal_with_nonequal_distribution_in_neural/,danielcanadia,1468770944,"Hi everyone! I've lately been stumped with this question that seems seemingly simple.

Let's say we have a training data consisting of 100-factors, labelled with one of five classes (one-hot). The distribution in the training and testing set of class labels is the ratio 1:3:4:3:1.

What is the ideal way to train such a model to ensure a correct class distribution when classifying? From what I can think of I could:

1) Sample equal of each label when training which would:

i) ensure the model doesn't have any biases from distribution

ii) could lead to disproportionally more distribution of more rare classes


2) Use training data the way it is (1:3:4:3:1 ratio)

i) could result in biases from distribution that could snowball

ii) would take into account the ""correct"" distrubition


3) Alternative: ??? Maybe some kind of  dynamic sampling where you change the sample distribution according to last validation case's distribution.


I would really appreciate people's inputs on this (no pun intended).",13,14
505,2016-7-18,2016,7,18,0,4ta79z,theano cuda windows,https://www.reddit.com/r/MachineLearning/comments/4ta79z/theano_cuda_windows/,[deleted],1468771149,[deleted],8,1
506,2016-7-18,2016,7,18,3,4taw3r,"Mathematical Definition of ""Ideas""",https://www.reddit.com/r/MachineLearning/comments/4taw3r/mathematical_definition_of_ideas/,leodicapricorn,1468780561,[removed],2,3
507,2016-7-18,2016,7,18,3,4tawvw,Training new images with deep learning,https://www.reddit.com/r/MachineLearning/comments/4tawvw/training_new_images_with_deep_learning/,xyggy,1468780852,[removed],3,1
508,2016-7-18,2016,7,18,4,4tb0eh,Which approach is best for this type of forecasting?,https://www.reddit.com/r/MachineLearning/comments/4tb0eh/which_approach_is_best_for_this_type_of/,stackinpointers,1468782168,"I'm new to ML and looking for some advice on the best technique to use for the given application. 

Let's say I have a large number of time series:

    a: [0, 5, 3, 3, 2, 1, ... ] 
    b: [8, 8, 7, 9, 4, 6, ... ]
    ...
    zzzz: [0, 1, 1, 0, 4, 3, 1, ... ]

Given another incomplete time series, i'd like to predict the nth value (and ideally, n+1, n+2, etc.)

    test: [ 3, 2, 3, ?, ? ]

What is the best approach for doing this? Bonus upvotes if you can point me to an example using TensorFlow (I already have it installed :)",17,0
509,2016-7-18,2016,7,18,4,4tb4v3,Keras Embedding Layer with stateful LSTMs question,https://www.reddit.com/r/MachineLearning/comments/4tb4v3/keras_embedding_layer_with_stateful_lstms_question/,butWhoWasBee,1468783824,[removed],0,1
510,2016-7-18,2016,7,18,6,4tbmdw,"Deep learning tutorial on Caffe technology : basic commands, Python and C++ code",https://www.reddit.com/r/MachineLearning/comments/4tbmdw/deep_learning_tutorial_on_caffe_technology_basic/,hyperqube12,1468790299,,2,6
511,2016-7-18,2016,7,18,7,4tbsqg,Social Dreamz 2,https://www.reddit.com/r/MachineLearning/comments/4tbsqg/social_dreamz_2/,Hidden_dreamz,1468792886,,3,1
512,2016-7-18,2016,7,18,7,4tc0wk,NIPS reviews question,https://www.reddit.com/r/MachineLearning/comments/4tc0wk/nips_reviews_question/,anonDogeLover,1468796221,"What exactly is the ""author rebuttal period""? Does that mean reviews will first be released to the authors today, or something else?",45,10
513,2016-7-18,2016,7,18,9,4tchny,Introducing Cloud Hosted Deep Learning Models,https://www.reddit.com/r/MachineLearning/comments/4tchny/introducing_cloud_hosted_deep_learning_models/,josephd,1468802752,,0,0
514,2016-7-18,2016,7,18,10,4tckwy,"A nice blog post on trueskill, the bayesian ranking system behind xbox matchmaking.",https://www.reddit.com/r/MachineLearning/comments/4tckwy/a_nice_blog_post_on_trueskill_the_bayesian/,gabrielgoh,1468804061,,17,217
515,2016-7-18,2016,7,18,10,4tcqjz,"This Week in ML &amp; AI Podcast - On Public Datasets, Wide &amp; Deep Learning Models, and Benchmarking CNNs",https://www.reddit.com/r/MachineLearning/comments/4tcqjz/this_week_in_ml_ai_podcast_on_public_datasets/,sbc1906,1468806327,,1,1
516,2016-7-18,2016,7,18,11,4tcwjl,Tone Analysis - Fresh Machine Learning #3,https://www.reddit.com/r/MachineLearning/comments/4tcwjl/tone_analysis_fresh_machine_learning_3/,llSourcell,1468808712,,0,0
517,2016-7-18,2016,7,18,12,4td1xk,How to Install GPU TensorFlow f/ Sources,https://www.reddit.com/r/MachineLearning/comments/4td1xk/how_to_install_gpu_tensorflow_f_sources/,wagonhelm,1468810920,,3,0
518,2016-7-18,2016,7,18,12,4td69g,Steps from buying a GPU to running deep learning code on it?,https://www.reddit.com/r/MachineLearning/comments/4td69g/steps_from_buying_a_gpu_to_running_deep_learning/,iamquah,1468812830,"Hey all,

As the title says, does anyone have any resources for the entire process? I've never actually done the setup myself and I'm interested in doing it for some personal projects. My budget is probably around 500 or so if that helps?

I don't have a desktop so would I need to buy a raspberry pi (would a raspberry pi even cut it? or would it be fine as the computation would be pushed off to the GPU?)

Maybe steps from choosing a GPU to connecting it to the desktop (or the thing acting as the desktop) to cooling and all that. Or would this be a better question for PC gaming or something of the sort? ",9,0
519,2016-7-18,2016,7,18,13,4tdelo,Kaggle Kernel - kNN from scratch in Python on digit recogniser task achieves 97.1% accuracy.,https://www.reddit.com/r/MachineLearning/comments/4tdelo/kaggle_kernel_knn_from_scratch_in_python_on_digit/,Sn_Shines,1468816475,,0,0
520,2016-7-18,2016,7,18,15,4tduza,Question Independent Grading using Machine Learning Demo,https://www.reddit.com/r/MachineLearning/comments/4tduza/question_independent_grading_using_machine/,nishank_varshney,1468824011,,0,1
521,2016-7-18,2016,7,18,15,4tdwes,"[R] Trying to implement knn on Churn dataset but keep getting ""Some factor has empty levels"" error. Please help",https://www.reddit.com/r/MachineLearning/comments/4tdwes/r_trying_to_implement_knn_on_churn_dataset_but/,xBambii,1468824739,"I'm implementing knn with [this dataset](https://github.com/ericchiang/churn/blob/master/data/churn.csv) but I keep getting this error

&gt;Some factor has empty levels

The code I have is [here](http://pastebin.com/uWmAwHjy).

I'd like to mention im using knncat because I want to use the symbolic data also like the states. I've been trying at this for few days now with no results, fairly new to R also so any help is appreciated!",2,0
522,2016-7-18,2016,7,18,17,4te5ys,Recalibrating Neural Networks?,https://www.reddit.com/r/MachineLearning/comments/4te5ys/recalibrating_neural_networks/,sleeksteel,1468829977,"I am creating a prediction model for daily water consumption for households using Neural networks in R.
 
The input parameters would be  Past 2 days consumption, past 2 days temperature (min and max) and the forecast day temperature.

Suppose I want to create a model that will train on the past 3 years water consumption data (once) and then be used for daily water predictions for the next 100 years. 
I was thinking of setting a max error threshold at 10% for 5 consecutive days, and when this is voided the model will retrain itself using the past 3 months data and then use this for further predictions which should hopefully have better accuracy than before. 
The reason being that peoples water consumption patterns are likely to change over the next few years, due to water conservation methods and awareness plans or just people caring more about water, a precious resource.
Does this idea of retraining when the error crosses a threshold make sense?

Or does it make sense when I have very little past data(1-2 months) and after a couple of years it will never need to retrain itself?

Thanks!",4,0
523,2016-7-18,2016,7,18,17,4te6xn,Recalibrating Neural Networks,https://www.reddit.com/r/MachineLearning/comments/4te6xn/recalibrating_neural_networks/,[deleted],1468830580,[deleted],0,1
524,2016-7-18,2016,7,18,17,4te906,Someone worked with Google DataLab &amp; Tensorflow?,https://www.reddit.com/r/MachineLearning/comments/4te906/someone_worked_with_google_datalab_tensorflow/,pgaleone,1468831829,"I'm trying to figure out if Google DataLab (or the whole Google Cloud Platform) is a good alternative for training and testing deep models instead of having physical GPUs in the laboratory.

I think there is a lack of documentation about DataLab. The only thing I found is a tutorial on how to forecast demand with Google BigQuery, DataLab &amp; Tensorflow ( https://cloud.google.com/blog/big-data/2016/05/how-to-forecast-demand-with-google-bigquery-public-datasets-and-tensorflow ).

But I can't find out if I can use the google cloud platform to manually download datasets (eg, some computer vision dataset), storing it and use it to do distributed train across a big number of instances.

There's some documentation I missed? Someone that used it can share his experience?",6,0
525,2016-7-18,2016,7,18,19,4teh3w,Approaching (almost) any machine learning problem,https://www.reddit.com/r/MachineLearning/comments/4teh3w/approaching_almost_any_machine_learning_problem/,[deleted],1468836658,[deleted],0,0
526,2016-7-18,2016,7,18,19,4telzr,"Machine Learning Summer School - Arequipa, Peru",https://www.reddit.com/r/MachineLearning/comments/4telzr/machine_learning_summer_school_arequipa_peru/,amjams,1468839213,[removed],0,1
527,2016-7-18,2016,7,18,20,4teslg,How AI &amp; Machine Learning for Apps can Help Your SMB,https://www.reddit.com/r/MachineLearning/comments/4teslg/how_ai_machine_learning_for_apps_can_help_your_smb/,[deleted],1468842499,[deleted],0,1
528,2016-7-18,2016,7,18,21,4tf1dx,What's a good tool to draw neural networks figures ?,https://www.reddit.com/r/MachineLearning/comments/4tf1dx/whats_a_good_tool_to_draw_neural_networks_figures/,Jean-Porte,1468846218,"Do you know a good (preferably online) software that allows drawing neural networks ? I know they are standard graph but I want to show the units and the fully connected links
I don't have enough time to use Tikz and Google slides isn't that appropriate.

Thanks",7,1
529,2016-7-18,2016,7,18,22,4tf89f,Dynamic Memory Networks (DMN) for Text QA - in TensorFlow,https://www.reddit.com/r/MachineLearning/comments/4tf89f/dynamic_memory_networks_dmn_for_text_qa_in/,therne7,1468849095,,1,5
530,2016-7-18,2016,7,18,23,4tfj5o,Good case study projects on neural networks for an Undergraduate,https://www.reddit.com/r/MachineLearning/comments/4tfj5o/good_case_study_projects_on_neural_networks_for/,minato3421,1468853364,I am an Undergrad CS student and I want to do a project in neural networks. What are some good projects that I can do to publish a low level paper?,5,0
531,2016-7-19,2016,7,19,0,4tfmbq,Language Evolution Simulation,https://www.reddit.com/r/MachineLearning/comments/4tfmbq/language_evolution_simulation/,3eyedravens,1468854461,,6,66
532,2016-7-19,2016,7,19,1,4tfxnu,Predict financial markets with algos | QUANTLABS.NET,https://www.reddit.com/r/MachineLearning/comments/4tfxnu/predict_financial_markets_with_algos_quantlabsnet/,[deleted],1468858309,[deleted],0,1
533,2016-7-19,2016,7,19,1,4tfybh,Nervana releases 'deep learning course' for neon and AI cloud,https://www.reddit.com/r/MachineLearning/comments/4tfybh/nervana_releases_deep_learning_course_for_neon/,jessiclr,1468858528,,0,11
534,2016-7-19,2016,7,19,1,4tfyxq,I found these notes since I had been looking for a python N-armed bandit testbed code. The plots in the ipython notebook referenced do not match with the plots in the book. What is the issue ?,https://www.reddit.com/r/MachineLearning/comments/4tfyxq/i_found_these_notes_since_i_had_been_looking_for/,sasaram,1468858740,,0,0
535,2016-7-19,2016,7,19,1,4tg0fm,Question about Kaldi and where to find decoded/transcribed outputs,https://www.reddit.com/r/MachineLearning/comments/4tg0fm/question_about_kaldi_and_where_to_find/,[deleted],1468859247,[deleted],1,0
536,2016-7-19,2016,7,19,1,4tg1wx,Anyone heard of Neuramatix NeuraBASE?,https://www.reddit.com/r/MachineLearning/comments/4tg1wx/anyone_heard_of_neuramatix_neurabase/,desu-no,1468859719,"I don't know but, after random googling stuff, I found this.

It's a commercial artificial neural network-based product.

[http://neuramatix.com/](http://neuramatix.com/)

[How it works](http://neuramatix.com/howToApply.php)

They got few proof of concepts videos.
[\(1\)](http://neuramatix.com/video_inverted.php), [\(2\)](http://neuramatix.com/video_muscle.php), [\(3\)](http://neuramatix.com/video_digit.php)


The company itself seems very low profile, with only some [white papers and several conference papers](http://www.neuramatix.com/papers.phpp) in 2013 in IEEE sponsored conference

From what I read seems the system use some sort of reinforcement learning to train itself.

*The same founder of the company also founded other [spin-off company researching in bioinformatics  and genomics](http://www.synamatix.com/index.shtml), with some testimonies from like, the National Center for Biotechnology Information, U.S.",1,1
537,2016-7-19,2016,7,19,2,4tg6rd,How to choose the best metric?,https://www.reddit.com/r/MachineLearning/comments/4tg6rd/how_to_choose_the_best_metric/,heimson,1468861240,"Probably the most confusing problem I've encountered while learning ML (like clustering) is the choice of an appropriate distance measure, especially when dealing with categorical/mixed data. Most of the answers out there on the internet boil down to ""it depends"" and are not helpful at all. Libraries like sklearn implement many different metrics, yet choosing one is entirely up to the user. Looking up different distance measures on Wikipedia doesn't really help my intuition either.

Please notice, I understand this is a subtle matter and I'm not asking for a rule of thumb to use. On the contrary, I'd like to understand the problem more deeply and be able to analyse my data and pick the metric that's most suitable to the given case. Could you please point me to some resources I could use? What are the current best practices in this field?",2,0
538,2016-7-19,2016,7,19,2,4tg7dh,Figuring Out the Algorithms of Intelligence,https://www.reddit.com/r/MachineLearning/comments/4tg7dh/figuring_out_the_algorithms_of_intelligence/,davidhagar,1468861436,,0,0
539,2016-7-19,2016,7,19,2,4tg9g2,Fast Recommendations for Activity Streams Using Vowpal Wabbit,https://www.reddit.com/r/MachineLearning/comments/4tg9g2/fast_recommendations_for_activity_streams_using/,davidhagar,1468862112,,0,6
540,2016-7-19,2016,7,19,2,4tg9km,Detecting the Programming Language of Source Code Snippets using Machine Learning and Neural Networks,https://www.reddit.com/r/MachineLearning/comments/4tg9km/detecting_the_programming_language_of_source_code/,seojoeschmo,1468862151,,0,0
541,2016-7-19,2016,7,19,2,4tga7c,Data Scientist Tools Survey,https://www.reddit.com/r/MachineLearning/comments/4tga7c/data_scientist_tools_survey/,pmigdal,1468862364,,5,5
542,2016-7-19,2016,7,19,2,4tgadi,"Edward: a Python library for probabilistic modeling, inference, and criticism",https://www.reddit.com/r/MachineLearning/comments/4tgadi/edward_a_python_library_for_probabilistic/,isc_master,1468862423,,10,34
543,2016-7-19,2016,7,19,2,4tgank,Best Way to Start with Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/4tgank/best_way_to_start_with_machine_learning/,austinc3030,1468862531,[removed],1,0
544,2016-7-19,2016,7,19,2,4tgd6x,Recurrent Highway &amp; Multiplicative Integration -- Implementation in TensorFlow,https://www.reddit.com/r/MachineLearning/comments/4tgd6x/recurrent_highway_multiplicative_integration/,LeavesBreathe,1468863340,,5,16
545,2016-7-19,2016,7,19,3,4tgjye,Dallas Dreamz,https://www.reddit.com/r/MachineLearning/comments/4tgjye/dallas_dreamz/,Hidden_dreamz,1468865514,,1,0
546,2016-7-19,2016,7,19,3,4tgnk7,"Theano implementation of pixelCNN architectures for unconditional image generation: with no blind spots, 4-way pixel quantization, residual connections, etc.",https://www.reddit.com/r/MachineLearning/comments/4tgnk7/theano_implementation_of_pixelcnn_architectures/,kundan2510,1468866711,,0,16
547,2016-7-19,2016,7,19,3,4tgoff,Gradient Boosting Interactive Playground,https://www.reddit.com/r/MachineLearning/comments/4tgoff/gradient_boosting_interactive_playground/,alxndrkalinin,1468867001,,0,11
548,2016-7-19,2016,7,19,4,4th21k,TensorFlow Disappoints  Google Deep Learning falls shallow,https://www.reddit.com/r/MachineLearning/comments/4th21k/tensorflow_disappoints_google_deep_learning_falls/,rohshall,1468871703,,2,0
549,2016-7-19,2016,7,19,5,4th8og,Attempting to solve Andrew Ng's logistic regression exercise on Coursera with Tensorflow.,https://www.reddit.com/r/MachineLearning/comments/4th8og/attempting_to_solve_andrew_ngs_logistic/,crystalclear506,1468873976,"I'm pretty new to Tensorflow and somewhat new to ML in general so I attempted to replicate what I had done so far in Octave for the sake of learning, here's the following code that I did:

    import tensorflow as tf

    X = tf.placeholder(tf.float32, [100, 3], name=""X"")
    y = tf.placeholder(tf.float32, [100, 1], name=""y"")
    theta = tf.Variable(tf.random_normal([3,1], stddev=0.01), name=""weights"")


    def model(X, theta):
        return tf.sigmoid(tf.matmul(X, theta))

    y_ = model(X, theta)

    loss_function = (1./100.) * (tf.reduce_sum(tf.mul(tf.log(y_),(y))) - tf.reduce_sum(tf.mul(tf.log(1.-y_),(1.-y))))

    train_step = tf.train.GradientDescentOptimizer(0.01).minimize(loss_function)

    init = tf.initialize_all_variables()
    sess = tf.Session()
    sess.run(init)

    for i in xrange(100):
        sess.run(train_step, feed_dict={X:data_X, y:data_y})
    
    print sess.run(theta)

I had done this successfully in Octave (As it has been used by the course) But somehow I couldn't see why it doesn't produce the same result here with Tensorflow (with the following code). It instead produces three NaN values for theta, where in Octave it produces [-25.16, 0.2, 0.2]' with the exact same input dataset as provided by the exercise #2 (part 1 unregularized logistic regression). I have checked the output of my loss function, but the results are different comparing my octave one, so that could be the reason. I just couldn't really see where.

Could anyone here give me some pointers ? Thanks a lot.",5,0
550,2016-7-19,2016,7,19,6,4thexz,Geoff Hinton's Neural Networks for Machine Learning Course Is Being Offered Again,https://www.reddit.com/r/MachineLearning/comments/4thexz/geoff_hintons_neural_networks_for_machine/,modeless,1468876223,,46,333
551,2016-7-19,2016,7,19,6,4thfeq,"machine learning - no parameters, no constants",https://www.reddit.com/r/MachineLearning/comments/4thfeq/machine_learning_no_parameters_no_constants/,hans1328,1468876391,[removed],0,1
552,2016-7-19,2016,7,19,6,4thi8e,Sequence Classification Inputs and Outputs,https://www.reddit.com/r/MachineLearning/comments/4thi8e/sequence_classification_inputs_and_outputs/,kazooki117,1468877415,"Hello,

I'm trying to figure out if using a RNN would fit my problem. Here is my input data:

I have 1000 sequences. Each sequence contains 6000 samples. For each sequence, I have a single label, either 1 or 0.

I have read many sources such as http://deeplearning.net/tutorial/lstm.html and 
http://keras.io/getting-started/sequential-model-guide/, but I can't seem to figure out a definitive answer to some of my questions.

I don't have a label for each sample, just a single label for the entire sequence. Can I still use a RNN to classify the sequence? It seems like I should be able to, but I can't quite figure out how the training would work. If I understand correctly, I would wait to perform the backward step and update the weights of the network until all of the 6000 samples were fed into the RNN. I would get a single label once the 6000th sample is fed into the RNN, and I would compare this with the actual label for that sequence. Does this sound correct? If so, then what does the backpropagation look like compared to if I had a label for each sample, is the only difference the number of times that I perform the backward step?

I'm a little lost, any help would be greatly appreciated.",2,0
553,2016-7-19,2016,7,19,6,4thirl,What is the best python based ML course available online?,https://www.reddit.com/r/MachineLearning/comments/4thirl/what_is_the_best_python_based_ml_course_available/,wgpubs,1468877599,"I'm currently finishing up Ng's ML course on coursera, and while I consider it foundational, I'd really like to find a course that demonstrates how to systematically solve different ML problems using python and packages like numpy, scipy, pandas, tensorflow, sckit-learn, sckit-flow, etc...

Any recommendations on upcoming classes I should be looking at?",15,8
554,2016-7-19,2016,7,19,7,4thqr8,Recurrent Neural Network (LSTM) Learns to Generate Voice,https://www.reddit.com/r/MachineLearning/comments/4thqr8/recurrent_neural_network_lstm_learns_to_generate/,Hiibb,1468880550,,20,56
555,2016-7-19,2016,7,19,8,4thz7o,Gradient Boosting Interactive Playground,https://www.reddit.com/r/MachineLearning/comments/4thz7o/gradient_boosting_interactive_playground/,pmigdal,1468883612,,0,3
556,2016-7-19,2016,7,19,8,4ti5m4,"Great opportunity to practice ML? Before i suggest to boss, want feedback",https://www.reddit.com/r/MachineLearning/comments/4ti5m4/great_opportunity_to_practice_ml_before_i_suggest/,[deleted],1468886005,[deleted],4,0
557,2016-7-19,2016,7,19,8,4ti68s,"I wrote an article about learning image generation with DRAW &amp; DCGAN, feedback welcome.",https://www.reddit.com/r/MachineLearning/comments/4ti68s/i_wrote_an_article_about_learning_image/,FredrikNoren,1468886246,,11,4
558,2016-7-19,2016,7,19,10,4tinqy,Knowing what you know about ML/DL. What are you willing to sell for a job?,https://www.reddit.com/r/MachineLearning/comments/4tinqy/knowing_what_you_know_about_mldl_what_are_you/,[deleted],1468892700,[deleted],9,0
559,2016-7-19,2016,7,19,11,4tisd4,"People with PHD's, how did you manage personal life(work, family, marriage) when doimg graduate studies?",https://www.reddit.com/r/MachineLearning/comments/4tisd4/people_with_phds_how_did_you_manage_personal/,WashimNeupane,1468894437,"I am a high school graduate who wants to pursue PHD someday. But I can't imagine how difficult it would make my life. I am currently aiming for a career in Data Science(esp. machine learning) and so far based on people's opinion it would require a Masters+ degree to acheive any recognition.

 Also, I will be an international student in the US next year with very small income. I will probably attend some community colleges and work my way to top. I dont know if I can go through a PHD straight without doing jobs because of my economic status. Hell, do they even let people from community college do PHDs? Do I need to go to State Uni?

 Since PHDs take about 5 years, i will have 10 years more academics in my way if I choose this path. While I am deeply passionate, I am in this confused state and would like to have an idea about how you guys maintained work-life-research balance. If there is no way of balancing it, I will probably switch majors to suit a better career for myself.

 If any one of you were in this state, can you share your story. What majors you took. What Uni you went to. What you ended up doing? What I am asking for is more like a step-by-step method to have a successful career in ML with minimal expenses; a guide to problems you faced and how you handled all the academics parallel to your personal life.",9,3
560,2016-7-19,2016,7,19,11,4titf6,Machine Learning in a Nutshell | Episode 1,https://www.reddit.com/r/MachineLearning/comments/4titf6/machine_learning_in_a_nutshell_episode_1/,EsportsinaNutshell,1468894843,,0,0
561,2016-7-19,2016,7,19,11,4tiywv,how to diacharge the welded wire mesh coil form the machine?,https://www.reddit.com/r/MachineLearning/comments/4tiywv/how_to_diacharge_the_welded_wire_mesh_coil_form/,susanwang333999,1468896863,,1,0
562,2016-7-19,2016,7,19,14,4tjj9i,Ranking Sales Leads with Machine Learning,https://www.reddit.com/r/MachineLearning/comments/4tjj9i/ranking_sales_leads_with_machine_learning/,Neb519,1468906202,"I just published [this article](http://gormanalysis.com/convert-more-sales-leads-with-machine-learning/) based on a real job I did for a client, ranking sales leads so they could efficiently identify and pursue the best ones. 

I generated a sample dataset and 3 model solutions - logistic regression, random forest, and gradient boosting (each implemented in Python and R) [here](https://github.com/ben519/MLPB/tree/master/Problems/Rank%20Sales%20Leads), in the [Machine Learning Problem Bible](https://github.com/ben519/MLPB).

Enjoy",10,0
563,2016-7-19,2016,7,19,16,4tjwg3,What is a dispersion system?,https://www.reddit.com/r/MachineLearning/comments/4tjwg3/what_is_a_dispersion_system/,mixmachinery,1468912997,,1,1
564,2016-7-19,2016,7,19,17,4tk39b,What is Passive Vibration Isolation of Shock Isolator?,https://www.reddit.com/r/MachineLearning/comments/4tk39b/what_is_passive_vibration_isolation_of_shock/,hoanindustry,1468916850,,0,1
565,2016-7-19,2016,7,19,17,4tk664,This Guy Trains Computers to Find Future Criminals,https://www.reddit.com/r/MachineLearning/comments/4tk664/this_guy_trains_computers_to_find_future_criminals/,[deleted],1468918422,[deleted],0,0
566,2016-7-19,2016,7,19,18,4tkao9,Understanding Bias: A Pre-requisite For Trustworthy Results,https://www.reddit.com/r/MachineLearning/comments/4tkao9/understanding_bias_a_prerequisite_for_trustworthy/,BenoitParis,1468920878,,0,18
567,2016-7-19,2016,7,19,19,4tkeer,Suggestion Need for Werkstudent On ML/Big Data,https://www.reddit.com/r/MachineLearning/comments/4tkeer/suggestion_need_for_werkstudent_on_mlbig_data/,naimrajib,1468922872,[removed],0,1
568,2016-7-19,2016,7,19,19,4tkiso,[1607.04331] Random projections of random manifolds,https://www.reddit.com/r/MachineLearning/comments/4tkiso/160704331_random_projections_of_random_manifolds/,bdamos,1468925287,,0,14
569,2016-7-19,2016,7,19,19,4tkjyt,Exploring the Artificially Intelligent Future of Finance,https://www.reddit.com/r/MachineLearning/comments/4tkjyt/exploring_the_artificially_intelligent_future_of/,reworksophie,1468925870,,0,1
570,2016-7-19,2016,7,19,20,4tkon9,Companies in the UK who aren't DeepMind?,https://www.reddit.com/r/MachineLearning/comments/4tkon9/companies_in_the_uk_who_arent_deepmind/,ineedahugepoo,1468928207,"It seems like any nugget of UK-specific machine learning news which hits the media lately is coming from DeepMind, but I'm sure there are many more out there doing cool stuff. I just can't find them...

",35,6
571,2016-7-19,2016,7,19,20,4tkq7e,"how to sample ""partitioning arrangement"" and ""parameter"" in Dirichlet process?  /r/statistics",https://www.reddit.com/r/MachineLearning/comments/4tkq7e/how_to_sample_partitioning_arrangement_and/,[deleted],1468928936,[deleted],0,0
572,2016-7-19,2016,7,19,20,4tkqdb,Implicit parameters GMM in SKLearn? (x-post with r/datascience),https://www.reddit.com/r/MachineLearning/comments/4tkqdb/implicit_parameters_gmm_in_sklearn_xpost_with/,ice_wendell,1468929020,"Original (identical) post [here](https://m.reddit.com/r/datascience/comments/4tis2r/implicit_parameters_gmm_in_sklearn/).

All,

I am trying to fit a GMM (Gaussian Mixture Model) using the python SciKit-Learn library. It's pretty straightforward in all the examples can find, and I have done so successfully on a number of datasets, but I now have (I think) a unique case.

Basically, the parameters for which I want to fit the mixture model are only implicitly defined, so there is no transform to isolate them as data. For example, I am trying to estimate a mixture model over the parameters a, b and c, which are implicit in the equation,

x^a + by^c = z,

where I do have the values for x, y and z in each observation. The actual equation is more complex, but hopefully this makes the problem clear.

So, does anyone have any tips? Code examples? Nudges in a more fruitful direction? If you have experience with solving this type of problem in all sklearn, I'd love to hear about it.

Note: I am bound to GMM as a methodology because I am trying to replicate and then improve upon a published article.

Things I have tried:

""Rolling my own"" GMM by hand coding the EM Algo using numpy and scipy. This either ended in too slow code or bad convergence, so I would much prefer to find a way to hook in to the battle tested code in the sklearn library.

Reading about the Transformer API in sklearn. As far as I can tell, this is only meant to pre-process data, and there is not an apparent way to handle the implicit parameters problem. Am I maybe missing the purpose or correct application of the Transformer Mixin?

Thank you!

Edit:

Clarified that GMM refers to a Gaussian mixture model, not to Generalized Method of Moments. I am referring to [GMM as used within the `sklearn.mixture` module](http://scikit-learn.org/stable/modules/generated/sklearn.mixture.GMM.html).

Edit 2:

Thank you (mods?) for the visual.",6,0
573,2016-7-19,2016,7,19,22,4tl2sy,Luck combining zoneout with input dropout for LSTMs?,https://www.reddit.com/r/MachineLearning/comments/4tl2sy/luck_combining_zoneout_with_input_dropout_for/,SuperFX,1468934414,"I have gotten excellent results using zoneout (both on hidden state and memory cells) and using dropout on the inputs to an LSTM, but I haven't had much luck combining the two. I either have to tone dropout down to very low levels that it makes no difference, or if I have both zoneout and dropout at their usual individual levels (0.9 and 0.5, respectively), I end up with substantial instability in the learning, where the error (both training and validation) is bouncing wildly all over the place. Anyone had luck with getting both to work? Any tricks that helped?",0,3
574,2016-7-19,2016,7,19,22,4tl5lx,What you missed in Big Data: Machine learning gains yet more momentum,https://www.reddit.com/r/MachineLearning/comments/4tl5lx/what_you_missed_in_big_data_machine_learning/,vaughtcourtney,1468935475,,0,0
575,2016-7-19,2016,7,19,23,4tlgez,Blind assessment of threshold for a classifier,https://www.reddit.com/r/MachineLearning/comments/4tlgez/blind_assessment_of_threshold_for_a_classifier/,seesawtron,1468939291,[removed],0,1
576,2016-7-19,2016,7,19,23,4tljnl,GTX 1060 released,https://www.reddit.com/r/MachineLearning/comments/4tljnl/gtx_1060_released/,cvikasreddy,1468940356,,7,4
577,2016-7-20,2016,7,20,1,4tlwwo,[1607.04793] Learning to Decode Linear Codes Using Deep Learning,https://www.reddit.com/r/MachineLearning/comments/4tlwwo/160704793_learning_to_decode_linear_codes_using/,enk100,1468944576,,6,11
578,2016-7-20,2016,7,20,1,4tm0hg,Multi-label Text Classification,https://www.reddit.com/r/MachineLearning/comments/4tm0hg/multilabel_text_classification/,NihilisticFool,1468945698,"I am trying to build a multi-label classifier for suggesting tags on blog posts. The textual data is full of noise. The approach I have been following until now was a BOW approach with Tf-idf weighting. However I could only get an accuracy of around 0.35 on the test set using OnevsRest approach and a SVC. I then decided to eliminate the noise and I applied TextRank on my data to obtain a more meaningful summary and then applied the same BOW approach which however resulted in further loss of accuracy. I am quite new to machine learning and data science in general. Can someone please guide me with the approach? I am looking specifically on how to create my features and what else can I try. I have 12 labels and around 1500 blog posts. I know it isn't much data, but the data-set is still well-balanced and won't be the primary reason for such low accuracy.",9,0
579,2016-7-20,2016,7,20,1,4tm2mu,Good Resource/Read on Search Engine and Machine Learning Application?,https://www.reddit.com/r/MachineLearning/comments/4tm2mu/good_resourceread_on_search_engine_and_machine/,dandanbang,1468946379,"Want to know if people have worked on similar problem or know of any good read on Machine Learning and Search Engine, preferably with Elasticsearch.",0,0
580,2016-7-20,2016,7,20,1,4tm308,"Training a pre-trained NN with new data, incrementally.",https://www.reddit.com/r/MachineLearning/comments/4tm308/training_a_pretrained_nn_with_new_data/,akshayxyz,1468946507,"Hi All,

I have a use-case where I am using a NN based model for Collaborative filtering (specifically RBM based approach), and would like to keep updating the model with new data as frequently as possible.

Initially I considered the naive engineering solution - Keep a moving window of data, and re-train the model every N hours/day. But this is not truly incremental in sense of updating the model which is already trained on almost all but new data.

Then I started exploring whether it makes sense to start from the pre-trained RBM, and update it only with new data. I have few open questions in this option:

- I understand that re-training with just new data will make the network forget information learnt from old data. But as long as forgetting is gradual (decays over time, with control over decay-window), then I am fine with it. Any problems with the assumption that forgetting will be inherently gradual? Or should I explicitly control the forgetting with some kind of time-decayed sampling (older samples less likely to be picked)? Can the forgetting window be controlled w/o explicit time-decayed sampling? (I am not yet prepared to re-think my architecture with RNN/LSTM based architectures.

- When I load the pre-trained model, what learning rate should I start with? Given that previous model would have been trained with some learning-rate decay strategy.

- Any reference on similar attempts/research? I could find only RNN counterpart - Continually running Fully RNN (http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.52.9724&amp;rep=rep1&amp;type=pdf)

Thanks,
Akshay",2,1
581,2016-7-20,2016,7,20,1,4tm3lw,What's the state of the art performance on detecting birds? What was it in 2014? (XKCD reference),https://www.reddit.com/r/MachineLearning/comments/4tm3lw/whats_the_state_of_the_art_performance_on/,AlexCoventry,1468946689,"[This xkcd comic](http://xkcd.com/1425/) got me wondering today, how hard is it, really, to write something with OTS open-source software which finds birds in images?  What is the state of the art for, say, 90% sensitivity and 10^-4 selectivity?

Also, I think that comic was published in 2014, so I'm curious about the same questions given the state of the art at that time.",5,0
582,2016-7-20,2016,7,20,3,4tmjc5,efficient ML workflow?,https://www.reddit.com/r/MachineLearning/comments/4tmjc5/efficient_ml_workflow/,dimmtree,1468951439,"I was wondering what type of workflow you have found efficient? When training deep models it often feels like I'm twiddling my thumbs. 

On my desktop I noticed tensorflow leaves the GPUs very under utilized, while maxing out the cpu. (30% load for 2 Titan Xs, 100% load on all cores i7-3970x)

For distributed training is there a recommended framework? I have been training on my desktop for some time, but gained access to a large k80 gpu cluster recently.",2,0
583,2016-7-20,2016,7,20,3,4tmpl9,[Tribute] An art made by Andr Pinto (Anthill Comics) for our Machine Learning Meetup,https://www.reddit.com/r/MachineLearning/comments/4tmpl9/tribute_an_art_made_by_andr_pinto_anthill_comics/,perone,1468953425,,10,149
584,2016-7-20,2016,7,20,4,4tn10v,Rodeo &amp; ScienceOps Webinar: July 27 @ 2 PM EST,https://www.reddit.com/r/MachineLearning/comments/4tn10v/rodeo_scienceops_webinar_july_27_2_pm_est/,elisebreda,1468956800,,0,0
585,2016-7-20,2016,7,20,4,4tn2e6,What NLP Similarity Measurement To Use For Melanie Trump's And Michelle Obama's Speech?,https://www.reddit.com/r/MachineLearning/comments/4tn2e6/what_nlp_similarity_measurement_to_use_for/,kirakun,1468957210,,9,10
586,2016-7-20,2016,7,20,5,4tnf54,Complex outputs from Deep Learning models,https://www.reddit.com/r/MachineLearning/comments/4tnf54/complex_outputs_from_deep_learning_models/,Mordicon,1468961359,"Recently I have seen deep learning used to create self driving cars ([Comma.ai](https://www.youtube.com/watch?v=KTrgRYa2wbI&amp;t=5m20s)  and [Nvidia](https://www.youtube.com/watch?v=KnVVJSIiKpY&amp;feature=youtu.be&amp;t=1h4m45s) are notable examples).  I am wondering how the Deep Learning model outputs the decisions needed to drive.

&amp;nbsp;

In my mind the simplest outputs needed to drive a car would be the steering wheel angle, the gas pedal, and the brake pedal. However I think that these 3 things could not be decided separately because they effect each other (turning the wheel at your current speed would result in different movement than turning while accelerating). This means that one model must decide on all three of these things. Do these sort of models just have a huge output space? for example the space that is all (plausible steering wheel angles) * (the positions of the gas pedal) * (the positions of the brake pedal). This method seems like it would be very expensive and not scale. In order to signal intent to other drivers you need turn signals which would triple the output size (both off, right signal on, left signal on). 

&amp;nbsp;

The general version of this problem seems to be how a deep learning model can decide on/output multiple related things.
Does anyone know how these kind of outputs work? Is it a huge output space or something I haven't thought of? I would also be interested in any papers on this subject.",3,0
587,2016-7-20,2016,7,20,7,4tnudh,Setting up a self-driving car model: Livestream,https://www.reddit.com/r/MachineLearning/comments/4tnudh/setting_up_a_selfdriving_car_model_livestream/,vanboxel,1468966656,,0,0
588,2016-7-20,2016,7,20,9,4toew0,why do nvidia's gpus have relatively small amounts of memory?,https://www.reddit.com/r/MachineLearning/comments/4toew0/why_do_nvidias_gpus_have_relatively_small_amounts/,jstrong,1468974212,"From my relatively limited experience with deep learning, it appears that it's somewhat easy to run up against the 12gb of vram in a Titan X, which to my knowledge currently has the most memory of any graphics card (soon to be eclipsed by the Titan P). 

Would it be technically difficult to double the ram on a Titan? I don't know enough about how they work to have any sense for whether this is really hard or easy. 

Is there a demand issue? Besides bigger neural nets in deep learning, are any other major uses of GPUs maxing out the memory on these things?  

Are there sophisticated commercial entities employing other methods of training neural networks to avoid memory bottlenecks and increase the size of the models (phi, for example)? 

Or is it that, since only very recently have new methods come along that allow the easy training of extremely deep nets, the possibilities just haven't been fully explored? 

If memory weren't an issue, what would deep learning researchers be trying first? ",19,0
589,2016-7-20,2016,7,20,10,4tomi2,Less training data giving better results? (NLP / Text Generation),https://www.reddit.com/r/MachineLearning/comments/4tomi2/less_training_data_giving_better_results_nlp_text/,butWhoWasBee,1468977259,"Hello, I am doing text generation with LSTM networks using Keras, and I have noticed that when I train on only the first 200k characters of my source novel, I get better results with much less training time (more epochs go by in this lesser time though). When training with very large amounts of data, I almost never get spelling errors, but the generated text tends to repeat the same statements over and over. Could my net be too small, or is it just that I just need to wait for more epochs to go by when using the larger data set? 

My network is 3 hidden LSTM layers, fully connected with 512 nodes. There are about 70 inputs and 70 outputs, so 5 layers total including the in and out layers. I am using 1 hot encoding to pass in characters, and the output is the probability of a given character coming next. The way I generate text is by passing in a seed character and let the net output the probabilities of the next character. I then sample from this with a softmax function, and pass in this character.",4,1
590,2016-7-20,2016,7,20,11,4toz8n,Google Cuts Its Giant Electricity Bill with DeepMind-Powered AI,https://www.reddit.com/r/MachineLearning/comments/4toz8n/google_cuts_its_giant_electricity_bill_with/,iamkeyur,1468982164,,15,24
591,2016-7-20,2016,7,20,11,4tp02d,Deep learning tutorials (containing more than 40 presentations),https://www.reddit.com/r/MachineLearning/comments/4tp02d/deep_learning_tutorials_containing_more_than_40/,samchoi7,1468982475,,11,178
592,2016-7-20,2016,7,20,12,4tpaoa,building construction reinforcing mesh welding machine,https://www.reddit.com/r/MachineLearning/comments/4tpaoa/building_construction_reinforcing_mesh_welding/,susanwang333999,1468986749,,1,0
593,2016-7-20,2016,7,20,13,4tpccb,Generative SDRs,https://www.reddit.com/r/MachineLearning/comments/4tpccb/generative_sdrs/,CireNeikual,1468987471,,7,13
594,2016-7-20,2016,7,20,15,4tpqxr,"Polyurethane injection is a technique to fix the cracks in the foundation walls of buildings, garages or basements. This technique provides a long lasting solution to water leakages and also it is very cost efficient and this process of injection take no time to complete.",https://www.reddit.com/r/MachineLearning/comments/4tpqxr/polyurethane_injection_is_a_technique_to_fix_the/,italartworld001,1468994403,,0,1
595,2016-7-20,2016,7,20,15,4tpwdz,Best practices when doing deep learning for computer vision projects?,https://www.reddit.com/r/MachineLearning/comments/4tpwdz/best_practices_when_doing_deep_learning_for/,ml_newcomer,1468997258,"Hi,

I'm trying to do train an image classifier on a ~2 millions labeled images. The images are scenic, with 20 distinct lables. I'm quite familiar with the concept of the state-of-the-art techinques in image classification (CNN and alike) but lacks the practical experience.

I have some questions:

1\. How do we handle different image sizes? As I know, the first layer of the network will receive the input. Do we have to rescale all the train/test image to the same size first?

2\. I'm not sure about the correctness of the labelling of all the 2 million images. What is a good way to check the label with acceptable accuracy? Do people usually just use a pre-trained models and compare the labels? Or manual check/mechanical turk is the best way?
Is there anything else I should check before using the data to train a model?

3\. I've heard good things about Caffe for vision research projects. But how easy is it to be productionized? How does it compare to Tensorflow?",3,4
596,2016-7-20,2016,7,20,17,4tq71r,Question: Caffe + LBFGS ?,https://www.reddit.com/r/MachineLearning/comments/4tq71r/question_caffe_lbfgs/,fulcrum_xyz,1469003437,[removed],2,0
597,2016-7-20,2016,7,20,17,4tq7dl,Question about Doc2Vec - How does training work ?,https://www.reddit.com/r/MachineLearning/comments/4tq7dl/question_about_doc2vec_how_does_training_work/,datatatatata,1469003654,"Hi,

I just read the paper about [Distributed Representations of Sentences and Documents](http://cs.stanford.edu/~quocle/paragraph_vector.pdf), and I don't understand the training part.  

As far as I know, the ""raw material"" of Word2Vec is a one-hot-encoder of words. Basically, any word is encoded as a very large vector with one 1 and many 0s. Training will lead to compressing these vectors in say ~200 dimension ones.

Now, in doc2vec, I don't understand what the ""raw material"" is. What is the  input ? How is a paragraph ""poorly encoded"" to start with, before a good representation is learned ?

Thank you !",6,0
598,2016-7-20,2016,7,20,17,4tq8xy,"A prediction tool for curious scientists using word2vec, recurrent neural networks and the DBLP data set.",https://www.reddit.com/r/MachineLearning/comments/4tq8xy/a_prediction_tool_for_curious_scientists_using/,[deleted],1469004586,[deleted],0,1
599,2016-7-20,2016,7,20,18,4tqdde,Unreasonable Effectiveness of Counting - Approximating deep learning with simple counts,https://www.reddit.com/r/MachineLearning/comments/4tqdde/unreasonable_effectiveness_of_counting/,pmigdal,1469007239,,1,0
600,2016-7-20,2016,7,20,18,4tqewu,"What will you publish next? A prediction tool for curious scientists using word2vec, recurrent neural networks and the DBLP data set.",https://www.reddit.com/r/MachineLearning/comments/4tqewu/what_will_you_publish_next_a_prediction_tool_for/,whonjayne,1469008253,,1,2
601,2016-7-20,2016,7,20,21,4tr00k,Course on Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/4tr00k/course_on_reinforcement_learning/,minato3421,1469019439,,8,20
602,2016-7-20,2016,7,20,22,4tr0p8,Five most popular similarity measures implementation in python,https://www.reddit.com/r/MachineLearning/comments/4tr0p8/five_most_popular_similarity_measures/,dataaspirant,1469019739,,0,1
603,2016-7-20,2016,7,20,22,4tr13c,Replacing Softmax with Class Sphere Embedding [Franois Chollet - Keras],https://www.reddit.com/r/MachineLearning/comments/4tr13c/replacing_softmax_with_class_sphere_embedding/,[deleted],1469019903,[deleted],1,1
604,2016-7-20,2016,7,20,22,4tr1cy,[1607.05691] Information-theoretical label embeddings for large-scale image classification - Franois Chollet (Keras),https://www.reddit.com/r/MachineLearning/comments/4tr1cy/160705691_informationtheoretical_label_embeddings/,Bardelaz,1469020004,,23,20
605,2016-7-20,2016,7,20,22,4tr3px,"Mission.ai goal is to accelerate implementation of AI into industries. Connect leaders from corporate giants to small local shops and - AI startups, innovative AI businesses into working together. http://mission.ai",https://www.reddit.com/r/MachineLearning/comments/4tr3px/missionai_goal_is_to_accelerate_implementation_of/,Mission_AI,1469021015,,1,1
606,2016-7-20,2016,7,20,22,4tr4jy,VU Amsterdam or Double degree program?,https://www.reddit.com/r/MachineLearning/comments/4tr4jy/vu_amsterdam_or_double_degree_program/,[deleted],1469021365,[removed],0,0
607,2016-7-20,2016,7,20,23,4trb82,Matching words to their speech.,https://www.reddit.com/r/MachineLearning/comments/4trb82/matching_words_to_their_speech/,onemanforeachvill,1469023963,"Hi,

Suppose you have a text of a speech, the start time of the very first word, and the audio of someone speaking the speech.  How easy is it to identify from the audio when the speaker moves from one word to the next?

An application would be to highlight a word as it is spoken by the speaker.  

Any pointers or papers on how to do this?

Thanks for any help.
",2,1
608,2016-7-20,2016,7,20,23,4trbki,Approaching (Almost) Any Machine Learning Problem,https://www.reddit.com/r/MachineLearning/comments/4trbki/approaching_almost_any_machine_learning_problem/,abhisvnit,1469024089,,0,0
609,2016-7-21,2016,7,21,0,4trk36,Seeking Postdocs for Deep Learning Research (including Deep Reinforcement Learning),https://www.reddit.com/r/MachineLearning/comments/4trk36/seeking_postdocs_for_deep_learning_research/,jclune,1469027112,[removed],1,1
610,2016-7-21,2016,7,21,0,4trmky,Why aren't line search algorithms used in optimizing neural networks?,https://www.reddit.com/r/MachineLearning/comments/4trmky/why_arent_line_search_algorithms_used_in/,chillwombat,1469027925,"As I understand, usually the step size (or learning rate) is kept fairly fixed or varied slowly in machine learning. In other optimization problems line search algorithms are frequently used to determine the best step size. 

I'm doing a non-machine learning optimization with millions of parameters and am thinking about applying optimization methods used in ML, but I'm not sure if I should use line search methods (for some algorithms they don't seem to work at all...)

Currently it seems that for my problem, the best way is to make a lot of fixed steps (where the direction is found by x algorithm; e.g. CG; L-BFGS, nesterov's accelerated gradient descent or just  plain simple gradient direction) and the occasionally do a line search in the direction of -gradient (and it takes a huge step, sometimes even 10^5 times bigger than the fixed step).",19,8
611,2016-7-21,2016,7,21,1,4trypa,An experiment in trying to predict Google rankings  /r/programming,https://www.reddit.com/r/MachineLearning/comments/4trypa/an_experiment_in_trying_to_predict_google/,dabshitty,1469031897,,2,7
612,2016-7-21,2016,7,21,1,4ts413,"If only people knew they were using cutting edge AI: ""Prisma will make you fall in love with photo filters all over again""",https://www.reddit.com/r/MachineLearning/comments/4ts413/if_only_people_knew_they_were_using_cutting_edge/,omniron,1469033632,,0,0
613,2016-7-21,2016,7,21,2,4tsfok,Rule of thumb for # of LSTM/GRU units + layers?,https://www.reddit.com/r/MachineLearning/comments/4tsfok/rule_of_thumb_for_of_lstmgru_units_layers/,fullon604,1469037550,"One can of course do a bit of hyper-parameter searching, and I'm mindful of what will work with my GPU constraints, but is there any good theoretical or practical advice here? Expressive power per parameter, less training/running time, over-vs-under-fitting, etc? Difference for a sequence of words or numbers, vs something with some spatial content like the top layer of a convnet? 

eg - Is it tied to the dimensions of the input vector? Batch size? Tied to the number of samples in the data set? Some [0-100%] ratio of layers and/or units per layer vs the maximum likely input or output sequence length (words per sentence, frames in audio/video clips, etc)?

Would you ever have FC/ReLU/dropout/etc layers sandwiched in a stack of LSTMs (or a sequence of LSTMs)? Repeating Inception-ish LSTM blocks? Other approaches?
",3,5
614,2016-7-21,2016,7,21,3,4tsi2n,Udacity Self-Driving Car Engineer Nanodegree - just EMail notification for now,https://www.reddit.com/r/MachineLearning/comments/4tsi2n/udacity_selfdriving_car_engineer_nanodegree_just/,badhri,1469038348,,45,65
615,2016-7-21,2016,7,21,3,4tsl58,Machine Learning over 1M hotel reviews finds interesting insights,https://www.reddit.com/r/MachineLearning/comments/4tsl58/machine_learning_over_1m_hotel_reviews_finds/,wildcodegowrong,1469039380,,5,4
616,2016-7-21,2016,7,21,3,4tsoi3,Noob question about gradient descent and using parallax/triangulation to estimate step size,https://www.reddit.com/r/MachineLearning/comments/4tsoi3/noob_question_about_gradient_descent_and_using/,jpfed,1469040463,"I've heard of gradient step size adaptation by seeing how similar one gradient direction is to the previous, or by estimating local curvature, but I have another idea for gradient step size adaptation. It's simple enough that it must already exist, but I don't know what it's called or how it performs.

The idea is, don't step along the gradient. Step perpendicular to the gradient and make another gradient measurement.  Your gradient measurements define a couple lines; step to the where those lines make their closest approach to one another.

What is this called? Or is it so obviously defective that no one has bothered to study it?",7,0
617,2016-7-21,2016,7,21,4,4tsspg,NMF(Non-negative Matrix Factorization) for topic extraction,https://www.reddit.com/r/MachineLearning/comments/4tsspg/nmfnonnegative_matrix_factorization_for_topic/,pouria3,1469041829,,0,0
618,2016-7-21,2016,7,21,5,4tt378,Elements of Statistical Learning: Reading Group,https://www.reddit.com/r/MachineLearning/comments/4tt378/elements_of_statistical_learning_reading_group/,[deleted],1469045269,[deleted],1,0
619,2016-7-21,2016,7,21,5,4tt3ii,[Discussion] Link flairs now in /r/MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4tt3ii/discussion_link_flairs_now_in_rmachinelearning/,olaf_nij,1469045374,[removed],0,1
620,2016-7-21,2016,7,21,5,4tt4pu,[Discussion] /r/MachineLearning tagging posts.,https://www.reddit.com/r/MachineLearning/comments/4tt4pu/discussion_rmachinelearning_tagging_posts/,olaf_nij,1469045765,[removed],0,1
621,2016-7-21,2016,7,21,5,4tt9pa,"Done with the Coursera Machine Learning course, what now?",https://www.reddit.com/r/MachineLearning/comments/4tt9pa/done_with_the_coursera_machine_learning_course/,pulkitmaloo,1469047473,[removed],3,3
622,2016-7-21,2016,7,21,10,4tuks5,"High Quality, High Performance Clustering with HDBSCAN",https://www.reddit.com/r/MachineLearning/comments/4tuks5/high_quality_high_performance_clustering_with/,lmcinnes,1469065028,,0,25
623,2016-7-21,2016,7,21,10,4tul9f,"New TensorFlow Book! ""TensorFlow for Machine Intelligence""",https://www.reddit.com/r/MachineLearning/comments/4tul9f/new_tensorflow_book_tensorflow_for_machine/,TheTwigMaster,1469065227,,44,113
624,2016-7-21,2016,7,21,12,4tv1mi,Deeplearning4j - what do you think of this framework?,https://www.reddit.com/r/MachineLearning/comments/4tv1mi/deeplearning4j_what_do_you_think_of_this_framework/,ill-logical,1469071872,"If we were to go by Github stars, it's fairly popular, but I don't see it discussed in this subreddit too much.

Is DL4j comparable to Torch and the rest in awesomeness?

Is JVM in general a viable platform for deep learning, or does it impose unforgivable overhead when moving data from, say, its internal Tensor class to the native world, where CUDA lives?",37,8
625,2016-7-21,2016,7,21,14,4tvhqb,[1606.05579] Early Visual Concept Learning with Unsupervised Deep Learning,https://www.reddit.com/r/MachineLearning/comments/4tvhqb/160605579_early_visual_concept_learning_with/,downtownslim,1469079562,,3,17
626,2016-7-21,2016,7,21,14,4tvjqb,[1606.03657] InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets,https://www.reddit.com/r/MachineLearning/comments/4tvjqb/160603657_infogan_interpretable_representation/,downtownslim,1469080579,,2,22
627,2016-7-21,2016,7,21,14,4tvk0n,Could someone please explain`conv2d_transpose` in TensorFlow?,https://www.reddit.com/r/MachineLearning/comments/4tvk0n/could_someone_please_explainconv2d_transpose_in/,woobart,1469080708,[removed],0,1
628,2016-7-21,2016,7,21,16,4tvwv5,How to make your chat-bot more human,https://www.reddit.com/r/MachineLearning/comments/4tvwv5/how_to_make_your_chatbot_more_human/,kiote_the_one,1469087532,"I have an addiction to chat-bots, as you can see from this series of articles: [one](https://kiote.io/2016/04/28/why-do-you-need-a-bot/), [two](https://kiote.io/2016/02/05/simplest-slack-bot-of-yours/).

They can be really useful, but a bit boring. They usually have some **pre-defined answers** to share with you, depending on the situation. As an example, you could imagine an umbrella-bot. You might ask it, Do I need an umbrella today? and it will tell you yes or no, depending on the latest weather report.

But what if you want to add some **individuality** to your bot? Say you want to make it more human-like. You dont want it to pass the Turing test necessarily, but you still want your users to feel something when they are interacting with your bot.

**What does it actually mean to be more human?**

Well, no offence, but it usually means not optimal. And you actually can get the list of **non-optimal behavior** here: [List of cognitive biases](https://en.wikipedia.org/wiki/List_of_cognitive_biases)

For some inspiration, you can create a too [optimistic](https://en.wikipedia.org/wiki/Optimism_bias) bot from our example with the umbrella-bot. Or maybe youd rather try a [cynical](https://en.wikipedia.org/wiki/Na%C3%AFve_cynicism) one? You choose!",5,0
629,2016-7-21,2016,7,21,18,4tw70i,How to compare 2 bags of words?,https://www.reddit.com/r/MachineLearning/comments/4tw70i/how_to_compare_2_bags_of_words/,perceptron01,1469093814,[removed],1,2
630,2016-7-21,2016,7,21,20,4twipa,[1607.04228] Fifty Shades of Ratings: How to Benefit from a Negative Feedback in Top-N Recommendations Tasks,https://www.reddit.com/r/MachineLearning/comments/4twipa/160704228_fifty_shades_of_ratings_how_to_benefit/,bdamos,1469100632,,0,3
631,2016-7-21,2016,7,21,20,4twm4t,Sequence to sequence models toturial in pycnn,https://www.reddit.com/r/MachineLearning/comments/4twm4t/sequence_to_sequence_models_toturial_in_pycnn/,[deleted],1469102397,[deleted],0,1
632,2016-7-21,2016,7,21,21,4twmox,Is there a model for automated recovery of notes from raw music (mp3 perhaps) ?,https://www.reddit.com/r/MachineLearning/comments/4twmox/is_there_a_model_for_automated_recovery_of_notes/,xingdongrobotics,1469102640,"Possible inputs are mp3 files of raw music or songs, and the outputs are the stave  for melodies",4,6
633,2016-7-21,2016,7,21,21,4twnl0,Sequence to sequence tutorial in PyCNN,https://www.reddit.com/r/MachineLearning/comments/4twnl0/sequence_to_sequence_tutorial_in_pycnn/,talbaumel,1469103061,"Overview of RNNs, encoder-decoders and attention models.
Have fun, let me know what you think

https://www.cs.bgu.ac.il/~talbau/notebooks/attention.html",3,4
634,2016-7-21,2016,7,21,21,4twoo0,A Beginner's Guide To Understanding Convolutional Neural Networks,https://www.reddit.com/r/MachineLearning/comments/4twoo0/a_beginners_guide_to_understanding_convolutional/,perceptron01,1469103600,,23,234
635,2016-7-21,2016,7,21,21,4twqg6,"Generative Adversarial Networks vs Variational Autoencoders, who will win?",https://www.reddit.com/r/MachineLearning/comments/4twqg6/generative_adversarial_networks_vs_variational/,rantana,1469104380,"It seems these days that for every GAN paper there's a complementary VAE version of that paper. Here's a few examples:

disentangling task:
https://arxiv.org/abs/1606.03657
https://arxiv.org/abs/1606.05579

semisupervised learning:
https://arxiv.org/abs/1606.03498
https://arxiv.org/abs/1406.5298

plain old generative models:
https://arxiv.org/abs/1312.6114
https://arxiv.org/abs/1511.05644

The two approaches seem to be fundamentally completely different ways of attacking the same problems. Is there something to takeaway from all this? Or will we just keep seeing papers going back and forth between the two?",17,32
636,2016-7-21,2016,7,21,23,4tx4pu,LSTM for predicting time series data,https://www.reddit.com/r/MachineLearning/comments/4tx4pu/lstm_for_predicting_time_series_data/,DarkPhalanx,1469110000,"I am trying to predict the probabilistic location of a person, given a time series of various sensor readings (from accelerometers and cameras). Based on these readings, I want to output the probability of the presence of the person in different zones, whose locations I know. 

Since LSTMs learn from the context, I think they are suited for this task. However, almost all the tutorials I've encountered come from an NLP point of view. Translating these examples to time series is not exactly one to one.

My inputs are in the form of batches (one entire series of readings for each person). However, they are not of uniform length due to different number of measurements. However, the number of sensors I have is fixed.

So far, I've looked at the documentation of keras and this is what I've done:

Each person's data is stored in a separate numpy array. For example, one array of size 1500x20 (1500 measurements, 20 sensors), another array is of size 1700x20. Since the input LSTM layer requires the data to be three dimensional, I expanded the dimensions along the first axis. So the size becomes 1x1500x20.

My model in keras is:

    model = Sequential()
    model.add((LSTM(output_shape, input_shape=input_shape)))
    model.add(Dense(output_shape))
    model.add(Activation('sigmoid'))

where `input_shape = (1, 1500, 20)` and `output_shape = 5`.  The dense layer is used for classification as I have the target probabilities for the outputs (the probabilities of the five locations). 

This doesn't work as keras complains that it expected an input with three dimensions, but the number of dimensions being passed in are 4. Does anyone know where the extra dimension is coming from?

Also, how does keras handle batch data? I know there is a `train_on_batch`, but could someone explain how to use it? Is it suited for sending in data at each time step?

Thanks",3,3
637,2016-7-21,2016,7,21,23,4tx8pu,"ToraxXx/gsdr: Generative Sparse Distributed Representations, a fast generative model written in Python",https://www.reddit.com/r/MachineLearning/comments/4tx8pu/toraxxxgsdr_generative_sparse_distributed/,fergbyrne,1469111441,,6,14
638,2016-7-21,2016,7,21,23,4tx9mj,Using two different GPUs in DL workstation,https://www.reddit.com/r/MachineLearning/comments/4tx9mj/using_two_different_gpus_in_dl_workstation/,spurious_recollectio,1469111757,"Sorry for another DL workstation question (maybe we should have a link on the side-bar with an up-to-date build suggestion) but some part of this question is non-standard (mixing GPUs).

I'm planning to build a new DL workstation in about a month.  I'm getting some components together now cause I can get them a bit cheaper abroad (i live in europe with high VAT).

My initial goal is to build a very expandable multi-GPU workstation.  I currently have a 980 Ti but think I want to get hold of a 1070 now.    I'm still not sure how much better the 1080 is for DL but 1070 seems the right compromise for me right now (unless someone strongly recommends otherwise).

So one question is: is there any problem putting a 980 Ti and a 1070 on the same machine?  I would generally be running two different networks on them (hyperparameter tuning, etc...), not splitting one net between the two.  Also I work in theano so I'm curious how well supported something like this might be (two networks on two different GPUs).

A final question is about the architecture.  I generally see (e.g. [here](http://graphific.github.io/posts/building-a-deep-learning-dream-machine/)) something like an I7-5930K on Asus X99-E WS motherboard recommended to get a lot of PCI lanes but this is a rather expensive combo.  Is there any good skylake alternative to this now?  Or is the 5820k a viable alternative?
",7,0
639,2016-7-22,2016,7,22,0,4txidp,"Google Sprints Ahead in AI Building Blocks, Leaving Rivals Wary",https://www.reddit.com/r/MachineLearning/comments/4txidp/google_sprints_ahead_in_ai_building_blocks/,perceptron01,1469114634,,1,0
640,2016-7-22,2016,7,22,0,4txpb2,Investing in artificial intelligence: What's all the fuss about?,https://www.reddit.com/r/MachineLearning/comments/4txpb2/investing_in_artificial_intelligence_whats_all/,nb410,1469116795,,0,0
641,2016-7-22,2016,7,22,1,4txzyu,Features for relation extraction,https://www.reddit.com/r/MachineLearning/comments/4txzyu/features_for_relation_extraction/,spurious_recollectio,1469120261,"I've seen many papers reference the idea of using standard NLP features for relation extraction -- dependency parse, pos, entities, etc...  I've rarely found a paper with a very specific description of _how_ these features are used.  The main issue is that some of the features are of unspecified length.  E.g. if I have two entities in a sentence the dependency path between them is not necessarily a fixed length.  Do people just use some fixed window?  Obviously one could use some kind of sequence model like an RNN but my feeling is that most of the models people use are linear classifiers built on top of hand-engineered feature templates.  So can anyone point me to an example of such a template?",2,1
642,2016-7-22,2016,7,22,4,4tywgn,Where and how to start?,https://www.reddit.com/r/MachineLearning/comments/4tywgn/where_and_how_to_start/,DDerTyp,1469131090,[removed],4,1
643,2016-7-22,2016,7,22,6,4tzaem,Tutorial on Inference Networks,https://www.reddit.com/r/MachineLearning/comments/4tzaem/tutorial_on_inference_networks/,dustintran,1469135902,,0,19
644,2016-7-22,2016,7,22,8,4tztox,Laptop with strong graphics card for ML on Ubuntu usage ?,https://www.reddit.com/r/MachineLearning/comments/4tztox/laptop_with_strong_graphics_card_for_ml_on_ubuntu/,OmegawOw,1469142979,"I've been looking for a good laptop with strong graphics capabilities for running ML algorithms on it.

I heard System76 and Dell XPS dev are both built for Ubuntu and have good graphical capabilities.

Are there any other options ? What would you guys recommend ?",9,0
645,2016-7-22,2016,7,22,10,4u0a74,[1607.06450] Layer Normalization,https://www.reddit.com/r/MachineLearning/comments/4u0a74/160706450_layer_normalization/,alecradford,1469149491,,15,66
646,2016-7-22,2016,7,22,10,4u0awn,Deep Learning For Better Understanding Of Consumer Behavior And Preferences,https://www.reddit.com/r/MachineLearning/comments/4u0awn/deep_learning_for_better_understanding_of/,JoeyRob,1469149782,,0,1
647,2016-7-22,2016,7,22,10,4u0dgt,Online text-editor with real-time syntax parsing.,https://www.reddit.com/r/MachineLearning/comments/4u0dgt/online_texteditor_with_realtime_syntax_parsing/,[deleted],1469150821,[deleted],1,6
648,2016-7-22,2016,7,22,10,4u0dm9,Using two TitanX GPUs from inside a VM,https://www.reddit.com/r/MachineLearning/comments/4u0dm9/using_two_titanx_gpus_from_inside_a_vm/,mayank1123,1469150885,My lab recently got a new computer with 2 TitanX GPUs. I tried talking to the systems support and they do not support Linux OS. I could use a VM but I wanted to ask if anyone has tried doing this. Can I run things on the GPU from a VM. Any help would be much appreciated ! Thank you. ,14,0
649,2016-7-22,2016,7,22,10,4u0eps,Session 1 for #CADL Creative Applications of Deep Learning now live. New Session/Code every 2 weeks.,https://www.reddit.com/r/MachineLearning/comments/4u0eps/session_1_for_cadl_creative_applications_of_deep/,pkmital,1469151353,,0,2
650,2016-7-22,2016,7,22,11,4u0l57,Newbie at machine learning and Weka. Can someone help?,https://www.reddit.com/r/MachineLearning/comments/4u0l57/newbie_at_machine_learning_and_weka_can_someone/,[deleted],1469153905,[removed],11,0
651,2016-7-22,2016,7,22,11,4u0ljd,Nvidia launches pascal Titanx GPU at an AI meetup - Suprise Launch!,https://www.reddit.com/r/MachineLearning/comments/4u0ljd/nvidia_launches_pascal_titanx_gpu_at_an_ai_meetup/,nigh8w0lf,1469154061,"Nvidia's Jensen launched the pascal Titanx GPU at a Stanford AI meetup
Live Cast:http://www.ustream.tv/channel/fWbQyaEMfbh

Details from the Nvidia blog post link below:
11 TFLOPS FP32,
44 TOPS INT8 (new deep learning inferencing instruction),
12B transistors,
3,584 CUDA cores at 1.53GHz (versus 3,072 cores at 1.08GHz in previous TITAN X),
Up to 60% faster performance than previous TITAN X,
High performance engineering for maximum overclocking,
12 GB of GDDR5X memory (480 GB/s)",71,103
652,2016-7-22,2016,7,22,12,4u0ric,Question about Probabilistic Least Squares,https://www.reddit.com/r/MachineLearning/comments/4u0ric/question_about_probabilistic_least_squares/,[deleted],1469156435,[deleted],2,4
653,2016-7-22,2016,7,22,12,4u0t39,Introduction to Deep Learning for Natural Language Processing using Keras(Python),https://www.reddit.com/r/MachineLearning/comments/4u0t39/introduction_to_deep_learning_for_natural/,rousemaga,1469157092,,3,43
654,2016-7-22,2016,7,22,14,4u1838,Offline Handwriting Recognition,https://www.reddit.com/r/MachineLearning/comments/4u1838/offline_handwriting_recognition/,antmandan,1469164417,"Hi ML redditors. I'm beginning a project that involves the digitization of handwritten English letters. Rather than reinvent the wheel I was wondering if anyone is aware of software that can accept a scanned page of writing, automatically detect words on the page and sequence them, and then (but not essential) provide an output guess of the word with a confidence score? I know that LSTM and other DNN based models have been getting good success on this problem, and would be fine implementing such an approach as the last stage, however the pre-work that needs to be done to segment the text ready to be fed through such a system is the bit I'd like to avoid implementing if there is an existing solution somewhere? Any leads appreciated. ",4,1
655,2016-7-22,2016,7,22,14,4u1ask,Tensorflow and Caffe Softmax,https://www.reddit.com/r/MachineLearning/comments/4u1ask/tensorflow_and_caffe_softmax/,snapillar,1469165780,"Hi all, I have one question about softmax function in tensorflow.
In caffe, the softmax with loss is defined as below.

loss -= log(std::max(prob_data[i * dim + label_value * inner_num_ + j],  Dtype(FLT_MIN)));

This function makes softmax loss pixel by pixel.
In tensorflow, is this function exactly same as function, named 'sparse_softmax_cross_entropy_with_logits'?

",2,0
656,2016-7-22,2016,7,22,14,4u1bha,Who is the most influential computer scientist in the history of Machine Learning,https://www.reddit.com/r/MachineLearning/comments/4u1bha/who_is_the_most_influential_computer_scientist_in/,thunderking500,1469166128,[removed],0,1
657,2016-7-22,2016,7,22,14,4u1bvm,"A question about an equation in ""Variational Bayesian Inference with Stochastic Search""",https://www.reddit.com/r/MachineLearning/comments/4u1bvm/a_question_about_an_equation_in_variational/,jazzsaxmafia,1469166316,"Hello, I am reading this paper titled ""Variational Bayesian Inference with Stochastic Search"".

In the paper, after equation (5),  there is a sentence saying,
""We use the identity q(|) = q(|) ln q(|)"".

I have no idea how that is the case. 
Could anyone help me derive that identity?

Thank you.",3,3
658,2016-7-22,2016,7,22,16,4u1ppz,MrMiss: an optimal missing data razor,https://www.reddit.com/r/MachineLearning/comments/4u1ppz/mrmiss_an_optimal_missing_data_razor/,godspeed_china,1469174225,"http://www.drwang.top/MrMiss.7z  
I meet a problem when I got a dataset with many missing data.  
Thus I developed this software to optimally omit some rows and/or columns. The objective function is to maximize the total number of elements in a submatrix without missing data. I achieved this goal by a genetic algorithm with a windows implementation.  
It also has some fine tuning parameters: row and columns weights to protect some important sample/columns against the razor. A column-versus-row importance to obtain more variables while losing more rows.  
I love this subreddit so I post it here rather than /r/statistics ^_^

",5,10
659,2016-7-22,2016,7,22,18,4u1xuv,advice for new trend or study point in community detection or social network analysis,https://www.reddit.com/r/MachineLearning/comments/4u1xuv/advice_for_new_trend_or_study_point_in_community/,wxyyxc1992,1469179195,,0,1
660,2016-7-22,2016,7,22,20,4u283q,Program to read the NIST dataset (note: Not MNIST),https://www.reddit.com/r/MachineLearning/comments/4u283q/program_to_read_the_nist_dataset_note_not_mnist/,dangmanhtruong,1469185316,"I want to try out what the authors did in the preprocessing step to create MNIST from the NIST dataset. And to do that I need to at least understand what the NIST looks like. Unfortunately it seems that they use ""Wavelet Scalar Quantization algorithm"" for compression, so I don't really know what to do. If possible I want to spend my time on the preprocessing steps as described by the authors, to understand the process involved. So it would be great if there is some code to read the NIST dataset. Please help me, thank you very much :( ",10,0
661,2016-7-22,2016,7,22,20,4u29y8,Understanding the preprocessing steps in the creation of MNIST dataset,https://www.reddit.com/r/MachineLearning/comments/4u29y8/understanding_the_preprocessing_steps_in_the/,dangmanhtruong,1469186311,"I'm trying to understand the various steps used to create the MNIST dataset, but the authors's explanation is not really straightforward: 
""The original black and white (bilevel) images from NIST were size normalized to fit in a 20x20 pixel box while preserving their aspect ratio. The resulting images contain grey levels as a result of the anti-aliasing technique used by the normalization algorithm. the images were centered in a 28x28 image by computing the center of mass of the pixels, and translating the image so as to position this point at the center of the 28x28 field.""

1) ""The original black and white (bilevel) images from NIST were size normalized to fit in a 20x20 pixel box while preserving their aspect ratio."" So I have to ""size normalize"" the images to fit a box, and still preserve their aspect ratio (I assume they mean the aspect ratio of the digits, not the entire image). I'm clueless :( 

2) Center of mass: I have found some online code about this, but I don't think I understand the principle. Here is my take on this: The coordinate of each pixel is actually a vector from the origin to that point, so for each point you multiply the coordinate with the image intensity, then sum everything, before dividing by the total intensity of the image. I may be wrong about this :(

3)Translating the image so as to position this point at the center: Maybe cook up some translation equation, or maybe use a convolutional filter to facilitate translation, then find a path that leads to the center (Dijikstra's shortest path ?).

Obviously I don't fully understand what the authors are saying. Can anyone explain to me in details? Also it would be great if there is some code for this :( ",12,3
662,2016-7-22,2016,7,22,22,4u2o5c,any advice for new trend in community detection?,https://www.reddit.com/r/MachineLearning/comments/4u2o5c/any_advice_for_new_trend_in_community_detection/,wxyyxc1992,1469192988,[removed],0,1
663,2016-7-22,2016,7,22,23,4u30xb,How much of neural network research is being motivated by neuroscience? How much of it should be?,https://www.reddit.com/r/MachineLearning/comments/4u30xb/how_much_of_neural_network_research_is_being/,rantana,1469197820,"DeepMind seems to be making a lot of connections to neuroscience with their recent papers:

http://www.cell.com/trends/cognitive-sciences/fulltext/S1364-6613(16)30043-2

http://arxiv.org/abs/1606.05579

https://arxiv.org/abs/1606.04460

Even Yoshua Bengio, who as far as I can tell didn't have a neuroscience background, is first authoring papers about this connection:

**""Feedforward Initialization for Fast Inference of Deep Generative Networks is biologically plausible""**
http://arxiv.org/abs/1606.01651

There's MANY more papers, the Cell paper gives a good list of references. So I wonder how much future work in machine learning will connect to biology?

Yann LeCun mentioned that ""And describing it like the brain gives a bit of the aura of magic to it, which is dangerous.""


Also, note I make these discussion threads just for interesting conversation. I'm not trying to say one view is right or wrong, but I really like seeing the wide perspective of the community here.",22,21
664,2016-7-22,2016,7,22,23,4u32mg,1D Generative Adversarial Networks: demo and Jupyter notebook,https://www.reddit.com/r/MachineLearning/comments/4u32mg/1d_generative_adversarial_networks_demo_and/,[deleted],1469198417,[deleted],0,1
665,2016-7-22,2016,7,22,23,4u34nm,Spatial Pooling: Input Space &amp; Connections,https://www.reddit.com/r/MachineLearning/comments/4u34nm/spatial_pooling_input_space_connections/,numenta,1469199126,,0,0
666,2016-7-23,2016,7,23,2,4u3xkg,[1606.04934] Improving Variational Inference with Inverse Autoregressive Flow,https://www.reddit.com/r/MachineLearning/comments/4u3xkg/160604934_improving_variational_inference_with/,ajmooch,1469208708,,9,14
667,2016-7-23,2016,7,23,2,4u3xnm,Which regression models to use to perform better than random forest regression and GBRTs?,https://www.reddit.com/r/MachineLearning/comments/4u3xnm/which_regression_models_to_use_to_perform_better/,Zeekawla99ii,1469208736,"I am work with some regression models using random forest regressors and gradient boosted regression trees, GBRTs. 

What other regression models does one try after these in order to improve performance? ",5,2
668,2016-7-23,2016,7,23,3,4u45tf,Simple Residual Building Block for Keras,https://www.reddit.com/r/MachineLearning/comments/4u45tf/simple_residual_building_block_for_keras/,aqwin,1469211403,,6,30
669,2016-7-23,2016,7,23,4,4u4k0w,How to determine individual contribution when data is aggregated by team,https://www.reddit.com/r/MachineLearning/comments/4u4k0w/how_to_determine_individual_contribution_when/,wgpubs,1469216131,"I want to know the individual contribution of each salesperson in terms of actual sales where the dataset I have aggregates sales data by sales ""team"".

The dataset looks as follows:

Month | # of Sales | SalesPerson1 | SalesPerson2 | SalesPerson3

1 | 25 | Fred | Susan | Tom

1 | 12 | John | Wayne | Connor

2 | 16 | Fred | John | Wayne

2 | 11 | Susan | Tom | Winona

3 | 34 | Connor | Susan | Tom


Is there a way to predict the individual contribution of any given salesperson based on the data above where the membership of any given sales team can change from one month to the next?

Thanks!",4,2
670,2016-7-23,2016,7,23,5,4u4tgd,Is Open NN the best way to start working with neural networks on windows?,https://www.reddit.com/r/MachineLearning/comments/4u4tgd/is_open_nn_the_best_way_to_start_working_with/,RoamBear,1469219388,"Open NN is the only c++ NN package I've seen for windows, does anyone with experience know if its the standard package to use or are their other better ones? If you have used it whats your experience been like?",6,3
671,2016-7-23,2016,7,23,6,4u4zrl,How large of a corpus do I need to run a successful GAN?,https://www.reddit.com/r/MachineLearning/comments/4u4zrl/how_large_of_a_corpus_do_i_need_to_run_a/,windweller,1469221605,"I'm quite new to generative models! I've heard it takes a long time (and a very large corpus) to train a generative deocder (in NLP seq2seq learning) to output reasonable sentences. I have a very moderate corpus (40,000 training pairs, sentences), and I wonder if GAN requires less training examples than a traditional Seq2Seq training!",2,0
672,2016-7-23,2016,7,23,6,4u509w,Five Things I Learned at CVPR 2016,https://www.reddit.com/r/MachineLearning/comments/4u509w/five_things_i_learned_at_cvpr_2016/,amplifier_khan,1469221789,,1,0
673,2016-7-23,2016,7,23,6,4u519b,I need an Artificial Intelligence Bullshit Meter,https://www.reddit.com/r/MachineLearning/comments/4u519b/i_need_an_artificial_intelligence_bullshit_meter/,amplifier_khan,1469222132,,2,0
674,2016-7-23,2016,7,23,6,4u52ej,"Applied machine learning researchers, what do you think of theoretical conferences like COLT?",https://www.reddit.com/r/MachineLearning/comments/4u52ej/applied_machine_learning_researchers_what_do_you/,filigreed_is_good,1469222550,"Do you glance at the titles and peek at the abstracts of whatever looks interesting? Do you read some papers? Do you ignore it entirely?

Add ICML in there if you want. (I'm an early grad student in learning theory - I'm curious.)",2,0
675,2016-7-23,2016,7,23,6,4u56sh,Tales from ICML,https://www.reddit.com/r/MachineLearning/comments/4u56sh/tales_from_icml/,amplifier_khan,1469224153,,0,5
676,2016-7-23,2016,7,23,6,4u57ng,Approaching (Almost) Any Machine Learning Problem,https://www.reddit.com/r/MachineLearning/comments/4u57ng/approaching_almost_any_machine_learning_problem/,emzeq,1469224453,,21,87
677,2016-7-23,2016,7,23,6,4u57u1,"Input distances, output latitude and longitude",https://www.reddit.com/r/MachineLearning/comments/4u57u1/input_distances_output_latitude_and_longitude/,lanemik,1469224521,"Hey. So I've got a hackathon coming up and I had an idea, but I'm falling short of figuring out how to implement it. Given the (approximate) distances from 3 or more known locations (presumably set up so that the inputs are meaningful), use a learner rather than trilateration to determine the positioning.

My first idea was an ANN. Inputs could be the distances and other relevant information and the output layer could have 2 nodes, latitude and longitude. But, alas, I've only ever done this in class and that was for a classifier and the more I think of it, this isn't a classification problem.

I suppose it *could* be a classification problem. If I can divide the area I'm interested in into discrete blocks of a suitable size (and for my scenario, I can), then I could have one learner for latitude and one for longitude (the classifiers being ""am I in this strip of latitude or not? Am I in this strip of longitude or not?""). I'm working on testing out some artificial scenarios with this regard right now.

But I feel like I might be barking up the wrong tree. It occurred to me that this is a regression problem. Alas, I'm a regression noob. Can I do a 2D regression? If so, can I do that with an ANN or should I be looking into a different kind of network? If I can use an ANN, do I need to do anything special for the backprop step?

I wouldn't mind learning/using Tensor Flow (because hackathon + TF = all the wins). Is there some documentation in that which would get me working on this problem?

Thanks!",3,0
678,2016-7-23,2016,7,23,13,4u6nmc,"Hey guys, I tried to make a Word Level text generator using keras, check it out and please suggest me how should I improve it.",https://www.reddit.com/r/MachineLearning/comments/4u6nmc/hey_guys_i_tried_to_make_a_word_level_text/,rulerofthehell,1469246443,,6,10
679,2016-7-23,2016,7,23,13,4u6twl,Hosted by DeepMind: Workshop on Contemporary Neural Network Models,https://www.reddit.com/r/MachineLearning/comments/4u6twl/hosted_by_deepmind_workshop_on_contemporary/,downtownslim,1469249796,,6,14
680,2016-7-23,2016,7,23,13,4u6tzj,(x-post) at what mathematical level did you feel comfortable learning ML related things?,https://www.reddit.com/r/MachineLearning/comments/4u6tzj/xpost_at_what_mathematical_level_did_you_feel/,dreamsofanothershore,1469249841,"Tried MLquestions, didn't get too many responses, so I thought I'd try here. if it's against the rules, sorry. please delete the post mods, thanks.




Hello, I've been lurking /r/machinelearning for awhile and always see these tutorials about learning ML and related things. I'm very interested in learning about ML, but I can't shake the feeling that these resources are underestimating the required mathematical knowledge to fully understand.  




From the ones I've taken a look at, they seem very formula applying and not very rigorous, just teaching about how to program them in R or python. For background info, I'm a CS major at at university in California, so i'm quite comfortable programming, but the parts that seem mysterious are all the mathematical things. I've taken the typical required classes for CS majors like a semester of calculus 1 and 2, statistics, and linear algebra, but I've never actually felt like I've mastered it or anything given how short the classes are and the countless times the logic in a proof was way above my head. Not to mention the lack of coursework given in analyzing data.  




So, my question is when did you feel comfortable in you're abilities to fully understand what's going? Am I being too skeptical? is it really as easy as they make it seem it be? I want to be taken seriously one day as someone who knows what they're doing but don't want to invest time into a 3-4 boot camp for nothing, it seems fishy to me that people are able to learn that much material in such a short time. In the mean time, I've begun studying math again from the beginning starting from algebra, I'm in this for the long haul. I'd appreciate any help, thanks.


tl;dr are all these MOOPs over selling their products?  



",7,3
681,2016-7-23,2016,7,23,15,4u75ta,"How to replicate ""Hey Siri""?",https://www.reddit.com/r/MachineLearning/comments/4u75ta/how_to_replicate_hey_siri/,drhon1337,1469256546,[removed],5,1
682,2016-7-23,2016,7,23,21,4u7z1h,"Using Deep Learning to Optimize the ""Traveling Salesman"" Problem.",https://www.reddit.com/r/MachineLearning/comments/4u7z1h/using_deep_learning_to_optimize_the_traveling/,InaneMembrane,1469275794,,28,44
683,2016-7-23,2016,7,23,21,4u8008,Could anyone list the courses they took for a master degree in AI at stanford ?,https://www.reddit.com/r/MachineLearning/comments/4u8008/could_anyone_list_the_courses_they_took_for_a/,sechelc,1469276402,[removed],0,2
684,2016-7-23,2016,7,23,21,4u80v6,"How do I, as a 14 year old learn machine learning and the mathematics involved?",https://www.reddit.com/r/MachineLearning/comments/4u80v6/how_do_i_as_a_14_year_old_learn_machine_learning/,louislva,1469276909,"I can't go to a university and study it because I currently go to the Danish Grundskole(Directly translated: Groundschool). More specifically I would like to learn how to use Tensorflow(Doesn't have to be Tensorflow, I've just seen some nice things done with it!) and more importantly the mathematics I need to know behind it.

I already know a lot of programming so that won't be a problem.

Any help on where to start would be much appreciated.

Update: Thank you for the great answers, they have been very useful!",157,215
685,2016-7-23,2016,7,23,22,4u8af4,Neural Net Turns NYC Into A Moving Painting (Danil Krivoruchko),https://www.reddit.com/r/MachineLearning/comments/4u8af4/neural_net_turns_nyc_into_a_moving_painting_danil/,[deleted],1469281973,[removed],0,2
686,2016-7-23,2016,7,23,22,4u8b4r,So I have been working on this project for a few years now... What do you guys think?,https://www.reddit.com/r/MachineLearning/comments/4u8b4r/so_i_have_been_working_on_this_project_for_a_few/,jackzhane,1469282322,,5,0
687,2016-7-23,2016,7,23,23,4u8d4t,[1607.02533] Adversarial examples in the physical world,https://www.reddit.com/r/MachineLearning/comments/4u8d4t/160702533_adversarial_examples_in_the_physical/,downtownslim,1469283271,,0,18
688,2016-7-24,2016,7,24,1,4u8ttd,Udacity or Bitbootcamp,https://www.reddit.com/r/MachineLearning/comments/4u8ttd/udacity_or_bitbootcamp/,[deleted],1469290454,[removed],1,0
689,2016-7-24,2016,7,24,1,4u8uxd,[REQUEST] pre-trained model(end to end) for attention based image captioning.,https://www.reddit.com/r/MachineLearning/comments/4u8uxd/request_pretrained_modelend_to_end_for_attention/,cvikasreddy,1469290879,[removed],0,1
690,2016-7-24,2016,7,24,1,4u8z0q,An introduction to Machine learning using Python,https://www.reddit.com/r/MachineLearning/comments/4u8z0q/an_introduction_to_machine_learning_using_python/,abinash111,1469292540,,1,0
691,2016-7-24,2016,7,24,3,4u9c85,Random Classification Noise Defeats All Convex Potential Boosters (2008) [PDF - abstract in comments],https://www.reddit.com/r/MachineLearning/comments/4u9c85/random_classification_noise_defeats_all_convex/,ill-logical,1469297622,,1,10
692,2016-7-24,2016,7,24,4,4u9l09,No BS illustrated guide to understanding ML in less than 3 mins.,https://www.reddit.com/r/MachineLearning/comments/4u9l09/no_bs_illustrated_guide_to_understanding_ml_in/,[deleted],1469300894,[deleted],1,0
693,2016-7-24,2016,7,24,7,4uac4e,(slides) Neural Machine Translation: Breaking the Performance Plateau -,https://www.reddit.com/r/MachineLearning/comments/4uac4e/slides_neural_machine_translation_breaking_the/,WilliamDhalgren,1469311550,,4,17
694,2016-7-24,2016,7,24,8,4uaokc,Book recommendations for Machine Learning,https://www.reddit.com/r/MachineLearning/comments/4uaokc/book_recommendations_for_machine_learning/,GuydeMeka,1469316810,"Hey guys, I'm looking for some books to help me get better at machine learning. Which books helped you the most for learning ML? I'm ideally looking for books that cover different algorithms and use python. I have only beginners knowledge in python.",10,0
695,2016-7-24,2016,7,24,8,4uas54,I have two months before I finish my MS. What can I do to make myself more employable?,https://www.reddit.com/r/MachineLearning/comments/4uas54/i_have_two_months_before_i_finish_my_ms_what_can/,askacadthrowaway,1469318347,"If you're wondering, I've been working on my MS thesis this summer. My original plan was to do research and go to a PhD but now I've decided to delay that for a while. Obviously interning would have been preferable but at this point that's not possible.

My thesis work at this point is mostly just running models with different parameters (which obviously takes a long time). So while those run in the background of my laptop/ server, I have time to be making myself more employable. I want to do ML for a career, and I know it's hard to find a good job with just an MS.

I've considered doing Kaggle, but I've also wondered if doing small projects in various deep learning frameworks (I work in a Neural Nets lab right now) like Caffe, Torch, Theano, and TensorFlow would be a better use of my time. I haven't really done too much Kaggle so I'm not sure how Kaggle works so maybe it's possible to do both?

Another thing I've wanted to do was, since I'm a huge NBA fan, work with analytic stats. If you follow basketball, I've thought about making a ""better"" BPM stat if I can figure out how to scrape the data successfully. 

But I'm wondering if there are any other suggestions. Maybe implement by scratch various ML techniques (like SVM or whatever)?  Or should I find and focus on a large, particular project that interests me?

Any and all advice would be appreciated, especially if someone knows what kind of ML you're expected to know to get a good ML job (at startups, at big corps like Facebook or whatever, or a medium sized company a la Quora)... my grades are fine and definitely not a problem.

Thanks!",5,0
696,2016-7-24,2016,7,24,9,4uazhq,Question about neural network results,https://www.reddit.com/r/MachineLearning/comments/4uazhq/question_about_neural_network_results/,[deleted],1469321728,[deleted],0,1
697,2016-7-24,2016,7,24,10,4ub2kw,Machine Learning - WAYR (What Are You Reading) - Week 4,https://www.reddit.com/r/MachineLearning/comments/4ub2kw/machine_learning_wayr_what_are_you_reading_week_4/,Deinos_Mousike,1469323164,"This is a place to share machine learning research papers, journals, and articles that you're reading this week. If it relates to what you're researching, by all means elaborate and give us your insight, otherwise it could just be an interesting paper you've read.

Preferably you should link the arxiv page (not the PDF, you can easily access the PDF from the summary page but not the other way around) or any other pertinent links.

[Week 1](https://www.reddit.com/r/MachineLearning/comments/4qyjiq/machine_learning_wayr_what_are_you_reading_week_1/)

[Week 2](https://www.reddit.com/r/MachineLearning/comments/4s2xqm/machine_learning_wayr_what_are_you_reading_week_2/)

[Week 3](https://www.reddit.com/r/MachineLearning/comments/4t7mqm/machine_learning_wayr_what_are_you_reading_week_3/)


Here are some of the most upvoted links from last week with the user who found it:

[Combine all the layers of a CNN at image scale (the top layers are upsampled with bilinear interpolation). Train a K*K grid of classifiers and interpolate between them because position is important (head at the bottom is unlikely). At train time, the interpolation is forgotten. Good results on many localization / segmentation tasks. Good ideas but, I am more convinced by the atrous convolutions, based on similar intuitions. - /u/ernesttg](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Hariharan_Hypercolumns_for_Object_2015_CVPR_paper.pdf)

[Unsupervised loss regularizing the network based on variations caused by: data augmentation, dropout, randomized max-pooling. Each training sample is passed n (here n=4 or 5, higher n --&gt; fewer epochs required) times through the network, ""Transformation/Stability"" unsupervised loss. But could lead to a trivial solution, so complemented by the Mutual Exclusivity loss. - /u/ernesttg](http://arxiv.org/abs/1606.04586)

[Signal Processing and Machine Learning with Differential Privacy: Algorithms and Challenges for Continuous Data - /u/Caesarr](http://cseweb.ucsd.edu/~kamalika/pubs/survey.pdf)

[Online Learning paper: A Multiworld Testing Decision Service - /u/flakifero](https://www.microsoft.com/en-us/research/project/multi-world-testing-mwt/)

Besides that, there are no rules, have fun.",15,55
698,2016-7-24,2016,7,24,11,4ub9ef,How are machine learning techniques (i.e. neural networks) applied to anything more complex than number crunching?,https://www.reddit.com/r/MachineLearning/comments/4ub9ef/how_are_machine_learning_techniques_ie_neural/,zDev19,1469326292,"I've read up a lot on neural networks, and I get the concept behind them. I've even written little demos to test them out with simple pattern identification, but I don't understand how the jump is made from that to AI's that design websites, analyze images, etc. I've seen tutorials on how to set up the basics of neural networks, and I've seen amazing examples of AI's doing complicated tasks, but I don't know how to make the stepping stone between those. I was hoping you guys could provide some guidance. ",10,0
699,2016-7-24,2016,7,24,12,4ubgb0,Time Series Data Exploration,https://www.reddit.com/r/MachineLearning/comments/4ubgb0/time_series_data_exploration/,[deleted],1469329561,[deleted],6,9
700,2016-7-24,2016,7,24,13,4uboyf,Which type of ML algorithm would you use to fix image warping?,https://www.reddit.com/r/MachineLearning/comments/4uboyf/which_type_of_ml_algorithm_would_you_use_to_fix/,scottyler89,1469333953,"Consider this a sort of creative challenge on how to apply ML to an interesting astronomy problem. This is I've been kicking around in my head for a while, but don't have a good answer for. In astronomy, the turbulence in the atmosphere causes warping of an image away from ground truth. However, with enough frames, and enough exposure time, you can usually take the image stack as input, and an algorithm will sort through them, selecting the best images (least affected by the turbulence), and combine them to make a good approximation of ground truth. There is a lot of data that just gets tossed out because some areas are warped by turbulence. Can you think of a ML algorithm that could take in an image stack (and perhaps the other algorithms' best guess at the ground truth), and predict unwarped versions of each layer in the stack of images? Simulating datasets would be relatively easy, so I don't think training data would be difficult. Any thoughts?",21,2
701,2016-7-24,2016,7,24,13,4ubtf3,Pain-free Ubuntu Cuda Nvidia installation,https://www.reddit.com/r/MachineLearning/comments/4ubtf3/painfree_ubuntu_cuda_nvidia_installation/,Jxieeducation,1469336362,,3,2
702,2016-7-24,2016,7,24,19,4ucld2,Top journals and their differences,https://www.reddit.com/r/MachineLearning/comments/4ucld2/top_journals_and_their_differences/,Pieranha,1469354747,"I see many posts regarding ML conferences such as NIPS, ICLR, ICML etc., but not a lot of information regarding journals focusing on ML. Are there no ML journals that are considered as attractive/prestigious as these conferences?

Suppose that I aim to submit a paper to a journal. Which journals are considered the best within ML and how do they differ among each other?",14,10
703,2016-7-24,2016,7,24,20,4ucq31,"[Discussion] hello guys,my father is A car mechanic and I want to help him. So please give me some hint How can I connect a car to my laptop to know which part of a car is not simple or have some problem. Thank you!",https://www.reddit.com/r/MachineLearning/comments/4ucq31/discussion_hello_guysmy_father_is_a_car_mechanic/,madmax001,1469358307,,7,0
704,2016-7-24,2016,7,24,20,4ucqnw,Implemented Free Image Recognition API 8bit.ai. What do you think guys ?,https://www.reddit.com/r/MachineLearning/comments/4ucqnw/implemented_free_image_recognition_api_8bitai/,erogol,1469358781,"I implemented FREE Image Recognition API on many different domains (Car, Nudity, Food, Scene and vast amount of Concepts). These models are not variants of open-source solutions on contrary to the many other alternatives. I tried really hard for the best possible model in its domain. 

You are able to use on https://market.mashape.com/8bit/image-classification or directly signing to 8bit.ai

For the next stage I target face detection, analysis and recognition field. 

Feel free to contact me for anything... I hope you like it and use for any of your problem.
",7,22
705,2016-7-24,2016,7,24,22,4ud53p,Google's DeepMind AI Reduces Data Center Cooling Bill By 40%,https://www.reddit.com/r/MachineLearning/comments/4ud53p/googles_deepmind_ai_reduces_data_center_cooling/,Martin81,1469367535,,25,298
706,2016-7-24,2016,7,24,22,4ud6t0,"DCGAN Upsampling Arithmetic Question, and a potential error?",https://www.reddit.com/r/MachineLearning/comments/4ud6t0/dcgan_upsampling_arithmetic_question_and_a/,cuda_curious,1469368384,"In Radford's [DCGAN code](https://github.com/Newmu/dcgan_code/blob/master/faces/train_uncond_dcgan.py) they upsample in the generator with a set of 5x5 filters with stride 2 and ""border mode"" of (2,2), using fractionally strided convolutions. As per the convolution arithmetic outlined [here](http://arxiv.org/abs/1603.07285), however, if ""border mode"" corresponds to zero padding as in section 4.6, these dimensions don't line up! If you take a 4x4 input and upsample using a transposed convolution with filter size 5, zero padding 2, and stride 2, then the result should be 

output = stride*(input - 1) +filter_size - 2 * padding

Which makes 2*(4-1) + 5 - 2*2 = 7, not 8 as shown in the paper (and as results from applying the deconv op defined in that code). Taking a look at the [op that defines the deconv](https://github.com/Newmu/dcgan_code/blob/master/lib/ops.py#L85), it looks like they are allocating memory of the appropriately upsampled size and then applying a convolution that somehow works out to fit the dimensions correctly. Changing the border mode parameter in the source code doesn't affect the output size at all (presumably because the alloc call is only dependent on the subsampling params).

I've been having trouble parsing all of the GpuDNNConvDesc stuff in the cuDNN section of Theano, so I can't quite figure it out--what exactly is going on that makes these dimensions line up? Am I misunderstanding ""border mode""? Are they padding the output with an additional set of zeros (it doesn't look like it from the results), are they not actually using a 5x5  filter (a 4x4 filter with a zero padding of 1 makes these dims line up ), are they...what? Any assistance would be appreciated.",2,4
707,2016-7-24,2016,7,24,23,4ud8qq,Google Will Use Machine Learning To Bring Comic Books To Life - ARC,https://www.reddit.com/r/MachineLearning/comments/4ud8qq/google_will_use_machine_learning_to_bring_comic/,mountefren,1469369298,,0,2
708,2016-7-25,2016,7,25,0,4udmy4,Assistance with working out a bug in SVM implementation in R?,https://www.reddit.com/r/MachineLearning/comments/4udmy4/assistance_with_working_out_a_bug_in_svm/,hlyates,1469375390,,0,0
709,2016-7-25,2016,7,25,0,4udoc7,"This Week in ML &amp; AI Podcast - 7/22/16: ML to Optimize Google Datacenters, ""Crazy"" New GPU from NVIDIA, Faster RNNs",https://www.reddit.com/r/MachineLearning/comments/4udoc7/this_week_in_ml_ai_podcast_72216_ml_to_optimize/,sbc1906,1469375956,"Another fun podcast episode...

The show page is over at: https://twimlai.com/10

Episode highlights:

* Google uses Machine Learning to cut datacenter cooling by 40%
* New NVIDIA TITAN X GPU
* Layer Normalization technique by Hinton lab at U. Toronto speeds RNN training
* Google Cloud Platform Natural Language and Speech APIs in public beta
* Generating human-sounding voice with neural nets
* A deep learning solution to the Traveling Salesman Problem

and more.",0,1
710,2016-7-25,2016,7,25,1,4udow3,Question: Resizing training images,https://www.reddit.com/r/MachineLearning/comments/4udow3/question_resizing_training_images/,toto187,1469376179,"Hi there,

I am attempting a very basic binary classification problem to predict if an image contains a human or not. I have a set of 400k images to train using Keras/Theano. I am achieving acceptable training accuracy however my validation accuracy is erratic, see image below.

https://s32.postimg.org/f5b885hid/first_try.png

I have a feeling this is caused by my image pre-processing. Most of my positive training images were images of people that were not square images and were resized to 32x32, thus distorting them significantly (vertically squishing them). My validation images were only scaled down to 32x32 but they were square images originally.

Does anyone have any suggestions of how to overcome this? My first thought was to resize the training images by stretching the edge pixels out on the sides to create square images; this would preserve the actual shape of the human in the image. However, wouldn't my convnet just learn to take that new information into account, and again over-fit the training data? 

I am using the cifar10 example (hence the 32x32 images) in Keras as my template model, and only using mean subtraction on each image as pre-processing - as well as the resizing. 

Thanks a lot for any help!",14,5
711,2016-7-25,2016,7,25,2,4ue3pa,Dealing with Exploding Gradients,https://www.reddit.com/r/MachineLearning/comments/4ue3pa/dealing_with_exploding_gradients/,[deleted],1469381780,[deleted],0,1
712,2016-7-25,2016,7,25,3,4uedve,Andrew Ng: What Is Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/4uedve/andrew_ng_what_is_machine_learning/,dynamicstm,1469385573,,0,0
713,2016-7-25,2016,7,25,3,4ueft4,Accelerating Eulerian Fluid Simulation With Convolutional Networks,https://www.reddit.com/r/MachineLearning/comments/4ueft4/accelerating_eulerian_fluid_simulation_with/,TheInvisibleHand89,1469386296,,13,31
714,2016-7-25,2016,7,25,3,4ueg3u,ELI5 Data augmentation for classification algorithm,https://www.reddit.com/r/MachineLearning/comments/4ueg3u/eli5_data_augmentation_for_classification/,tia_0,1469386412,"I'm trying to understant the benefit apported by the step of data augmentation in a classification algorithm. I have a vector of hexadecimal strings and a column vector containing the label associated with the string in the same position. As an optional step in the classification algorithm a data augmentation process is performed by subsetting the strings in pieces and replating the associated label for the number of split performed.

What are the benefit of this process?
What is the theory behind it and where i can look that up?",1,0
715,2016-7-25,2016,7,25,4,4ueqbr,How to use a RNN/LSTM on a time sequence without the sliding time window approach?,https://www.reddit.com/r/MachineLearning/comments/4ueqbr/how_to_use_a_rnnlstm_on_a_time_sequence_without/,wederer42,1469390193,"So on [this post](http://stats.stackexchange.com/questions/8000/proper-way-of-using-recurrent-neural-network-for-time-series-analysis) on stackexchange somebody asked how a RNN differs from other NN when analysing time series. However I do not quite understand how one would use a RNN without a sliding time window.

Lets assume you have power data of a whole house and want to disaggregate one appliance. I would be approaching this by sliding a time window (e.g. 512 timesteps) and training my Neural Network to output the power data at the last timestep for one appliance. So 512 IN, 1 OUT.

Could a RNN learn by using only 1 input of aggregate data and 1 output of the target appliance data? I do not see how this is possible.

Please tell me if my question is unclear. Thanks!

edit: if you downvote my question, please comment why it is a bad question.",5,2
716,2016-7-25,2016,7,25,5,4ueroh,Learning in Brains and Machines (4): Episodic and Interactive Memory | The Spectator,https://www.reddit.com/r/MachineLearning/comments/4ueroh/learning_in_brains_and_machines_4_episodic_and/,[deleted],1469390675,[deleted],0,17
717,2016-7-25,2016,7,25,5,4uet83,Is there any noticable improvement possible in the function of mouse position with parameter history of physical mouse movement?,https://www.reddit.com/r/MachineLearning/comments/4uet83/is_there_any_noticable_improvement_possible_in/,BenRayfield,1469391233,"When you look at cursor on screen and move mouse with your hand, cursor does not move proportional to physical movement. 

Depending on the last half second of how fast and what direction you've moved mouse, cursor moves in the most recent direction but different speed.

Then you slow the mouse and stop somewhere.

The result is the cursor gets where you wanted it faster. It puts the precision where its most useful to you.

Or does it? Its closer than a linear function, but could it be improved by AI? And could it be done efficiently enough that the milliseconds delay added by computing AI pay for themself in getting the mouse where you wanted it slightly earlier? Someone might build a continuous mouse speed adjuster, for example using java.awt.Robot as I've experimented with (and it has low enough lag, sometimes costing too much cpu), that makes your mouse more a pleasure to use. How much improvement could be made by learning by example of mouse movements where you wanted the mouse to stop, and getting it there faster?",2,4
718,2016-7-25,2016,7,25,5,4ueuk7,"Dracula.js: a deep, hierarchical LSTM model, running in the browser",https://www.reddit.com/r/MachineLearning/comments/4ueuk7/draculajs_a_deep_hierarchical_lstm_model_running/,acornalert,1469391715,,0,8
719,2016-7-25,2016,7,25,5,4uevxe,Generate Rap Lyrics - Fresh Machine Learning #4,https://www.reddit.com/r/MachineLearning/comments/4uevxe/generate_rap_lyrics_fresh_machine_learning_4/,llSourcell,1469392208,,3,1
720,2016-7-25,2016,7,25,6,4uf7k0,Learn to build recurrent neural networks in tensorflow dynamically from raw equations.,https://www.reddit.com/r/MachineLearning/comments/4uf7k0/learn_to_build_recurrent_neural_networks_in/,[deleted],1469396440,[deleted],0,0
721,2016-7-25,2016,7,25,6,4uf7qy,Some Classes Sometimes Having Zero Probability Across A Testing Set,https://www.reddit.com/r/MachineLearning/comments/4uf7qy/some_classes_sometimes_having_zero_probability/,danielcanadia,1469396517,"Every once in a while I come across this problem. 

I train a model that maps a probability of input x belonging to let's say 30 potential classes (y-labels). Sometimes, one or two classes come out with essentially zero average probability in the testing set. And it's not consistent. If I retrain the model, that problem typically goes away (or other classes may become ""omitted""). I feel like this is a common machine learning problem. 

How/Where can I read more about this problem?",3,0
722,2016-7-25,2016,7,25,8,4ufmhf,Any other tutorials like this?,https://www.reddit.com/r/MachineLearning/comments/4ufmhf/any_other_tutorials_like_this/,MEOWmix_SWAG,1469402317,"Hi, I found this amazing Machine Learning tutorial on medium.com and was wondering if anyone had a link to similar ones.

https://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78#.89cvux27p

Thank you.",0,0
723,2016-7-25,2016,7,25,8,4ufmxy,"Layer Normalization Implemented In TensorFlow -- LSTM, GRU, Recurrent Highway Networks",https://www.reddit.com/r/MachineLearning/comments/4ufmxy/layer_normalization_implemented_in_tensorflow/,LeavesBreathe,1469402505,,14,30
724,2016-7-25,2016,7,25,8,4ufqo9,[1607.05690] Stochastic Backpropagation through Mixture Density Distributions,https://www.reddit.com/r/MachineLearning/comments/4ufqo9/160705690_stochastic_backpropagation_through/,m000pan,1469404101,,0,42
725,2016-7-25,2016,7,25,10,4ug6e5,Weight initialization &amp; Residual connections,https://www.reddit.com/r/MachineLearning/comments/4ug6e5/weight_initialization_residual_connections/,enematurret,1469410762,"So, I was playing around with different weight initialization techniques (mostly Gaussian x Glorot x He) on MLPs and realized that the latter two lose pretty much all their theoretical foundation on residual networks, right?

He Init is based upon keeping the variance 1 on forward passes for ReLU networks where u = ReLU(Wx), so var[u] = n var[w] * var[x] / 2 = 1 (the divison by 2 is due to ReLU and by symmetry).

Since var[x] = 1, we have var[w] = 2/n, as in the paper.

However, if we have u = ReLU(Wx) + x and follow the same intuition, we get var[w] = 0.
So my questions are:

1) is there any initialization technique that is better adapted to ResNets?
2) does that possibly explain why plain nets with depth up to 30 can be trained with He Init and without BN, but ResNets can't?",4,5
726,2016-7-25,2016,7,25,11,4ugbq4,Start here to learn R,https://www.reddit.com/r/MachineLearning/comments/4ugbq4/start_here_to_learn_r/,iamkeyur,1469413071,,1,0
727,2016-7-25,2016,7,25,11,4ugd9k,"Machine learning - I have training examples, that are 10000 png image about triangle and How can i do processing 10000 png image before ML?",https://www.reddit.com/r/MachineLearning/comments/4ugd9k/machine_learning_i_have_training_examples_that/,theiron97,1469413767,"Like the title, I think i should convert 10000 image to binary. Concretly, I convert 10000 image 20x20 to the example, that has 400 features. That serve machine learning problem. But i dont know how convert. 
Thanks for reading my question.
",5,0
728,2016-7-25,2016,7,25,12,4ugkbw,"Machine learning - I have training examples, that are 10000 png image about triangle and How can i do processing 10000 png image before ML?",https://www.reddit.com/r/MachineLearning/comments/4ugkbw/machine_learning_i_have_training_examples_that/,theiron97,1469416952,,0,0
729,2016-7-25,2016,7,25,16,4uhb7m,Question concerning MatConvNet,https://www.reddit.com/r/MachineLearning/comments/4uhb7m/question_concerning_matconvnet/,kaifung,1469430279,"I am new to machine learning and I am trying to learn MatConvNet and build my own convolutional neural network. However, I am some problem when reading the cnn_cifar.m sample code.
  In the function cnn_cifar, i see the code ""[opts, varargin] = vl_argparse(opts, varargin) ;"" appears when the 'opts' object is updated. And ""opts = vl_argparse(opts, varargin) ;"" appears at the very end of the function. I try to read the function description in the MatConvNet official website but I get no clue what is it saying.
  Could anyone help me out? I really appreciate this.
",1,0
730,2016-7-25,2016,7,25,16,4uhf7g,"As a skill, how well does ML transfer over to Deep Learning",https://www.reddit.com/r/MachineLearning/comments/4uhf7g/as_a_skill_how_well_does_ml_transfer_over_to_deep/,jathweatt,1469432474,"I was exploring Udacity's Machine Learning nanodegree and am now considering taking the course. However, I see another nanodegree for Deep Learning that also piques my interest. I would like to study Deep Learning down the road, but I'm wondering if the skills developed in Machine Learning will carry over to Deep Learning. How different are the two, and how long would it take to learn Deep Learning after I've gained experience in Machine Learning?",13,0
731,2016-7-25,2016,7,25,17,4uhhx7,AI behind thegrid.io ?,https://www.reddit.com/r/MachineLearning/comments/4uhhx7/ai_behind_thegridio/,minnuu,1469433969,I have been keenly following the core science behind thegrid (www.thegrid.io) and to my surprise have not been able to come across research in this field. Could any one guide me to the resources ?,5,1
732,2016-7-25,2016,7,25,17,4uhijg,Can more data lower SVM performance?,https://www.reddit.com/r/MachineLearning/comments/4uhijg/can_more_data_lower_svm_performance/,[deleted],1469434323,[deleted],9,0
733,2016-7-25,2016,7,25,19,4uhxsj,[Q] Textbook suggestions on Machine learning in detection theory for graphs?,https://www.reddit.com/r/MachineLearning/comments/4uhxsj/q_textbook_suggestions_on_machine_learning_in/,meechosch,1469442850,[removed],1,4
734,2016-7-25,2016,7,25,19,4uhyu7,Telephone Hotline Data Forecasts Dengue Outbreaks Three Weeks Ahead,https://www.reddit.com/r/MachineLearning/comments/4uhyu7/telephone_hotline_data_forecasts_dengue_outbreaks/,nerdOnline,1469443384,,4,26
735,2016-7-25,2016,7,25,19,4uhyy0,Self-driving cars mimics human driver style using deep learning,https://www.reddit.com/r/MachineLearning/comments/4uhyy0/selfdriving_cars_mimics_human_driver_style_using/,kdsal,1469443439,[removed],0,1
736,2016-7-25,2016,7,25,21,4uia98,Worm / Screw high precision automatic thread rolling cutting machine,https://www.reddit.com/r/MachineLearning/comments/4uia98/worm_screw_high_precision_automatic_thread/,csdworshop,1469448927,,1,1
737,2016-7-25,2016,7,25,21,4uid21,AI to play 500,https://www.reddit.com/r/MachineLearning/comments/4uid21/ai_to_play_500/,Ozzah,1469450068,"I was thinking of putting together an ANN to play [500](https://en.wikipedia.org/wiki/500_%28card_game%29).

I have a good understanding of ANNs, including the mathematics behind them.

I'm not entirely sure how to go about designing the network, as there are actually several decision processes:

1. A bid needs to be made at the start of the hand, which is based on (i) the cards in the hand, (ii) the number of cards in the hand, (iii) the number of players, (iv) the bids so far, and (v) if the AI is the dealer, then whether they choose to exchange a card. If the dealer decides to exchange the card, then a decision needs to be made about which card to exchange.
2. Each round, the AI needs to decide which card to place down based on what cards have been played so far. The number of cards down and the number of cards to go will always be different, as each time the previous round's winner leads.

Any suggestions? Would I have one big ANN or lots of smaller ones with procedural code in between to maintain game logic? Would I need to train different ANNs for different number of players and different card counts, or is there a clever way to handle this in one go?

Once the structure is down, I was thinking of doing the AlphaGo strategy of pitting several AIs against one another so that they learn how to play the game by themselves.",1,0
738,2016-7-25,2016,7,25,21,4uie3k,PFAUTER SF1 WORM MLLNG MACHNE,https://www.reddit.com/r/MachineLearning/comments/4uie3k/pfauter_sf1_worm_milling_machine/,csdworshop,1469450492,,1,1
739,2016-7-25,2016,7,25,22,4uimq1,Human Activity Recognition - Weka,https://www.reddit.com/r/MachineLearning/comments/4uimq1/human_activity_recognition_weka/,[deleted],1469453877,[deleted],4,0
740,2016-7-25,2016,7,25,23,4uitho,[1607.06534] The Landscape of Empirical Risk for Non-convex Losses,https://www.reddit.com/r/MachineLearning/comments/4uitho/160706534_the_landscape_of_empirical_risk_for/,bdamos,1469456369,,0,16
741,2016-7-25,2016,7,25,23,4uiz2f,Tensorflow running on Windows through Bash shell (CPU only),https://www.reddit.com/r/MachineLearning/comments/4uiz2f/tensorflow_running_on_windows_through_bash_shell/,th3owner,1469458281,,6,8
742,2016-7-26,2016,7,26,0,4uj0xx,AMA Request: Daphne Koller,https://www.reddit.com/r/MachineLearning/comments/4uj0xx/ama_request_daphne_koller/,mlkrime,1469458906,,4,57
743,2016-7-26,2016,7,26,0,4uj1ct,[1607.04423] Attention-over-Attention Neural Networks for Reading Comprehension,https://www.reddit.com/r/MachineLearning/comments/4uj1ct/160704423_attentionoverattention_neural_networks/,SuperFX,1469459038,,4,26
744,2016-7-26,2016,7,26,0,4uj4t4,Deep Learning with Neural Networks and TensorFlow Tutorials,https://www.reddit.com/r/MachineLearning/comments/4uj4t4/deep_learning_with_neural_networks_and_tensorflow/,sentdex,1469460213,"A couple months ago, I shared the [machine learning course](https://www.reddit.com/r/MachineLearning/comments/4j0u2z/indepth_machine_learning_course_w_python/?st=ir26bp56&amp;sh=86c566b0) I was working on. 

The main structure for the course is to:

* Do a quick overview of the theory of each machine learning algorithm we cover.

* Show an application of that algorithm using a module, like scikit-learn, along with some real world data.

* Break down the algorithm and re-write it ourselves, without machine learning modules, in Python.

We've covered regression, K Nearest Neighbors, Support Vector Machines, flat clustering, hierarchical clustering, and we've just begun getting into neural networks, which is what most people expressed main interest with.

So far, we've covered the theory of a basic feed-forward + back prop neural network, and we've coded a simple Deep Neural Network in Python with TensorFlow.

If you're looking for a stupid simple example of making a neural network in TensorFlow, or you're interested in continuing to learn more, check out the [Deep Learning with Neural Networks and Tensorflow Tutorials](https://pythonprogramming.net/neural-networks-machine-learning-tutorial/). If you already understand the concepts of a neural network, feel free to skip the first tutorial, and you can skip the 2nd if you've already got TensorFlow installed.

We have a LOT of territory to cover when it comes to neural networks and TensorFlow, so expect a lot more to come. 

All tutorials come in both video and text form, and are free. 

As always: If you have questions, comments, concerns or suggestions, leave them below! ",17,224
745,2016-7-26,2016,7,26,1,4ujjaa,Undergraduate thesis advice,https://www.reddit.com/r/MachineLearning/comments/4ujjaa/undergraduate_thesis_advice/,IWishIwasGoodAtMath,1469464830,"Hi guys, I am not sure if this is the correct place to make this post so let me know if this is not appropriate. I am lucky enough to be in a position to try to convince a professor to be my advisor for an undergraduate thesis in statistical approaches to NLP. I am really interested in this field in general  in particular machine translation, topic analysis, and extracting knowledge from text. 

My undergraduate background is in math and computer science and I hope to pursue some kind of machine learning in graduate school. I have taken a related class on statistical approaches to machine learning and graphical models. My overall GPA is not the best, but I have 2 years of research and TA experience (3 papers, no publications yet lol).

I wanted to ask of you guys could give me some advice on these topics  do they seem reasonable for an undergraduate to make progress in/produce something publishable in 2 semesters? I am having trouble narrowing down my idea... and I want to propose a topic very soon so that I can use the rest of my summer to do focused studying on the topic. I think that projects which extend current implementations/algorithms or apply algorithms to new domains are probably the most feasible. Unfortunately, the professor mentioned above is not really into neural machine translation, so I am trying to focus more on statistical-based approaches. I hope you guys will criticize these topics, provide some papers for inspiration, or provide more ideas. Thanks!

**Applications (maybe the most straightforward)**

Proposal of SMT Framework for Twitter/microblog platforms

- Doing SMT on a restricted domain is already well studied, but there seems to be work lacking in the particular domain of tweets &amp; microblogs. This would be an interesting topic. There exist many interesting aspects of tweets &amp; microblogs which are not component to canonical spoken and written languages (i.e. post-length restrictions, sentence and grammatical structure, word spelling &amp; slang usage, etc.). 

SMT Framework for the translation of medical reports

- Similar to the above. The focus would be on spoken interactions between doctors and patients (this might be something already well studied)

Plagiarism detection

- Inspired by the fiasco at the republican convention.

**More theoretical (maybe the most fun)**

Unsupervised tokenization

- There isnt a ton of prior research on this topic. Unsupervised tokenization is a hard problem for some specific languages (e.g. Asian character-based languages that dont utilize spaces and Hungarian)

Unsupervised detection of abnormal (in the context of surrounding topics) text events (e.g. troll classification in forums/chat services)

Unsupervised WSI/Soft Word Clustering

Character-based Translation Error Rate as a metric for evaluation of morphologically complex languages

- Soft vs hard similarity scores for candidate/reference documents

**OSS Extensions (some cool project ideas for inspiration on the Moses website)**

Word Clustering

-Related to the above. The Giza++ word alignment system currently does preprocessing on monolingual corpora by applying the mkcls program to cluster words into classes based on their context. One improvement might be to alter Giza++ + mkcls to do clustering/soft clustering on multi-word sequences the reasoning stems from alignment issues on languages (such as Chinese, Hindi) where a single word on average has many more senses than other languages.

Survey: Improving the Performance of Giza++ By Heuristically smoothing EM during IBM model 1 training process

- See Improving IBM Word-Alignment Model 1 (Moore)

Thanks again for any help you can give. Let me know if I should provide more information too.",2,2
746,2016-7-26,2016,7,26,1,4ujkv9,Building a 1D Generative Adversarial Network in TensorFlow,https://www.reddit.com/r/MachineLearning/comments/4ujkv9/building_a_1d_generative_adversarial_network_in/,MikeWally,1469465328,,9,7
747,2016-7-26,2016,7,26,2,4ujqu1,Adversarial networks and overfitting/regularization,https://www.reddit.com/r/MachineLearning/comments/4ujqu1/adversarial_networks_and_overfittingregularization/,NichG,1469467140,"I've been using adversarial network architectures ([this kind](http://arxiv.org/abs/1511.05897), not generative ones) to deal with data sets in which there's a small number of distinct subjects, but a large number of data per subject, and the goal is to generalize to new subjects. The idea is to subtract out the subject-specific part of the representation at one of the hidden layers by making it impossible for the adversary to tell which subject the hidden layer activations are from, in order to protect the network against learning subject-specific profiles. This seems to work reasonably well.

In designing the networks for this though, I was wondering - what is the meaning of applying regularization such as dropout to the adversary? Does it matter? 

I'm beginning to think that actually its okay if the adversary overfits, because whatever the adversary discovers is something that the base network will try to remove from the data. If the adversary fits against a very specific detail, its easy for the base network to change that detail with little difference to the rest of the base network's loss function. So when it comes to overfitting, the adversary is chasing a moving target - it seems like the network pair ends up being somehow self-regularizing. I suppose if it were really bad, you might get unstable oscillations in the adversary which would prevent learning.

If you don't go unstable though, it seems that there might be an additional interesting feature - the base network might learn to remove things from its internal representations which are highly unique to particular samples, because those are the kinds of things which would be most prone to being memorized (and correspondingly, would be discovered by the adversary as well).  This means in some sense that it might be possible to treat a data set to remove things like outliers which encourage learning algorithms to overfit. I could see using a sort of overfit-proofing autoencoder, where one network is trying to just output its input, but the adversarial network is trying to learn to map samples to their index or something like that.

So, does anyone have any experiences in applying regularization to the adversarial component of a (non-generative) adversarial network? What difference did it make, if any?",0,3
748,2016-7-26,2016,7,26,2,4ujqv4,Tensorflow 3 Ways,https://www.reddit.com/r/MachineLearning/comments/4ujqv4/tensorflow_3_ways/,amplifier_khan,1469467148,,0,0
749,2016-7-26,2016,7,26,2,4ujr2s,what loss to use for variational auto encoder with real numbers?,https://www.reddit.com/r/MachineLearning/comments/4ujr2s/what_loss_to_use_for_variational_auto_encoder/,kvfrans,1469467221,"Let's say I'm trying to use a variational auto-encoder to encode some vector of real numbers. Normally, the generative loss p(x|z) is cross-entropy loss between the real and generated results, but this doesn't work if the numbers are negative.

Some options that might work are using L2 loss instead, adding a fixed amount to each vector so the numbers are positive, or applying sigmoid to each vector.

Anyone have ideas/experience on what to do?",3,4
750,2016-7-26,2016,7,26,2,4uju5v,ShiftSpace : GPU VMs Cheaper and Faster Than AWS,https://www.reddit.com/r/MachineLearning/comments/4uju5v/shiftspace_gpu_vms_cheaper_and_faster_than_aws/,cross_da_boss,1469468144,,7,13
751,2016-7-26,2016,7,26,2,4ujv25,Is deep learning just a neural network?,https://www.reddit.com/r/MachineLearning/comments/4ujv25/is_deep_learning_just_a_neural_network/,TheFunkyPeanut,1469468386,[removed],6,0
752,2016-7-26,2016,7,26,3,4uk1tk,Looking for best way to analyze flight data.,https://www.reddit.com/r/MachineLearning/comments/4uk1tk/looking_for_best_way_to_analyze_flight_data/,[deleted],1469470356,[deleted],1,1
753,2016-7-26,2016,7,26,4,4ukby8,Random Bits Forest: a Strong Classifier/Regressor for Big Data,https://www.reddit.com/r/MachineLearning/comments/4ukby8/random_bits_forest_a_strong_classifierregressor/,godspeed_china,1469473683,"http://www.nature.com/articles/srep30086
This is my first machine learning paper got published. It is not the state of the art and have certain weakness. ",3,1
754,2016-7-26,2016,7,26,4,4ukj1l,Methods to do vocabulary expansion for pretrained word embeddings?,https://www.reddit.com/r/MachineLearning/comments/4ukj1l/methods_to_do_vocabulary_expansion_for_pretrained/,[deleted],1469476001,Suppose I have pretrained word embeddings( glove or word2vec) and an out of vocabulary word along with its context. Is there any way to produce a word embedding for this new word without having to retrain the entire model? Any papers and ideas are welcome. ,6,0
755,2016-7-26,2016,7,26,5,4ukmps,Caffe Framework,https://www.reddit.com/r/MachineLearning/comments/4ukmps/caffe_framework/,ale86ch,1469477211,"Hello,
I'm quite new to reddit so sorry if I'm not using it properly. Anyway I have been looking for a while here and on internet generally about info on Caffe Framework. I have to implement a simple face detection approach using a convolutional neural network trained on publicly-available datasets.
I decided to try with Caffe, I never used it and I have no experience in machine learning.
I would like to know if anyone know a tutorial to start with, I am also looking bindings for Java, I found only jCaffe, https://github.com/fastturtle/jCaffe
anyone have some experience with that? Every information/help is very appreciated. Thank you very much",4,0
756,2016-7-26,2016,7,26,6,4ukypw,"Weight Init, Batch Normalization and Dropout",https://www.reddit.com/r/MachineLearning/comments/4ukypw/weight_init_batch_normalization_and_dropout/,enematurret,1469481217,"I've been playing a lot with different weight inits and also trying to mix dropout and BN to see how they fit together. I've been getting really annoying results that seem to point out that there's no best way to deal with these 3:

1) When Dropout is present, BN works better after the non-linearity instead of before. However in this case the weight initialization has huge impacts on the final performance of the network (up to 2% difference from Glorot to He init).

2) When Dropout is absent, BN works better before the non-linearity, but the training becomes extremely noisy and the final result is usually worse.

So, the best results I've managed to get are with Dropout + BN (before activation), however for some cases Glorot init is way better than He's, sometimes the other way around, and other times even traditional Gaussian initialization beats the previous two.

Anyone has some input on how to deal with that? I've been trying to come up with my first paper and it's been hell on earth to chose which setup to run my experiments on.",12,10
757,2016-7-26,2016,7,26,6,4ul39k,Wave Computing Announces Plans for a Family of Deep Learning Computers,https://www.reddit.com/r/MachineLearning/comments/4ul39k/wave_computing_announces_plans_for_a_family_of/,g2p2u,1469482798,,2,0
758,2016-7-26,2016,7,26,8,4uljb9,Tensorflow Neural Conversation Model,https://www.reddit.com/r/MachineLearning/comments/4uljb9/tensorflow_neural_conversation_model/,parry243,1469488495,,0,0
759,2016-7-26,2016,7,26,8,4ulnzt,Question about what type of NN I should use,https://www.reddit.com/r/MachineLearning/comments/4ulnzt/question_about_what_type_of_nn_i_should_use/,DayMan116,1469490302,"Have input data that is rows of 1's and 0's such as [1, 0, 0, 0, 0, 1]. This represents 3 games, where the first 1 is player A's action and the first 0 is player B's action, then the 3rd element is the A's action in the second game and the 4th is the B's action in the second game, and of course the 5th element is A's action in the 3rd game and the 6th is B's action in the 3rd game. Out put is the sum of payoffs from games 1-3 and there is different payoffs for each possible combo (0,0), (0,1), (1,0) and (1,1). So for example the row above would be 0 + 1 + 3 = 4. (payoffs for each game are limited to integers between 0, and 3). I would like the NN to be able to predict a row of all 0's or all 1's without having seen them before in the training set but my current implementation (feed forward with 1 hidden layer doesn't seem to do very well at this). Any help would be greatly appreciated.",3,0
760,2016-7-26,2016,7,26,8,4ulphj,Analyzing Robocup 2D logs,https://www.reddit.com/r/MachineLearning/comments/4ulphj/analyzing_robocup_2d_logs/,jemd13,1469490838,"Reposting here from /r/RoboCup/ because that sub doesn't seem very active :x. I hope someone here can help me.

Hello everyone. I have no idea if this sub is active, or if this is the place to ask this, but its pretty hard to find information about this online, so I figured it woudn't hurt to post here.
I'm working on my undergraduate Thesis project with my friend. We're working on making our own coach for a little team of Robocup 2D agents. Now, we want to use the .rcg logs that the system outputs when a game finishes, and at the moment we're struggling to find the meaning of certain lines in the files.
After all the player_params and server_params and all that, when the game actually starts, each line shows the following : (show X ((b) 0 0 0 0) ((l 1) ...)  ((l 11)  ) ((r 1)  )  ((r 11)  ) ) where X is the cycle number, the (b) part is the ball info, and then after that, it shows info about each of the players from each team.
Our current problem is that we don't know what the 0s are in the ball part and what the info for the players mean, I'll post the info shown by the log about one player here to see if anyone can help me finding out what each of these numbers and things mean :
((l 1) 0 0x9 -49.4581 0.0212 -0 -0 90.135 -90 (v h 180) (s 8000 1 1 130555) (f l 3) (c 0 1 1016 0 1 1018 1 2 0 0 24))
If anyone can tell me what those numbers and letters represent after the (l 1), or help me find a guide or manual that can help me, I'd really apreciate it. (I already tried the Robocup 2D server manual, but nothing there).",0,0
761,2016-7-26,2016,7,26,9,4uluj7,Why GPUs are necessary,https://www.reddit.com/r/MachineLearning/comments/4uluj7/why_gpus_are_necessary/,[deleted],1469492741,[deleted],7,0
762,2016-7-26,2016,7,26,9,4ulv4m,Importing datasets into a 3D data world will immerse users in computer generated worlds that will improve discoverability of the data ultimately leading to improved data insights,https://www.reddit.com/r/MachineLearning/comments/4ulv4m/importing_datasets_into_a_3d_data_world_will/,bradstimpson,1469492945,,0,0
763,2016-7-26,2016,7,26,9,4ulwoo,Unsupervised deep learning: how should I proceed?,https://www.reddit.com/r/MachineLearning/comments/4ulwoo/unsupervised_deep_learning_how_should_i_proceed/,to_the_sun,1469493575,"I've been dabbling with machine learning lately and I've got an application for it that I'd really like to see come to fruition, but I need some advice on how practical it is, as well as how to proceed i.e. what frameworks/libraries to begin with.

I would like to apply unsupervised deep learning to audio analysis. Currently I have a program that analyzes the audio from your instrument as you jam, records the ""best"" portions while rejecting the rest, looping around the length of a song that builds up around you as you play. The results ideally are fully written and recorded songs ready for you at the end of every jam session. As it is the program runs on MIDI and only takes rhythm into account, but I can envision a state in which it can take raw audio and analyze it for patterns of all kinds.

It would work by first accumulating many small windows of spectral data (maybe 250 or 500 ms apiece) which would generate filters for the first layer of the neural net. These patterns would represent where in the (say) quarter-note-length window a beat is likely to fall, and perhaps also what spectral shapes are prevelant (which could help weed out out-of-key or otherwise bothched notes). Moving up the heirarchy of the neural net, the windows would get larger and represent gradually more complex patterns, say, riffs that are used repeatedly and eventually at the top, whole choruses and verses.

In real time, a linear regression likelihood would need to be generated, rating what you play for how ""expected"" it is based on what you've been playing already and this is the score that would be used to determine the ""best"" portions to record. The data would also need to be gathered as you play, but the model would only have to be trained as often as could be, in the background. So at first the model would only be trained on very small amounts of data, but by the end of a say, 30 minute jam session the training data would comprise some 3600 500-ms windows. 

So my first question pertains to practicality: **On an ordinary laptop, how long would one expect training to take as the data set grows progressively throughout the jam session? Seconds at first, but how quickly would retraining be taking minutes or longer, impractical durations?** If spectral analysis is done on these windows at 10 ms intervals using 25 mel coefficients apiece, each training example would be 1250-dimensional, but I can imagine this being cut down drastically if need be.

I've been looking around to assess my options as far as what language to go with. The only comprehensive tutorial I've found is this one: http://deeplearning.net/tutorial/ and **as far as I can tell, Theano is the only framework equipped to deal with, not only deep learning, but unsupervised deep learning. I wonder, do any of the libraries written with Theano (such as the ones listed under Theano here http://www.teglor.com/b/deep-learning-libraries-language-cm569/) deal with unsupervised learning? Any other, better options I'm unaware of?**",9,3
764,2016-7-26,2016,7,26,11,4umd7s,Artificially Increasing Dataset Size,https://www.reddit.com/r/MachineLearning/comments/4umd7s/artificially_increasing_dataset_size/,FiniteDelight,1469500161,"Say I've trained a classification model on a training set, validated it on a validation set, and tested it on a test set. Let's say that the model classifies correctly 90% of the time.

Let's say it's not a particularly large amount of data, but there's a ton of unclassified data available (tweets, images, etc). Can you increase your training data by having the model classify a number of data points then re-train the model on old data + classified data (once again validated on validation set and tested on test set)? Would this increase or decrease the error rate?

On an intuitive level, this doesn't seem like something you should be able to do (the model would end up learning errors more). But, on the other hand, if the initial training of the model was better than 50%, shouldn't the model ultimately learn more regardless? In machine learning classes and articles, I've heard/read that as long as the data is labeled correctly more often than not, the model would learn, even if it learned slower.

So, can you artificially increase the size of the training set?",17,7
765,2016-7-26,2016,7,26,13,4umsaf,DeepMind AI reduces energy used for cooling Google data centers by 40%,https://www.reddit.com/r/MachineLearning/comments/4umsaf/deepmind_ai_reduces_energy_used_for_cooling/,conusferre,1469506348,,0,0
766,2016-7-26,2016,7,26,15,4un7bx,"Shall we make a sticky beginner post to minimize redundant questions, and be of more help?",https://www.reddit.com/r/MachineLearning/comments/4un7bx/shall_we_make_a_sticky_beginner_post_to_minimize/,jvdalen,1469513561,"I've been here for 2 months now. I'm a beginner, and like the beginners questions, but also see that some people get annoyed by the beginner questions. 

I think some questions are quite repetitive (which gpu, which setup, which course first, which course after Coursera Andrew NG course) etc. 

I thought maybe we can make a nice post to welcome beginners and guide them on their way?

Would mods think that is good idea? Anyone want to help with writing that? ",11,64
767,2016-7-26,2016,7,26,15,4un7gf,what is the differences between high speed dispersers and powerful dispersers for paint?,https://www.reddit.com/r/MachineLearning/comments/4un7gf/what_is_the_differences_between_high_speed/,mixmachinery,1469513627,,1,1
768,2016-7-26,2016,7,26,15,4uncbo,Running an ML product team,https://www.reddit.com/r/MachineLearning/comments/4uncbo/running_an_ml_product_team/,cdiddiest,1469516212,"Hi. I'm an ML engineering / team lead and I have a conundrum I hope you can help me with. If you work in a team that builds machine learning based product, I'd like to know how you do it?
How do you trade off research time vs getting a model out there?
How do you deal with estimation, project planning, sprints and timelines?
How do you deal with the uncertainty that a model will even work in production?

Proper applied ML research to me doesn't seem  to fit into standard agile methodology. Any thoughts, insights or learnings much appreciated :-)",19,27
769,2016-7-26,2016,7,26,17,4unpmc,Using machine learning for itemised electricity bills,https://www.reddit.com/r/MachineLearning/comments/4unpmc/using_machine_learning_for_itemised_electricity/,nipun_batra,1469523555,,0,0
770,2016-7-26,2016,7,26,18,4unpr3,ELI5: How exactly does SIFT work?,https://www.reddit.com/r/MachineLearning/comments/4unpr3/eli5_how_exactly_does_sift_work/,programthroway,1469523622,"I know what it is used for, but how does a DOG give you back features that are independent of scale and rotation? And can I use these features in CNNs? ",1,0
771,2016-7-26,2016,7,26,19,4unwgs,TensorFlow/TFLearn Introduction Tutorial for Beginners,https://www.reddit.com/r/MachineLearning/comments/4unwgs/tensorflowtflearn_introduction_tutorial_for/,mela1029,1469527579,,2,24
772,2016-7-26,2016,7,26,19,4unyh1,[Help] Finding volume without using formula,https://www.reddit.com/r/MachineLearning/comments/4unyh1/help_finding_volume_without_using_formula/,Theneonrider,1469528657,"Hello redditors,
I'm new to ML and so I was just wondering is it possible to input 3 numbers (say length, breadth and height of a cuboid) so that it returns volume. But the thing being that I can't use the volume formula, instead by using ML to eventually find the correct volume. If possible how would I go about it? Thank you in advance :)",12,0
773,2016-7-26,2016,7,26,20,4uo3b9,AMD Announces Radeon Pro SSG: Polaris With M.2 SSDs On-Board,https://www.reddit.com/r/MachineLearning/comments/4uo3b9/amd_announces_radeon_pro_ssg_polaris_with_m2_ssds/,nickl,1469531415,,25,66
774,2016-7-26,2016,7,26,20,4uo4fz,Language Modelling a Billion Words,https://www.reddit.com/r/MachineLearning/comments/4uo4fz/language_modelling_a_billion_words/,larseidnes,1469532032,,0,13
775,2016-7-26,2016,7,26,21,4uoc6i,[1607.07110] Deep nets for local manifold learning,https://www.reddit.com/r/MachineLearning/comments/4uoc6i/160707110_deep_nets_for_local_manifold_learning/,bdamos,1469535828,,3,11
776,2016-7-26,2016,7,26,22,4uohir,"Prisma, BAE, Algorithmia, Ainslie Sailing, DeepMind | Machine Learning in a Nutshell",https://www.reddit.com/r/MachineLearning/comments/4uohir/prisma_bae_algorithmia_ainslie_sailing_deepmind/,EsportsinaNutshell,1469538020,,2,0
777,2016-7-26,2016,7,26,22,4uokqv,Language modeling a billion words! using Noise Contrastive Estimation and multiple GPUs,https://www.reddit.com/r/MachineLearning/comments/4uokqv/language_modeling_a_billion_words_using_noise/,r-sync,1469539259,,22,38
778,2016-7-26,2016,7,26,23,4uouyk,Random Bits Forest: a Strong Classifier/Regressor for Big Data,https://www.reddit.com/r/MachineLearning/comments/4uouyk/random_bits_forest_a_strong_classifierregressor/,gogogadgetlegz,1469542980,,13,31
779,2016-7-26,2016,7,26,23,4uozh8,The Introduction to Neural Networks we all Need (Part -1) For Dummies!,https://www.reddit.com/r/MachineLearning/comments/4uozh8/the_introduction_to_neural_networks_we_all_need/,harshpokharna,1469544478,,0,0
780,2016-7-27,2016,7,27,0,4up1uv,Machine Learning is Fun! Part 4: Modern Face Recognition with Deep Learning (xpost: /r/programming),https://www.reddit.com/r/MachineLearning/comments/4up1uv/machine_learning_is_fun_part_4_modern_face/,descention,1469545298,,1,3
781,2016-7-27,2016,7,27,0,4up72e,FractalNet implementation in Keras with CIFAR-10/100 results (Ultra-Deep Neural Net without Residuals),https://www.reddit.com/r/MachineLearning/comments/4up72e/fractalnet_implementation_in_keras_with/,snfernandez,1469546988,,7,23
782,2016-7-27,2016,7,27,0,4up9f0,Are creative types driving the current AI revolution,https://www.reddit.com/r/MachineLearning/comments/4up9f0/are_creative_types_driving_the_current_ai/,toisanji,1469547705,,1,0
783,2016-7-27,2016,7,27,0,4upac8,What kinds of cloud compute systems are available for deep learning? Which one should I use?And I'm using Tensorflow.,https://www.reddit.com/r/MachineLearning/comments/4upac8/what_kinds_of_cloud_compute_systems_are_available/,Mr__Christian_Grey,1469548023,,1,0
784,2016-7-27,2016,7,27,1,4upfoq,DeepWarp: Photorealistic Image Resynthesis for Gaze Manipulation,https://www.reddit.com/r/MachineLearning/comments/4upfoq/deepwarp_photorealistic_image_resynthesis_for/,olBaa,1469549744,,7,51
785,2016-7-27,2016,7,27,2,4upoep,Making Sense of Everything with words2map,https://www.reddit.com/r/MachineLearning/comments/4upoep/making_sense_of_everything_with_words2map/,lmcinnes,1469552572,,2,7
786,2016-7-27,2016,7,27,2,4upxb9,Keras model accuray not changed,https://www.reddit.com/r/MachineLearning/comments/4upxb9/keras_model_accuray_not_changed/,alex43210-1,1469555334,"E.g. I have next function (just test example, not real):

    def f(x):
      return [
        {True: 1.0, False: 0.0}[item[0] &gt; 0.5] for item in x
      ]

And I want to approximate it with neural network:

    self.model = keras.models.Sequential()
    self.model.add(keras.layers.Dense(input_dim=self.class_count, output_dim=10*self.class_count))
    self.model.add(keras.layers.Dense(output_dim=self.class_count, activation='softmax'))
    self.model.compile(keras.optimizers.SGD(lr=0.05), 'mse', ['accuracy'])
    self.model.fit(x, y)

x have next view:

    [[0.00], [0.01], ... [0.99], [1.00]]

y have next view:

    [[0.00], [0.00], ... [1.00], [1.00]]

Training output next:

    Epoch 1/10
    100/100 [==============================] - 0s - loss: 0.5100 - acc: 0.4900     
    Epoch 2/10
    100/100 [==============================] - 0s - loss: 0.5100 - acc: 0.4900     
    Epoch 3/10
    100/100 [==============================] - 0s - loss: 0.5100 - acc: 0.4900     
    Epoch 4/10
    100/100 [==============================] - 0s - loss: 0.5100 - acc: 0.4900     
    Epoch 5/10
    100/100 [==============================] - 0s - loss: 0.5100 - acc: 0.4900     
    Epoch 6/10
    100/100 [==============================] - 0s - loss: 0.5100 - acc: 0.4900     
    Epoch 7/10
    100/100 [==============================] - 0s - loss: 0.5100 - acc: 0.4900     
    Epoch 8/10
    100/100 [==============================] - 0s - loss: 0.5100 - acc: 0.4900     
    Epoch 9/10
    100/100 [==============================] - 0s - loss: 0.5100 - acc: 0.4900     
    Epoch 10/10
    100/100 [==============================] - 0s - loss: 0.5100 - acc: 0.4900  

And result

    f(0.0) = [0.0], f_approx(0.0) = [ 1.]
    f(0.01) = [0.0], f_approx(0.01) = [ 1.]
    f(0.99) = [1.0], f_approx(0.99) = [ 1.]
    f(1.00) = [1.0], f_approx(1.00) = [1.]

What I can do wrong?
",2,0
787,2016-7-27,2016,7,27,3,4uq1s7,C2W with TensorFlow?,https://www.reddit.com/r/MachineLearning/comments/4uq1s7/c2w_with_tensorflow/,[deleted],1469556747,[deleted],0,0
788,2016-7-27,2016,7,27,3,4uq7lu,Genetic Algorithms vs Neural Network,https://www.reddit.com/r/MachineLearning/comments/4uq7lu/genetic_algorithms_vs_neural_network/,online204,1469558548,I've been reading more into genetic algorithms and I'm having a hard time finding the difference in use case from Neural Networks. Reading [this](http://stackoverflow.com/a/1402410/1477372) I think I understand the difference is similar to BFS to DFS in that one will find an answer that works faster (Neural Network) while the other will take a long time to find an answer but it will be the optimized path between the input and output. Is this the right thinking or is there a different way to compare the two?,4,0
789,2016-7-27,2016,7,27,3,4uq84k,Deep Learning and Prostate Cancer,https://www.reddit.com/r/MachineLearning/comments/4uq84k/deep_learning_and_prostate_cancer/,Zan_Huang,1469558723,[removed],0,1
790,2016-7-27,2016,7,27,3,4uq9mo,My process of solving a machine learning problem,https://www.reddit.com/r/MachineLearning/comments/4uq9mo/my_process_of_solving_a_machine_learning_problem/,Sig_Luna,1469559183,,2,1
791,2016-7-27,2016,7,27,4,4uqdz0,"How do you get ""cleaner"" text data for training word embeddings",https://www.reddit.com/r/MachineLearning/comments/4uqdz0/how_do_you_get_cleaner_text_data_for_training/,bearjustwannahavefun,1469560538,"Hello! I am a graduate student focusing on ML and NLP. I am trying to play with word2vec using text data that I crawled from the web. Not surprisingly, those data are much more ""dirty"" than any other standard dataset we used before. Here ""google"" can be treated as an ADJ since the are a lot ""google X(X=play, maps, etc.)"" (this is legal but annoying) and other real noises ""@yourGrandMother tweeted 14 minutes ago"". So in your experience what's the best way to get ""cleanest"" input for word2vec? Or should I use other tools like RNN instead?
Thanks!",4,1
792,2016-7-27,2016,7,27,5,4uqr65,Machine Learning Blog Feedback,https://www.reddit.com/r/MachineLearning/comments/4uqr65/machine_learning_blog_feedback/,[deleted],1469564767,[removed],0,1
793,2016-7-27,2016,7,27,5,4uqsy7,Example - Chat Bot using Microsoft Bot Framework and Cognitive Services (LUIS),https://www.reddit.com/r/MachineLearning/comments/4uqsy7/example_chat_bot_using_microsoft_bot_framework/,andreluizsecco,1469565346,,0,0
794,2016-7-27,2016,7,27,5,4uqwde,Machine Learning Blog Feedback,https://www.reddit.com/r/MachineLearning/comments/4uqwde/machine_learning_blog_feedback/,tsmith5151,1469566446,[removed],0,1
795,2016-7-27,2016,7,27,6,4ur0re,Commit-and-Push to GitHub from Jupyter Notebooks,https://www.reddit.com/r/MachineLearning/comments/4ur0re/commitandpush_to_github_from_jupyter_notebooks/,amplifier_khan,1469567938,,3,8
796,2016-7-27,2016,7,27,6,4ur4nr,SyntaxNet Subtoken Encodings in TensorFlow,https://www.reddit.com/r/MachineLearning/comments/4ur4nr/syntaxnet_subtoken_encodings_in_tensorflow/,[deleted],1469569238,[deleted],0,0
797,2016-7-27,2016,7,27,7,4ur821,Prof. Geoffrey Hinton Awarded IEEE Medal For His Work In Artificial Intelligence,https://www.reddit.com/r/MachineLearning/comments/4ur821/prof_geoffrey_hinton_awarded_ieee_medal_for_his/,andyandy16,1469570424,,34,366
798,2016-7-27,2016,7,27,7,4ur8dp,"Self-driving car model using deepdrive.io data from GTAV, Livestream",https://www.reddit.com/r/MachineLearning/comments/4ur8dp/selfdriving_car_model_using_deepdriveio_data_from/,vanboxel,1469570542,,5,5
799,2016-7-27,2016,7,27,7,4urchi,Trouble utilizing decisions trees for sales conversion rates,https://www.reddit.com/r/MachineLearning/comments/4urchi/trouble_utilizing_decisions_trees_for_sales/,[deleted],1469571987,[deleted],0,1
800,2016-7-27,2016,7,27,8,4urp4n,Is there any research on using distinct activation functions in the same network?,https://www.reddit.com/r/MachineLearning/comments/4urp4n/is_there_any_research_on_using_distinct/,Lajamerr_Mittesdine,1469576548,"Besides adding complexity in design is there advantages or disadvantages in using more than one activation function on different nodes in a single network?

Do certain types of problems benefit from this approach?

Is it slower to train?",6,6
801,2016-7-27,2016,7,27,9,4urz07,Is Classless Data Useful For Training Neural Nets,https://www.reddit.com/r/MachineLearning/comments/4urz07/is_classless_data_useful_for_training_neural_nets/,Greendogo,1469580375,"For training a conv net to do object detection, is there any benefit to having ""classless"" (by which I mean, images without any of the representative classes) data in your dataset to serve as something of a control group?

Are there any papers on this?

I have a LOT of images that have none of my classes in them, so if so, that would be perfect.",5,0
802,2016-7-27,2016,7,27,11,4usio3,"All the DNC emails - I meant to build a Markov chain, but then my grandmother had a heart attack,",https://www.reddit.com/r/MachineLearning/comments/4usio3/all_the_dnc_emails_i_meant_to_build_a_markov/,[deleted],1469588354,[deleted],1,0
803,2016-7-27,2016,7,27,12,4uspt1,[ML Search] blog post about inception architecture,https://www.reddit.com/r/MachineLearning/comments/4uspt1/ml_search_blog_post_about_inception_architecture/,Tamazy,1469591314,"Hello there,

Sorry for this thread, but I'm looking for a blog post about the inception architecture (or network in network) I saw recently on reddit, but I can't find it anymore... I don't have a lot of information about it besides the red and white colors of the website....

This is a desperate call. I really hope the distributed memory of the machine learners community will be able to help me.

Thanks in advance
",6,0
804,2016-7-27,2016,7,27,13,4ustp8,revrand - python software for large scale Gaussian process approximation,https://www.reddit.com/r/MachineLearning/comments/4ustp8/revrand_python_software_for_large_scale_gaussian/,dsberg,1469593089,,0,1
805,2016-7-27,2016,7,27,14,4usz9a,I'm unsure if this is the right place to ask this question --- what do you all think of Cylance. They are using AI and machine learning to enhance endpoint protection. Would love to know what you think about this technology.,https://www.reddit.com/r/MachineLearning/comments/4usz9a/im_unsure_if_this_is_the_right_place_to_ask_this/,Boschlana,1469595760,,1,0
806,2016-7-27,2016,7,27,16,4utf6g,Many Categorical Outputs vs a Single Continuous output,https://www.reddit.com/r/MachineLearning/comments/4utf6g/many_categorical_outputs_vs_a_single_continuous/,deepaurorasky,1469604076,"I am building a system where we need to check if 2 sentence embeddings are a ""match"" in terms of semantic similarity. So I'm designing a linear model that will take 2 sentence embeddings as input and I'm confused to what I should design the model to output. 

I need to know 2 things about these sentences:
(a) A binary category: Do these 2 sentences ""match"" or not. 
(b) If they do match, what is the quality of the match (are these 2 sentences semantically identical or weakly related). 

In my mind, the model should have a single output neuron with a single value which will be the ""quality"" of the match and then convert that into a binary decision based on a threshold (I'll use a ROC to decide what the threshold should be) 

In literature however, I've not found that people are doing that. In particular, I'm looking at Ryan Kiros et al's Skip Thought Vector paper: For semantic relatedness, each pair of sentences in the data is assigned a class between 1 and 5 where 1 is not related at all and 5 indicates highly related. Their output layer has 5 neurons. Each representing the probability of a pair of sentences belonging to the class 1,2,3,4,5. 

Why would they do this categorical output instead of using a singular continuous output? Does it provide more information? ",4,2
807,2016-7-27,2016,7,27,16,4utfh6,Tensorflow - Trying to follow the recurrent neural network tutorial,https://www.reddit.com/r/MachineLearning/comments/4utfh6/tensorflow_trying_to_follow_the_recurrent_neural/,Bocard,1469604234,"I'm trying to follow the rnn tutorial from tensorflow (https://www.tensorflow.org/versions/0.6.0/tutorials/recurrent/index.html#recurrent-neural-networks) but I am a bit confused about the sections LSTM and Truncated back propagation. Where is that code supposed to go?
So far I have added some code to save the model at different checkpoints.
You can see the code I have so far here:
http://pastebin.com/cLvST3sw

For now I would like to understand where the code from those 2 sections should go but the end goal is to have a model that I will be able to query which will provide the top 2 word predictions given a string/sentence as an input.

I am new to tensorflow and LSTM so any advice is welcome.",0,0
808,2016-7-27,2016,7,27,16,4utfoj,Need help starting a pet project,https://www.reddit.com/r/MachineLearning/comments/4utfoj/need_help_starting_a_pet_project/,thewhitetulip,1469604356,"Since a few months I started studying machine learning, but I got lost among the sheer number of books I got as recommendation.

So now I have started the experimental approach of building something rather than just reading, a friend of mine suggested me to start building things.

I want to build an AI/ML application as siri meets google now. but it'll be totally personalized for me only, no internet related stuff.

How shall I start?

My motive is to write posts like a diary entry and the AI will take its data from my posts and update me regarding things. let's say I write, I met with Paul. After few days or months if I don't meet paul, it'll tell me, hey it has been a while since you got in touch with Paul.

Or if I say allen had a birthday today, so it'll remind me every year allen's birthday comes",8,2
809,2016-7-27,2016,7,27,16,4uthtt,More than 20 years manufacturer of bottle filling machine and water bottling plant,https://www.reddit.com/r/MachineLearning/comments/4uthtt/more_than_20_years_manufacturer_of_bottle_filling/,[deleted],1469605587,[deleted],0,1
810,2016-7-27,2016,7,27,18,4utpp2,"12000BPH full automatic water bottling plant in China, bottled water production line",https://www.reddit.com/r/MachineLearning/comments/4utpp2/12000bph_full_automatic_water_bottling_plant_in/,stevenwangfilling,1469610338,,0,1
811,2016-7-27,2016,7,27,18,4utqlf,The best of ICML 2016 - from Drew Bagnell,https://www.reddit.com/r/MachineLearning/comments/4utqlf/the_best_of_icml_2016_from_drew_bagnell/,Bayes-Ian,1469610881,,0,26
812,2016-7-27,2016,7,27,18,4uts07,What is an epoch? Confused by terminology.,https://www.reddit.com/r/MachineLearning/comments/4uts07/what_is_an_epoch_confused_by_terminology/,[deleted],1469611778,[deleted],0,1
813,2016-7-27,2016,7,27,19,4utxw8,Machine Learning For Developers,https://www.reddit.com/r/MachineLearning/comments/4utxw8/machine_learning_for_developers/,akshaysondur,1469615303,,0,1
814,2016-7-27,2016,7,27,19,4utyz5,Flour mill machine,https://www.reddit.com/r/MachineLearning/comments/4utyz5/flour_mill_machine/,Machines2016,1469615903,,0,1
815,2016-7-27,2016,7,27,21,4uubt3,Create your own AI art with neural networks,https://www.reddit.com/r/MachineLearning/comments/4uubt3/create_your_own_ai_art_with_neural_networks/,jguertl,1469622179,,0,1
816,2016-7-27,2016,7,27,21,4uucun,SuperResolution: Better Image Scaling with a Convolutional Neural Network,https://www.reddit.com/r/MachineLearning/comments/4uucun/superresolution_better_image_scaling_with_a/,jntgdk,1469622587,,19,35
817,2016-7-27,2016,7,27,21,4uuf3p,Could someone take a look at my Theano MLP code?,https://www.reddit.com/r/MachineLearning/comments/4uuf3p/could_someone_take_a_look_at_my_theano_mlp_code/,[deleted],1469623493,[deleted],3,0
818,2016-7-27,2016,7,27,22,4uupj7,language model from dictionary using ctc loss for speech recognition,https://www.reddit.com/r/MachineLearning/comments/4uupj7/language_model_from_dictionary_using_ctc_loss_for/,[deleted],1469627596,[deleted],0,0
819,2016-7-27,2016,7,27,23,4uuse6,20+ use cases that shows how deep learning applications are used in various fields,https://www.reddit.com/r/MachineLearning/comments/4uuse6/20_use_cases_that_shows_how_deep_learning/,datameer,1469628597,,0,0
820,2016-7-27,2016,7,27,23,4uuwjt,Resources for Real-time recommendation,https://www.reddit.com/r/MachineLearning/comments/4uuwjt/resources_for_realtime_recommendation/,warosaurus,1469630090,"Could you help me find more good resources on Real-time Recommendation? I've searched quite a bit, found some info on SlopeOne and a good slide show from Spark-summit. I'm looking for more, preferably recent.",5,0
821,2016-7-27,2016,7,27,23,4uuy20,Understanding recent research in biological plausibility,https://www.reddit.com/r/MachineLearning/comments/4uuy20/understanding_recent_research_in_biological/,[deleted],1469630580,[deleted],2,5
822,2016-7-27,2016,7,27,23,4uv08m,Neural Net turns NYC into moving painting (Danil Krivoruchko),https://www.reddit.com/r/MachineLearning/comments/4uv08m/neural_net_turns_nyc_into_moving_painting_danil/,Chineseerotica,1469631279,,57,261
823,2016-7-27,2016,7,27,23,4uv0nl,Gradient Boosting explained by Alex Rogozhnikov,https://www.reddit.com/r/MachineLearning/comments/4uv0nl/gradient_boosting_explained_by_alex_rogozhnikov/,pmigdal,1469631405,,4,35
824,2016-7-28,2016,7,28,0,4uv693,Learning from Data MOOC starting in September,https://www.reddit.com/r/MachineLearning/comments/4uv693/learning_from_data_mooc_starting_in_september/,aprstar,1469633181,,0,5
825,2016-7-28,2016,7,28,0,4uv9rx,LSTM/RNN Network vs. Sliding Window with Feedforward Network,https://www.reddit.com/r/MachineLearning/comments/4uv9rx/lstmrnn_network_vs_sliding_window_with/,cm_kcg,1469634320,"I'm learning how to use keras to build neural networks for time series prediction. In many examples that I've seen (e.g. https://github.com/Vict0rSch/deep_learning/tree/master/keras/recurrent, http://danielhnyk.cz/predicting-sequences-vectors-keras-using-rnn-lstm/) there is a parameter sequence_length so that the network predicts the nth instance based on some fixed history.

What is the benefit of this approach over using a feedforward network with a sliding window? They both use a sliding window so it isn't clear why the LSTM cells should improve prediction.",6,10
826,2016-7-28,2016,7,28,0,4uvb9h,Machine Learning Applications,https://www.reddit.com/r/MachineLearning/comments/4uvb9h/machine_learning_applications/,collonmade,1469634804,,2,3
827,2016-7-28,2016,7,28,1,4uvfda,Questions thread #8 2016.07.27,https://www.reddit.com/r/MachineLearning/comments/4uvfda/questions_thread_8_20160727/,feedtheaimbot,1469636113,"**Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!**

Thread will stay alive until next one so keep posting after the date in the title. 

Thanks to everyone for answering questions in the previous thread!

Previous threads:

* [Questions thread #7 2016.06.29](https://www.reddit.com/r/MachineLearning/comments/4qg8yq/questions_thread_7_20160629/)

* [Questions thread #6 2016.05.23](https://www.reddit.com/r/MachineLearning/comments/4kq3jx/questions_thread_6_20160523/)

* [Questions thread #5 2016.05.07]
(https://www.reddit.com/r/MachineLearning/comments/4ibv66/questions_thread_5_20160507/)

* [Questions thread #4 2016.04.22]
(https://www.reddit.com/r/MachineLearning/comments/4fytfp/questions_thread_4_20160422/)

* [Questions Thread #3 2016.04.07](https://www.reddit.com/r/MachineLearning/comments/4dthzx/questions_thread_3_20160407/)

* [Simple Questions Thread #2 + Meta - 2016.03.23](https://www.reddit.com/r/MachineLearning/comments/4bp1ck/simple_questions_thread_2_meta_20160323/)

* [Simple Questions Thread #1 - 2016.03.08](https://www.reddit.com/r/MachineLearning/comments/49k54u/simple_questions_thread_20160308/)",252,29
828,2016-7-28,2016,7,28,1,4uvflh,Java Machine Learning,https://www.reddit.com/r/MachineLearning/comments/4uvflh/java_machine_learning/,ale86ch,1469636189,"Hello,
I have to implement a simple face detection approach using a convolutional neural network trained on publicly-available datasets. The face detection must be from pictures of course and from a webcam.
I found different Machine learning frameworks/library, but I have no experience at all so I have no idea which one is better for what I have to do.

So far I have found:
Weka: 			http://www.cs.waikato.ac.nz/ml/weka/
Rapidminer:		https://rapidminer.com/
Deeplearning4j:		http://deeplearning4j.org/
OpenCV: 			http://opencv.org/about.html

and some others but I need something simple that won't be too complicated to start with and to do implement my face detection. I also found Caffe and thanks to someone which I forgot the name this: https://github.com/bytedeco/javacpp-presets/tree/master/caffe

I would really like to know if one of those I have found or if there is something else that is the best and also the simplest for my purpose. I start from the very beginning so if someone have also some tutorials to share I would really appreciate it.",14,1
829,2016-7-28,2016,7,28,1,4uvfpa,[1607.04614] Guided Policy Search as Approximate Mirror Descent,https://www.reddit.com/r/MachineLearning/comments/4uvfpa/160704614_guided_policy_search_as_approximate/,pierrelux,1469636228,,0,5
830,2016-7-28,2016,7,28,1,4uvl76,Reliability of experience/knowledge gained with DL/ML experiments?,https://www.reddit.com/r/MachineLearning/comments/4uvl76/reliability_of_experienceknowledge_gained_with/,yield22,1469638086,"Especially with DL, whether or not something (like relu, or BN) is useful under certain circumstances needs validation from experiments. However, as ML experiments not like physical experiments, highly rely on data sets, and ""soft"" setup of experiments (e.g. something could work may be broken due to incorrectly learning rate used, or init. etc.). Likely sometimes something work due to other (intermediate) reasons but not the one you look for. It seems to me, reasoning based on experiments with limited reliability is probably not a very good thing to do.

I don't know how many people have similar feelings, and whether people see it as an issue, and any attempts to address it in general?",1,0
831,2016-7-28,2016,7,28,2,4uvtqy,Bijur Delimon Lubrication Accessories are Essential for Your Manufacturing Industry  Heres Why!,https://www.reddit.com/r/MachineLearning/comments/4uvtqy/bijur_delimon_lubrication_accessories_are/,jackerfrinandis,1469640801,,0,1
832,2016-7-28,2016,7,28,2,4uvxyj,Merging misspellings,https://www.reddit.com/r/MachineLearning/comments/4uvxyj/merging_misspellings/,njaard,1469642138,"I'm new to Machine Learning, so please treat my question and answer as such

I have a collection of data with names, like so:

    AADAS Anne Murray 120310230   
    QQRRF Anne Murray 123120202
    AA Ann Murray 12312
    ASGSD Billy Travis Texas 1234123
    523 Billy Texas 1234123
    523 William Travis Texas AASDAS
    (many more)

The first three of these should be grouped into bucket ""A"" and the last three are grouped into bucket ""B"". You'll notice that Anne is misspelled and Billy has multiple names, and every line has noise.

I have a training set where like-items are already grouped with eachother. My training set contains perhaps 1000 items and the entire dataset is possibly 10,000,000.

What is an algorithm /r/MachineLearning can recommend that would automatically take my input and use the training set to automatically merge like-items into buckets?
",0,1
833,2016-7-28,2016,7,28,3,4uwa5j,Baidu's AI Composer translates famous artworks into melodies you can listen to,https://www.reddit.com/r/MachineLearning/comments/4uwa5j/baidus_ai_composer_translates_famous_artworks/,xcfmv,1469645949,,4,0
834,2016-7-28,2016,7,28,4,4uwb2j,[1607.06520] Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings,https://www.reddit.com/r/MachineLearning/comments/4uwb2j/160706520_man_is_to_computer_programmer_as_woman/,victorhugo,1469646221,,7,9
835,2016-7-28,2016,7,28,4,4uwgmg,An implementation of the LexVec word embedding model that achieves state of the art results in multiple NLP tasks,https://www.reddit.com/r/MachineLearning/comments/4uwgmg/an_implementation_of_the_lexvec_word_embedding/,galapag0,1469647900,,3,7
836,2016-7-28,2016,7,28,5,4uwttj,Advice regarding a simple Entity Recognition algorithm,https://www.reddit.com/r/MachineLearning/comments/4uwttj/advice_regarding_a_simple_entity_recognition/,Zerithious,1469651902,"Hey guys. I want to write a simple Entity Recognition algorithm to extract some basic attributes from product titles on ebay. (e.g. 'New Sony Xperia Z3 Smartphone LTE 32GB Black' -&gt; {brand: 'Sony', model: 'Xperia Z3"", memory: '32gb', 'color': 'Black'})

I'm pretty new to NLP and ML and I've done some very basic work with sklearn. 

What I was thinking initially after reading some relevant material online is that I should aggregate some large dataset of tagged titles, and then writing a multi-class classifier based on some positional features  (for example, the model probably appears right after the brand, and maybe I can detect a brand based on some common patterns for brands in titles such as position and it not being a common word (such as New))

I still don't know several things:

1. I'm not sure how to deal with multi-word entities (such as ""Xperia Z3"")
2. If there's a common approach to this problem it would probably outperform anything I would come up with..


It would be very helpful if you could point me in the right direction on how to approach this problem. Thanks!

 ",9,0
837,2016-7-28,2016,7,28,5,4uww28,[1606.06582] Augmenting Supervised Neural Networks with Unsupervised Objectives for Large-scale Image Classification,https://www.reddit.com/r/MachineLearning/comments/4uww28/160606582_augmenting_supervised_neural_networks/,brockl33,1469652623,,2,9
838,2016-7-28,2016,7,28,5,4uwxb4,Apache Spark 2.0.0 has been released! [x-post from /r/programming],https://www.reddit.com/r/MachineLearning/comments/4uwxb4/apache_spark_200_has_been_released_xpost_from/,coffeecoffeecoffeee,1469653050,,4,20
839,2016-7-28,2016,7,28,6,4uwzbw,Syria Dreamz,https://www.reddit.com/r/MachineLearning/comments/4uwzbw/syria_dreamz/,Hidden_dreamz,1469653720,,0,0
840,2016-7-28,2016,7,28,6,4uwzca,Anylsis of ROC and Precision-Recall curve,https://www.reddit.com/r/MachineLearning/comments/4uwzca/anylsis_of_roc_and_precisionrecall_curve/,Bohemian90,1469653724,"Hello

I have run some machine learning experiments and now I have created some ROC and Precision-Recall curves (with the help of a toolbox).

Unfortunately, I'm not familiar with these two things. Of course, in the web there is plenty of material describing it but I did not find some good explanation based on an example.

Is there somebody who can show how one can analyse classifiers and also compare them based on the ROC and Precision-Recall curve? Perhaps this could easier be understood when following an example.

I would highly appreciate if somebody can bring light into the dark.",3,0
841,2016-7-28,2016,7,28,6,4ux24a,Anything2Vec,https://www.reddit.com/r/MachineLearning/comments/4ux24a/anything2vec/,amplifier_khan,1469654641,,1,3
842,2016-7-28,2016,7,28,8,4uxlbg,"Everything about my one year efforts to gather the critical mass for defining a remote sensing and photogrammetry proposal on area51 zone of StackExchange network, Now only 8% is left to success in this phase an just 6 more questions with 10 or10+ score",https://www.reddit.com/r/MachineLearning/comments/4uxlbg/everything_about_my_one_year_efforts_to_gather/,SepidehAbadpour,1469661329,,0,1
843,2016-7-28,2016,7,28,9,4uxspt,Missing features in a neural network,https://www.reddit.com/r/MachineLearning/comments/4uxspt/missing_features_in_a_neural_network/,RoamBear,1469664030,"Hey,

I'm making a neural network that will classify people based on a set of features pulled from a vision system. Some features might be missing some of the time depending on the person's pose (for example if they scrunch themselves up and block their body with their legs). 

Is there a placeholder value you can give a neural network when a feature is not visible? Sort of the equivalent of saying ""pay attention to this when it's present, but otherwise rely more on the other features"".",4,0
844,2016-7-28,2016,7,28,9,4uxvk5,Understanding neural networks with TensorFlow Playground,https://www.reddit.com/r/MachineLearning/comments/4uxvk5/understanding_neural_networks_with_tensorflow/,fhoffa,1469665040,,0,4
845,2016-7-28,2016,7,28,9,4uxza9,Tensorflow implementation of Layer Normalized GRU and LSTM,https://www.reddit.com/r/MachineLearning/comments/4uxza9/tensorflow_implementation_of_layer_normalized_gru/,parry243,1469666421,,0,1
846,2016-7-28,2016,7,28,10,4uy8vy,Keras w/ Theano vs Theano,https://www.reddit.com/r/MachineLearning/comments/4uy8vy/keras_w_theano_vs_theano/,[deleted],1469670227,[deleted],4,0
847,2016-7-28,2016,7,28,13,4uyy2w,"accuracy comparison of UltraForest, xgboost and Random Bits Forest",https://www.reddit.com/r/MachineLearning/comments/4uyy2w/accuracy_comparison_of_ultraforest_xgboost_and/,godspeed_china,1469681037,,4,2
848,2016-7-28,2016,7,28,14,4uz6h9,Variable size Convolutional Neural Network Input and Fixed output in Tensorflow,https://www.reddit.com/r/MachineLearning/comments/4uz6h9/variable_size_convolutional_neural_network_input/,yazfield,1469685031,"I am trying to make a CNN model that takes variable size input (sentence matrix) and produce a fixed size output for a subsequent fully connected layer ([similar to this paper](https://arxiv.org/pdf/1412.1058v2.pdf)).

I am trying to implement a dynamic kernel size for a max pooling layer so I need the shape of the input at runtime to achieve this.

    input = tf.placeholder(tf.float32)
    # convolution layer here .... 
    
    tf.nn.max_pool(convolution_output, ksize=[1, s, 1, 1],
                          strides=[1, 1, 1, 1], padding='VALID')

`s` in `ksize=[1, s, 1, 1]` should be inferred from the input shape.

However, I can't find a way to do it with Tensorflow.

Anyone knows a way to do it?

P.S: I've posted the question in [Stackoverflow](http://stackoverflow.com/questions/38628014/variable-size-convolutional-neural-network-input-and-fixed-output) too",2,1
849,2016-7-28,2016,7,28,15,4uzehf,Playing games using machine learning,https://www.reddit.com/r/MachineLearning/comments/4uzehf/playing_games_using_machine_learning/,[deleted],1469689143,[removed],0,1
850,2016-7-28,2016,7,28,16,4uzicw,System for Pre-Recommendation,https://www.reddit.com/r/MachineLearning/comments/4uzicw/system_for_prerecommendation/,Reubend,1469691192,"**The Problem**

I set up a recommendation engine based on the collaborative filtering and user properties (age, gender, etc.). Unfortunately, it's expensive to run and slow to finish, so I run it every 15 minutes with whatever new data I received in that time span. 

I want to make a ""pre-recommendation"" system that runs whenever a new user is added. The algorithm would take properties like age, gender, etc. and generate a set of temporary recommendations that can be used until the proper recommendation engine runs. 

**My Ideas**

* I know that there are recommendation engines that [can already do this](http://actionml.com/docs/ur) with correlated cross-occurrence algorithms, but something like that seems difficult to implement to me and as far as I know it's an unpopular technique. 
* I also thought that this could be solved by clustering the users based on their properties (and recommending a new user items based on their cluster) but that wouldn't take into account the rating data. 

How should I approach this problem? Is there any standard technique for this type of goal?",6,0
851,2016-7-28,2016,7,28,17,4uzmyn,How to detect PAID SHILLS using MachineLearning?,https://www.reddit.com/r/MachineLearning/comments/4uzmyn/how_to_detect_paid_shills_using_machinelearning/,j_lyf,1469693681,[removed],1,0
852,2016-7-28,2016,7,28,17,4uzow4,CS229T/STATS231: Statistical Learning Theory Lectures ?,https://www.reddit.com/r/MachineLearning/comments/4uzow4/cs229tstats231_statistical_learning_theory/,rishok,1469694810,"Hi

Are there video of the lectures from the stanford course: CS229T/STATS231: Statistical Learning Theory ?
 ",4,5
853,2016-7-28,2016,7,28,17,4uzpr1,"Doosan corporation y quyn  L phn phi Xe nng Doosan ti Vit nam, xe nng hn quc thng hiu doosan. i l c quyn xe nng doosan hn quc",https://www.reddit.com/r/MachineLearning/comments/4uzpr1/doosan_corporation_y_quyn_l_phn_phi_xe_nng/,xenangvn,1469695317,,0,1
854,2016-7-28,2016,7,28,18,4uzssf,How Vector Space Mathematics Reveals the Hidden Sexism in Language,https://www.reddit.com/r/MachineLearning/comments/4uzssf/how_vector_space_mathematics_reveals_the_hidden/,pmigdal,1469697022,,4,0
855,2016-7-28,2016,7,28,18,4uzuj3,When do I use k medoids instead of k means for text clustering?,https://www.reddit.com/r/MachineLearning/comments/4uzuj3/when_do_i_use_k_medoids_instead_of_k_means_for/,n00bto1337,1469697988,"I understand the difference, that k-medoids uses absolute distance, and centroid is an actual point.
But when do I use one against the other?",4,5
856,2016-7-28,2016,7,28,18,4uzvw3,"(((Delip Rao))) on Twitter: When it comes to DL, don't rely on citations to understand provenance. (Thread)",https://www.reddit.com/r/MachineLearning/comments/4uzvw3/delip_rao_on_twitter_when_it_comes_to_dl_dont/,metacurse,1469698767,,6,0
857,2016-7-28,2016,7,28,19,4uzyd7,Playing games using machine learning,https://www.reddit.com/r/MachineLearning/comments/4uzyd7/playing_games_using_machine_learning/,SingularityEvent,1469700153,[removed],0,1
858,2016-7-28,2016,7,28,20,4v08ua,"[Convnets, Keras] Training loss is stuck at the initial value when I start with a higher learning rate and slowly decrease it down to a learning rate for which the training loss decreases - Why?",https://www.reddit.com/r/MachineLearning/comments/4v08ua/convnets_keras_training_loss_is_stuck_at_the/,qwertz_guy,1469705671,"Something that I have observed multiple times when training neural networks (in particular convnets) is the following: Let's say I have a small learning rate of 0.00001 and if I start with this LR then I can see how my loss is (starting from like 0.95) decreasing after each epoch to a very good value (&lt;0.1) after let's say 20 epochs. So far so good. Now let's say I wanted to accelerate learning in the beginning and I start with a bigger LR of for example 0.1 and everytime 5 epochs have passed I divide LR by 10, so I eventually end up with the learning rate of 0.00001. But here comes the strange thing I can't explain: While it might be natural that a LR of 0.1 is too high and the loss stays at around 0.95, it still doesn't decrease after lowering the learning rate several time, down to 0.00001, the (training )loss stays the same. So to clarify: If I have this learning rate schedule with 5 epochs of each LR = 0.1, 0.001, 0.0001, 0.00001, the loss starts at 0.95 and doesn't decrease, even when it is set to 0.00001 (remember: my loss visibly decreases when I start the training with the LR 0.00001).


Is there any explanation for this behaviour other than a problem with the library I use? I use Keras and set the learning rate via model.optimizer.lr.set_value(), which is a valid way to adjust the learning rate according to one of the Keras devs.",4,1
859,2016-7-28,2016,7,28,21,4v0ddy,[1607.08221] MS-Celeb-1M: A Dataset and Benchmark for Large-Scale Face Recognition,https://www.reddit.com/r/MachineLearning/comments/4v0ddy/160708221_msceleb1m_a_dataset_and_benchmark_for/,bdamos,1469707735,,7,49
860,2016-7-28,2016,7,28,21,4v0ikj,A simple way to recognize handwritten shapes using machine Learning by Geoffrey Hinton,https://www.reddit.com/r/MachineLearning/comments/4v0ikj/a_simple_way_to_recognize_handwritten_shapes/,dynamicstm,1469709970,,2,1
861,2016-7-28,2016,7,28,21,4v0ipf,[blog post] Large Scale Spectral Clustering with Landmark-Based Representation (in Julia): x-post from r/julia,https://www.reddit.com/r/MachineLearning/comments/4v0ipf/blog_post_large_scale_spectral_clustering_with/,int8blog,1469710028,,1,4
862,2016-7-28,2016,7,28,22,4v0kn9,enaible - Artificial Intelligence for Image Analysis,https://www.reddit.com/r/MachineLearning/comments/4v0kn9/enaible_artificial_intelligence_for_image_analysis/,[deleted],1469710820,[deleted],0,0
863,2016-7-28,2016,7,28,22,4v0nyl,Machine learning on predicting presidential performance,https://www.reddit.com/r/MachineLearning/comments/4v0nyl/machine_learning_on_predicting_presidential/,isaacgerg,1469712130,I was wondering if anyone has tried to predict presidential performance using machine learning.  I am leaving presidential performance vague to cast a wide a net as possible.  ,3,0
864,2016-7-28,2016,7,28,23,4v0vpp,"One document to learn numerics, science, and data with Python",https://www.reddit.com/r/MachineLearning/comments/4v0vpp/one_document_to_learn_numerics_science_and_data/,iamkeyur,1469714970,,2,43
865,2016-7-28,2016,7,28,23,4v0xw2,Keras - Image Augmentation,https://www.reddit.com/r/MachineLearning/comments/4v0xw2/keras_image_augmentation/,Nader_Nazemi,1469715705,"Can someone please help me out figure what is the flow of applying ""datagen = ImageDataGenerator()""

For example I have: x_train , x_test , y_train , y_test 

How do I augment x_train AND x_test using ""datagen"" ? 

Do I do the following: 

datagen = ImageDataGenerator(...stuff...)
datagen.fit(x_train)
datagen.fit(x_test)

Then I create my Neural Network Model:

model = Sequential()
.
.
.

THEN 
model.compile(...stuff...)

THEN
model.fit_generator(datagen.flow(x_train, y_train, batch_size=32), samples_per_epoch=len(x_train), nb_epoch=100, verbose=2, validation_data=(x_test, y_test), nb_worker=1)

Thank you so much ",3,4
866,2016-7-29,2016,7,29,0,4v1abe,How do you all run your deep learning programs?,https://www.reddit.com/r/MachineLearning/comments/4v1abe/how_do_you_all_run_your_deep_learning_programs/,gmo517,1469719891,"I'm looking to buy a new laptop and I was curious as to how most of your train your neural nets. I'm using mac and usually just use AWS for training but sometimes I wish I had a nice nvidia GPU so I can test things quickly on my laptop (cuDNN, etc.).

What platform do you all use to train (anything better than AWS and free perhaps) ? And is it worth getting a mac with nvidia GPU (idk if the make ones with nvidia anymore)",24,18
867,2016-7-29,2016,7,29,1,4v1leo,MachineMatch identifies the author of anonymous posts. Should the code be released?,https://www.reddit.com/r/MachineLearning/comments/4v1leo/machinematch_identifies_the_author_of_anonymous/,JustThisNietzscheGuy,1469723486,,12,0
868,2016-7-29,2016,7,29,1,4v1nyc,Researchers have successfully tricked A.I. into seeing the wrong things.,https://www.reddit.com/r/MachineLearning/comments/4v1nyc/researchers_have_successfully_tricked_ai_into/,[deleted],1469724294,[deleted],0,0
869,2016-7-29,2016,7,29,2,4v1ttk,Who uses Microsoft CNTK ?,https://www.reddit.com/r/MachineLearning/comments/4v1ttk/who_uses_microsoft_cntk/,[deleted],1469726136,[deleted],0,1
870,2016-7-29,2016,7,29,2,4v1tvy,Who uses Microsoft CNTK ?,https://www.reddit.com/r/MachineLearning/comments/4v1tvy/who_uses_microsoft_cntk/,pilooch,1469726161,"I was playing with their Cognitive API service lately, and wondered whether they were using their own framework. CNTK is available from https://github.com/Microsoft/CNTK and looking at it I found out it appears pretty popular, some of us here using it ? Some report on goods and bads ? Genuinely curious here!",7,5
871,2016-7-29,2016,7,29,3,4v23de,"Getting Tensorflow, Theano and Keras on Windows  Learning Machine Learning",https://www.reddit.com/r/MachineLearning/comments/4v23de/getting_tensorflow_theano_and_keras_on_windows/,xenocide15,1469729156,,7,0
872,2016-7-29,2016,7,29,4,4v2gyy,OpenAI: Special projects,https://www.reddit.com/r/MachineLearning/comments/4v2gyy/openai_special_projects/,perceptron01,1469733604,,32,66
873,2016-7-29,2016,7,29,6,4v2zxd,"Nvidia 1080, which one?",https://www.reddit.com/r/MachineLearning/comments/4v2zxd/nvidia_1080_which_one/,[deleted],1469740099,"So I've convinced myself that it's time to upgrade my GPU for some ML fun. I've also convinced myself that I want a nvidia 1080. Again, I'm easily convinced I need these things! :)

Anyways when I hop onto newegg or other retailers I see all kinds of versions of the 1080 (different brands Gigi, Asus, etc and different versions overclocked, founders edition, blah blah blah). I'm not sure which one is better or not as good as others. Anyone here know what I should be looking for? The cards are all the same so do these 'versions' really matter? 

So conditions: Good possibility of adding a second card in the future, would like the spend the least amount of money. Not a gamer so don't need it for that. Will use it for neural nets, parallelized computing, etc.  ",2,1
874,2016-7-29,2016,7,29,7,4v3bwz,"Videos from the ""Are the skeptics right? Limits and Potentials of Deep Learning in Robotics"" Workshop at RSS",https://www.reddit.com/r/MachineLearning/comments/4v3bwz/videos_from_the_are_the_skeptics_right_limits_and/,yruz2,1469744377,,0,2
875,2016-7-29,2016,7,29,8,4v3kdf,What do you guys think of a double major in CS and Philosophy (undergrad) if my goal is to get in the cutting edge of AGI?,https://www.reddit.com/r/MachineLearning/comments/4v3kdf/what_do_you_guys_think_of_a_double_major_in_cs/,Th3_Prince,1469747627,[removed],13,0
876,2016-7-29,2016,7,29,11,4v4i6q,Python 2 or 3?,https://www.reddit.com/r/MachineLearning/comments/4v4i6q/python_2_or_3/,[deleted],1469761038,[deleted],16,1
877,2016-7-29,2016,7,29,12,4v4oby,Quora session with Yann LeCun,https://www.reddit.com/r/MachineLearning/comments/4v4oby/quora_session_with_yann_lecun/,clbam8,1469763573,,2,43
878,2016-7-29,2016,7,29,12,4v4pa2,What motivational factors drive you to take a specific job?,https://www.reddit.com/r/MachineLearning/comments/4v4pa2/what_motivational_factors_drive_you_to_take_a/,Pieranha,1469763996,"I'm in a crossroads in my career and it would be very interesting to understand what are the main factors that fellow ML researchers/practitioners think about when choosing what job to take.

I'm more interested in ""soft motivational factors"" rather than salary. Thanks!",2,0
879,2016-7-29,2016,7,29,14,4v556m,7-Layer Deep Fully Connected Network not giving sufficient accuracy (regression),https://www.reddit.com/r/MachineLearning/comments/4v556m/7layer_deep_fully_connected_network_not_giving/,metakone,1469771336,"I am trying to fit some numerical data with a 7-layer deep (each 128 wide) neural network but am achieving an accuracy of only 0.15 MSE with 10M data points. 
The activation function is Relu and I'm using weight decay. 
Any suggestions on how to improve the performance ? 
I've already tried widening and deepening the network and seen no improvement. ",27,0
880,2016-7-29,2016,7,29,15,4v58b2,Google Brain will be doing an AMA in /r/MachineLearning on August 11,https://www.reddit.com/r/MachineLearning/comments/4v58b2/google_brain_will_be_doing_an_ama_in/,olaf_nij,1469772833,"Happy to announce the [Google Brain](https://research.google.com/teams/brain/) team will be making a visit to /r/MachineLearning to do an AMA on August 11.

A thread will be created before the official AMA time for those who won't be able to attend on that day.",66,767
881,2016-7-29,2016,7,29,16,4v5g6x,Convnet benchmarks with new Pascal Titan X,https://www.reddit.com/r/MachineLearning/comments/4v5g6x/convnet_benchmarks_with_new_pascal_titan_x/,downtownslim,1469776966,,5,11
882,2016-7-29,2016,7,29,16,4v5ilx,The best explanation of Convolutional Neural Networks on the Internet!,https://www.reddit.com/r/MachineLearning/comments/4v5ilx/the_best_explanation_of_convolutional_neural/,harshpokharna,1469778282,,7,0
883,2016-7-29,2016,7,29,18,4v5rj9,Metacademy - Bayesian machine learning,https://www.reddit.com/r/MachineLearning/comments/4v5rj9/metacademy_bayesian_machine_learning/,udayj,1469783884,,1,5
884,2016-7-29,2016,7,29,18,4v5vch,"New Nvidia card: P6000, 24GB memory - for deep learning?",https://www.reddit.com/r/MachineLearning/comments/4v5vch/new_nvidia_card_p6000_24gb_memory_for_deep/,alrojo,1469786180,,24,14
885,2016-7-29,2016,7,29,20,4v64gq,[1607.08584] Connectionist Temporal Modeling for Weakly Supervised Action Labeling,https://www.reddit.com/r/MachineLearning/comments/4v64gq/160708584_connectionist_temporal_modeling_for/,bdamos,1469791640,,0,7
886,2016-7-29,2016,7,29,20,4v6546,Learning models without target data,https://www.reddit.com/r/MachineLearning/comments/4v6546/learning_models_without_target_data/,kazooki117,1469791996,"Hello,

Does anyone know of any papers detailing strategies to learn a model that can classify data from one domain using only data from a different domain for training?

For example, suppose I have a bunch of data from two different domains (Da and Db), and I want to train a model using data from those two domains that can then turn around and classify data from a third domain (Dc). Some of the features are shared between Da and Db, and some are not. Some of the features from Dc appear in Da and Db, but there are also some that do not. However, the labels in each domain are the same. So, I want to classify each instance as either class X or class Y, and I have labeled instances of class X and Y for Da and Db, and each instance I get from Dc will also either be of class X or class Y.

With the only information on Dc being which features are used and the classes to classify, is it even possible to determine which features specific to Dc are useful for classification, using the data and features from Da and Db? I know there are transfer learning techniques for cases where you have access to training data from Dc, labeled or no, but are there any techniques that can perform classification without any data from Dc? Intuitively I suspect not, because there is no way to know anything about the features specific to Dc without any training data from Dc. With some training data from Dc (labeled or not), it seems like current transfer learning techniques would be able to construct a mapping from features specific to Dc to features from Da or Db.

If it is impossible as I suspect, are there ways to incrementally learn the mapping as each instance is introduced from Dc?

Thanks!",4,0
887,2016-7-29,2016,7,29,20,4v65q6,ML data processing pipeline,https://www.reddit.com/r/MachineLearning/comments/4v65q6/ml_data_processing_pipeline/,michal_sustr,1469792313,"What are your favorite tools for performing repeated tasks such as data cleaning, data splitting to train/test/valid, preprocessing (filtering/converting to different encoding/etc.). Basically the annoying part that you always have to do when you get some new data? Do you know about some handy framework/pipeline, ideally that would 
1) distribute these operations
2) would be storage efficient (doesn't necessarily replicate the data too much in between the processing stages)

I'm doing a research about available tools, because I think this kind of plumbering takes a lot of time and I want to automate it as much as possible. Something along the lines of http://blog.kaggle.com/2016/07/21/approaching-almost-any-machine-learning-problem-abhishek-thakur/

Thanks!",15,5
888,2016-7-29,2016,7,29,21,4v6dds,Instance Normalization: The Missing Ingredient for Fast Stylization,https://www.reddit.com/r/MachineLearning/comments/4v6dds/instance_normalization_the_missing_ingredient_for/,MaxTalanov,1469796088,,1,14
889,2016-7-29,2016,7,29,21,4v6evc,Keras resources: directory of tutorials and projects using Keras,https://www.reddit.com/r/MachineLearning/comments/4v6evc/keras_resources_directory_of_tutorials_and/,MaxTalanov,1469796693,,0,31
890,2016-7-29,2016,7,29,21,4v6f6s,What are some good resources for modeling time series using deep learning?,https://www.reddit.com/r/MachineLearning/comments/4v6f6s/what_are_some_good_resources_for_modeling_time/,MasterEpictetus,1469796825,I have a multi-variate time series and I'd like to predict what happens with the series in the next day based on past information. What are some good references on using deep learning for this kind of problem? ,7,0
891,2016-7-29,2016,7,29,22,4v6lws,Speech and Language Processing (3rd ed. draft) by D. Jurafsky &amp; J. H. Martin,https://www.reddit.com/r/MachineLearning/comments/4v6lws/speech_and_language_processing_3rd_ed_draft_by_d/,pmigdal,1469799588,,4,35
892,2016-7-29,2016,7,29,23,4v6ppv,Best Artificial Intelligence and Deep Learning resources,https://www.reddit.com/r/MachineLearning/comments/4v6ppv/best_artificial_intelligence_and_deep_learning/,[deleted],1469801005,[deleted],0,2
893,2016-7-29,2016,7,29,23,4v6vib,How important is graduate school name during masters?,https://www.reddit.com/r/MachineLearning/comments/4v6vib/how_important_is_graduate_school_name_during/,[deleted],1469803062,[removed],3,2
894,2016-7-29,2016,7,29,23,4v6x2b,Best resources for getting started with AI and Deep Learning,https://www.reddit.com/r/MachineLearning/comments/4v6x2b/best_resources_for_getting_started_with_ai_and/,lumenwrites,1469803565,,0,0
895,2016-7-30,2016,7,30,0,4v74tk,Is this presidential campaign more negative than years past? Yes - Using Google Cloud Natural Language API,https://www.reddit.com/r/MachineLearning/comments/4v74tk/is_this_presidential_campaign_more_negative_than/,[deleted],1469806153,[deleted],2,0
896,2016-7-30,2016,7,30,2,4v7vb6,Which Professors/Universities are working jointly on biologically inspired methods (Computational Neuroscience/ Evolutionary Algos) and Deep Learning?,https://www.reddit.com/r/MachineLearning/comments/4v7vb6/which_professorsuniversities_are_working_jointly/,yashchandak,1469814704,"I want to work on the extending Deep Learning models using inspirations from biological methods, specifically which have capacity to keep growing, i.e transfer learning with evolution. I find Computational Neuroscience also pretty interesting as it tries to understand brain and build approximate algorithmic models for it.  Papers from deep mind (DPPN,  Progressive Nets, etc.) are inspirational, but I am looking out for Graduate schools with Professors working in this domain. ",0,0
897,2016-7-30,2016,7,30,3,4v7yxv,Question: Domain Adaptation using Synthetic data,https://www.reddit.com/r/MachineLearning/comments/4v7yxv/question_domain_adaptation_using_synthetic_data/,ginsunuva,1469815839,"Let's say for training a CNN image classifier I have a good amount of real image data for the subject I'm working with -- enough to train on its own and achieve very high (95-100%) accuracies.

Now I also have a larger (and potentially infinite, as it is procedurally generated) synthetic dataset that is very close to, but still noticeably different than the real images.

Is there any potential in improving accuracy on the real images using the synthetic ones? I've tried a few basic transfer-learning / domain-adaptation techniques, and all seem to simply find a compromise between the two domains, meaning the test accuracy on the real images is always less than or equal to the original classifier. 

Now I haven't tried some fancier stuff (pair-loss, etc) but I'm wondering, if the amount of data already there for the real images is above a decent amount, is there even a chance for synthetic to help?  
It seems transfer-learning is only helpful when the amount of real data is too little to train on its own. ",5,3
898,2016-7-30,2016,7,30,3,4v81qc,Strategies for when all of your new data are out of sample?,https://www.reddit.com/r/MachineLearning/comments/4v81qc/strategies_for_when_all_of_your_new_data_are_out/,ibarea_redux,1469816765,[removed],0,1
899,2016-7-30,2016,7,30,3,4v83gd,Microsoft is using Minecraft to explore new ways for people to collaborate with AI Robotics/machine learning,https://www.reddit.com/r/MachineLearning/comments/4v83gd/microsoft_is_using_minecraft_to_explore_new_ways/,reinforcementLearn,1469817317,,0,1
900,2016-7-30,2016,7,30,4,4v8dsd,An Open Letter On Ontologies and the New A.I.,https://www.reddit.com/r/MachineLearning/comments/4v8dsd/an_open_letter_on_ontologies_and_the_new_ai/,electric_mouse,1469820685,,14,0
901,2016-7-30,2016,7,30,5,4v8ppl,Judgment Day: Google is Making A 'Kill-Switch' for AI,https://www.reddit.com/r/MachineLearning/comments/4v8ppl/judgment_day_google_is_making_a_killswitch_for_ai/,smart999,1469824729,,0,1
902,2016-7-30,2016,7,30,5,4v8qp4,Using Inverse reinforcement learning to teach expert behaviors to an artificially intelligent agent.,https://www.reddit.com/r/MachineLearning/comments/4v8qp4/using_inverse_reinforcement_learning_to_teach/,rishabhJangir,1469825086,,12,12
903,2016-7-30,2016,7,30,8,4v9g9g,Training Faster-RCNN on multiple GPUs and/or multiple machines.,https://www.reddit.com/r/MachineLearning/comments/4v9g9g/training_fasterrcnn_on_multiple_gpus_andor/,piiswrong,1469834727,,0,4
904,2016-7-30,2016,7,30,9,4v9ln1,Datasets for vanilla fully-connected networks,https://www.reddit.com/r/MachineLearning/comments/4v9ln1/datasets_for_vanilla_fullyconnected_networks/,enematurret,1469836985,"Are there any respectable (MNIST/CIFAR-sized, not trivial) datasets that are well suited to benchmark fully-connected networks? By this I mean nothing related to images/videos (so that CNNs aren't the best pick) nor sequences (NLP, etc).

I've had a few good ideas that I'd like to test on deep fully-connected nets since they're not easily expandable to conv or recurrent nets. I know I can just use MNIST/CIFAR but it doesn't seem like publishing ~60% results on CIFAR-10 would be interesting.",3,3
905,2016-7-30,2016,7,30,9,4v9nxm,Training a simple toy model for computing parity using LSTM,https://www.reddit.com/r/MachineLearning/comments/4v9nxm/training_a_simple_toy_model_for_computing_parity/,hassanzadeh,1469837968,"Hi all,
I wonder if learning a sequence to one model with LSTM to find the parity of a given sequence is considered to hard to learn with LSTM? In other words, if I take the LSTM output from the last step (time) and feed it into a Linear layer and then softmax, should it work?
I used the torch-rnn code (https://github.com/jcjohnson/torch-rnn/blob/master/LSTM.lua) and wrote a simple code to do the job, not matter what learning rates or parameter setting I choose, the acc is almost 50%. Am I doing something wrong?
",7,4
906,2016-7-30,2016,7,30,10,4va18n,How about reactor vessels in chemical plant,https://www.reddit.com/r/MachineLearning/comments/4va18n/how_about_reactor_vessels_in_chemical_plant/,mixmachinery,1469843726,,0,1
907,2016-7-30,2016,7,30,15,4vax7n,How to identify and work on different data science problems that matter?,https://www.reddit.com/r/MachineLearning/comments/4vax7n/how_to_identify_and_work_on_different_data/,gmo517,1469858998,"I'm facing bit of a dilemma. I've been working with deep learning models for about a year now and I'm learning a lot through the fantastic research community and free resources online. But I find myself replicating other people's work, which is a great way to learn. But now, I want to work on something new. I'm specifically interested in using deep learning in the area of health and biotechnology but how do I even get started here? How can I identify problems that are worth working on and how do I get started?",4,1
908,2016-7-30,2016,7,30,15,4vaz7a,predict the probability of buying a product,https://www.reddit.com/r/MachineLearning/comments/4vaz7a/predict_the_probability_of_buying_a_product/,pradeepl,1469860216,[removed],0,1
909,2016-7-30,2016,7,30,16,4vb5il,Meridium Announces Availability of APM V4.2 with Machine Learning Capabilities,https://www.reddit.com/r/MachineLearning/comments/4vb5il/meridium_announces_availability_of_apm_v42_with/,upshawelbert,1469864227,,0,0
910,2016-7-30,2016,7,30,17,4vb9qu,Compensation for ML employees within industry,https://www.reddit.com/r/MachineLearning/comments/4vb9qu/compensation_for_ml_employees_within_industry/,Pieranha,1469867038,"It seems to be very difficult to gather information on how ML practitioners are being compensated within industry/startups.

What do you see as a typical compensation for:

a) Someone that has just finished their undergrad within CS/Math/Stats from a top 10 university. He or she has had some (albeit limited) exposure to ML.

b) Someone that has just finished their graduate degree within CS/Math/Stats from a top 10 university.  The person has had significant exposure to ML, although he or she may have had a very narrow focus on a specific subproblem.

c) A ML practitioner that has a relevant undergrad degree from a few years back and have worked with ML since then. This person likely has a more broad idea of how to apply ML in practice, but less understanding of specific theories etc.

I'm particularly interested in the compensation level on the East Coast of US, but it would also be interesting to hear from elsewhere. Perhaps there are some common ratios in terms of the difference in pay.",42,39
911,2016-7-30,2016,7,30,21,4vbtg2,[1607.07086] An Actor-Critic Algorithm for Sequence Prediction,https://www.reddit.com/r/MachineLearning/comments/4vbtg2/160707086_an_actorcritic_algorithm_for_sequence/,SuperFX,1469880288,,4,21
912,2016-7-30,2016,7,30,21,4vbwav,Question Independent Grading using Machine Learning: The Case of Computer Program Grading,https://www.reddit.com/r/MachineLearning/comments/4vbwav/question_independent_grading_using_machine/,Shenanigan5,1469882066,,1,2
913,2016-7-31,2016,7,31,0,4vcfvh,Learn to code dynamic RNN from Raw equations with Tensorflow!,https://www.reddit.com/r/MachineLearning/comments/4vcfvh/learn_to_code_dynamic_rnn_from_raw_equations_with/,kazi_shezan,1469891597,,0,0
914,2016-7-31,2016,7,31,0,4vcm6h,Question about learning ml/deep learning,https://www.reddit.com/r/MachineLearning/comments/4vcm6h/question_about_learning_mldeep_learning/,Joshua_Dunigan,1469894086,[removed],1,2
915,2016-7-31,2016,7,31,1,4vcu5d,How well do hyperparameters generalize to more complex problems?,https://www.reddit.com/r/MachineLearning/comments/4vcu5d/how_well_do_hyperparameters_generalize_to_more/,thai_tong,1469897104,"Michael Nielsons ebook on deep learning suggests that to find good hyperparameters it is better to start with a simple problem and find good hyperparameters to solve that before attempting to optimize the hyperparameters for the full problem.

For example if I was attempting MNIST then I might begin by using all the data for the images with numbers 0-4 on them, I would find a good learning rate, batch size, and the number of training epochs. Once Ive found good hyperparameters then I can use these as a basis for the full MNIST problem.

Does this approach work well for most parameters? I imagine that the optimal learning rate and batch size for the MNIST 0-4 problem would perform well for the MNIST 0-9 problem; however, since the size of the output layer has doubled the size of the other layers (or the number of other layers) probably need to be larger.

So which hyperparameters do generalize better to more complex problems? Im considering the number of training epochs, batch size, learning rate, number of hidden layers, layer sizes, and regularization parameters. Also are there any rules of thumb such as when doubling the size of the output layer should the hidden layers double in size (or increase by a factor of sqrt(2))?",10,9
916,2016-7-31,2016,7,31,2,4vd03i,timeseries classification using LSTM recurrent nets,https://www.reddit.com/r/MachineLearning/comments/4vd03i/timeseries_classification_using_lstm_recurrent/,xjackx,1469899325,[removed],0,2
917,2016-7-31,2016,7,31,2,4vd0vo,(Where to start) How to cluster sentences?,https://www.reddit.com/r/MachineLearning/comments/4vd0vo/where_to_start_how_to_cluster_sentences/,kimchi7daysaweek,1469899622,[removed],0,0
918,2016-7-31,2016,7,31,3,4vda9i,Neuromorphic Question,https://www.reddit.com/r/MachineLearning/comments/4vda9i/neuromorphic_question/,[deleted],1469903056,[removed],0,1
919,2016-7-31,2016,7,31,5,4vdsg4,Predicting the winner of a League of Legends match at the 10-minute mark,https://www.reddit.com/r/MachineLearning/comments/4vdsg4/predicting_the_winner_of_a_league_of_legends/,xMatias_LAS,1469909955,"Hello all!
I just finished my Machine Learning Engineer Nanodegree at Udacity and wanted to share the results of the last project.

This project is aimed to predict the winner of a match with the information available at the 10-minute mark and determine which behaviour is the most relevant to kill the enemy nexus. Matches were collected from this years patchs before the mid season changes to Dragons and Mages (From patch 6.2 to 6.6), so objectives like the elemental dragons and rift herald changes are not included here.

This problem was solved through supervised learning by collecting match data and modelling it as a classification problem where the label variable is going to be defined as Blue win and Red win. Riot Games has a public API where users can get match data like kills, assists, creep score, towers killed and wards placed, among other, which is going to be used to create the dataset. The scope of the analysis in this project is related entirely in team objectives (like Towers, Dragons, etc.) so no information regarding specific champion picks (early game vs late game, assassins vs control mages) is considered.

 TL &amp; DR Version 

- Predicting the winner of a match by telling which team has the highest amount of gold is a pretty good predictor. This approach has a 70.14% accuracy rate over the whole sample. When the gold lead is smaller, this approach has a smaller accuracy rate (around 50%)

- The Machine Learning approach was just slightly better than the Gold approach, getting an accuracy rate of 70.61%. Two important improvements are found; accuracy is improved at small gold leads and this approach can show of the most important behaviours rather than only gold.

- The most important variables in order of importance are Experience, Creep Score, Kills, Dragons and Jungle Creeps. Experience and Dragons are the only objectives that are not directly related to gold but they clearly show a sign of in game advantage.

 Detailed Version  

I also have a Git Hub repo were all the scripts are available. There are four kind of scripts here; one is in charge to find ranked matches and saving the detailed information into a .json file. The second script reads through all this files with the goal of creating a dataset for Machine Learning into a cvs file (mine is provided in the repo). The third script is in charge of providing visualisations of the dataset in order to make sense of the data. The last script, were the magic happens, is in charge of performing Machine Learning. All scripts are found here: https://github.com/MatiasSanchezCabrera/LOL-Prediction

You can find a detailed report with the process and my findings here: https://www.dropbox.com/s/hb9t1nfc0uhb9ge/Predicting%20the%20Winner%20of%20a%20League%20of%20Legends%20Match.pdf?dl=0

You can also check the discussion generate in the /r/LeagueofLegends subreddit here: https://www.reddit.com/r/leagueoflegends/comments/4v1hln/predicting_the_winner_of_a_match_at_the_10minute/

Ill be happy to gather feedback and answer any question. As soon as I have time I will answer them.",39,135
920,2016-7-31,2016,7,31,11,4vf6ka,Caffe installation via Arch linux / Manjaro?,https://www.reddit.com/r/MachineLearning/comments/4vf6ka/caffe_installation_via_arch_linux_manjaro/,smalldata99,1469930901,[removed],2,0
921,2016-7-31,2016,7,31,11,4vf7va,Shouldn't test accuracy be lower than train accuracy typically?,https://www.reddit.com/r/MachineLearning/comments/4vf7va/shouldnt_test_accuracy_be_lower_than_train/,jalligator,1469931491,"Recently a 3d convolution-based models I've been training have had the test (or val, seems like most people use test when it's actually validating the model) accuracy materially above the train accuracy.  I would have guessed that test accuracy should trail train accuracy basically every time.

I have checked that my test data is not in my training data, which, if it was, should have caused them to be more similar.

Have I just gotten lucky?  ",8,4
922,2016-7-31,2016,7,31,11,4vfbwh,Introduction to Stats and Basics of Maths for Data Science - The Hacker's Way,https://www.reddit.com/r/MachineLearning/comments/4vfbwh/introduction_to_stats_and_basics_of_maths_for/,rousemaga,1469933386,,0,1
923,2016-7-31,2016,7,31,11,4vfbzs,What is a free alternative to NIST Machine-Print Database of Gray Scale and Binary Images (MPDB)?,https://www.reddit.com/r/MachineLearning/comments/4vfbzs/what_is_a_free_alternative_to_nist_machineprint/,niankaki,1469933422,"Hi, newbie here.  
I want to make a Optical Character Reader software using a neural network and have learned the ropes from [this awesome website](http://neuralnetworksanddeeplearning.com/chap1.html). The website deals with digit recognition, and I want to now move on to printed character recognition.  
For that, I need training sets. I found one [here](http://www.nist.gov/srd/nistsd8.cfm). It's the NIST Machine-Print Database of Gray Scale and Binary Images (MPDB). However, it costs 90 bucks.  
Are there any free alternatives out there?  
If not, how can I make my own training and test sets? ",5,1
924,2016-7-31,2016,7,31,15,4vg1p5,Looking for a big standard non-spatial information dataset.,https://www.reddit.com/r/MachineLearning/comments/4vg1p5/looking_for_a_big_standard_nonspatial_information/,[deleted],1469947749,[deleted],0,0
925,2016-7-31,2016,7,31,15,4vg202,Visualize the steps involved in dealing with a ML problem.,https://www.reddit.com/r/MachineLearning/comments/4vg202/visualize_the_steps_involved_in_dealing_with_a_ml/,ayush0016,1469947965,,3,48
926,2016-7-31,2016,7,31,18,4vgh5m,Winograd Schema Challenge: Can computers reason like humans?,https://www.reddit.com/r/MachineLearning/comments/4vgh5m/winograd_schema_challenge_can_computers_reason/,bbsome,1469958625,,5,4
927,2016-7-31,2016,7,31,20,4vgpjt,"[20+ Videos ]Why Do We Need Machine Learning? , Types of Machine Learning and Different Models of Neurons for ML",https://www.reddit.com/r/MachineLearning/comments/4vgpjt/20_videos_why_do_we_need_machine_learning_types/,dynamicstm,1469964355,,1,0
928,2016-7-31,2016,7,31,21,4vgvxd,Looking for RNN dataset,https://www.reddit.com/r/MachineLearning/comments/4vgvxd/looking_for_rnn_dataset/,dzyl,1469968260,[removed],6,1
929,2016-7-31,2016,7,31,22,4vh4am,Machine Learning and the Natural Sciences - DZone Big Data,https://www.reddit.com/r/MachineLearning/comments/4vh4am/machine_learning_and_the_natural_sciences_dzone/,wamplerwinston,1469972602,,0,3
930,2016-7-31,2016,7,31,23,4vhdup,Implementation of Neural Network in Python using numpy [for beginners],https://www.reddit.com/r/MachineLearning/comments/4vhdup/implementation_of_neural_network_in_python_using/,iamkeyur,1469976877,,1,2
