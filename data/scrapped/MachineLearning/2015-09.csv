,date,year,month,day,hour,id,title,full_link,author,created,selftext,num_comments,score
0,2015-9-1,2015,9,1,10,3j5umu,Cool 'I support vector machines' T shirts and accessories ! :),https://www.reddit.com/r/MachineLearning/comments/3j5umu/cool_i_support_vector_machines_t_shirts_and/,mbehl,1441071270,,0,0
1,2015-9-1,2015,9,1,17,3j70k8,"New implementation of ""Neural Algorithm of Artistic Style"" (Torch + VGG17-net)",https://www.reddit.com/r/MachineLearning/comments/3j70k8/new_implementation_of_neural_algorithm_of/,samim23,1441094827,,74,69
2,2015-9-1,2015,9,1,18,3j786k,Predicting individual user level state transition,https://www.reddit.com/r/MachineLearning/comments/3j786k/predicting_individual_user_level_state_transition/,arclite123,1441100424,"I have developed a clustering model to identify 6 homogeneous group of users for a telco provider. Now as an extension to it, i want to predict a customer's cluster for the subsequent month. What would be the best approach to go about it. Couple of ideas which i have in mind is
1) Using Markov chain at individual level
2) Build 6 multi class classification model

I would also like to identify the key drivers/factors associated with transitions, hence getting inclined towards the latter one. 

Any help on this will be appreciated. Thanks",0,2
3,2015-9-1,2015,9,1,18,3j78aw,Classification based on sequences,https://www.reddit.com/r/MachineLearning/comments/3j78aw/classification_based_on_sequences/,vamsilg,1441100519,"I have a data set of sequences(variable lengths) and each of the sequence is tagged with a target label (1,0). I made a directed graph out of these sequences with edge weights proportional to transition. 

- I need to train a classification model on these sequences to classify the target labels 

What are the temporal models that can be used?
(HMM, CRF, RNN - I saw papers on these predicting the next element in the sequence but not the target associated)

Ex:  
(1) A B C 0
(2) B C D A 1
(3) A D 1

Any inputs or relevant papers on these would help
",5,4
4,2015-9-1,2015,9,1,19,3j7b4z,Virtual Machine: StyleNet - A Neural Algorithm of Artistic Style (Torch/GPU),https://www.reddit.com/r/MachineLearning/comments/3j7b4z/virtual_machine_stylenet_a_neural_algorithm_of/,samim23,1441102573,,0,1
5,2015-9-1,2015,9,1,20,3j7jut,"artistic style stuff: (probably enough of this already, but this post has some beautiful examples)",https://www.reddit.com/r/MachineLearning/comments/3j7jut/artistic_style_stuff_probably_enough_of_this/,fhuszar,1441108317,,0,29
6,2015-9-1,2015,9,1,22,3j7uo9,What is the possibility of applying recurrent neural networks (LSTM) to word sense disambiguation problem ?,https://www.reddit.com/r/MachineLearning/comments/3j7uo9/what_is_the_possibility_of_applying_recurrent/,ha2emnomer,1441113982,,7,2
7,2015-9-1,2015,9,1,22,3j7vn5,What is the possibility of applying recurrent neural networks (LSTM) to word sense disambiguation problem ?,https://www.reddit.com/r/MachineLearning/comments/3j7vn5/what_is_the_possibility_of_applying_recurrent/,ha2emnomer,1441114730,,2,8
8,2015-9-1,2015,9,1,23,3j80o1,k-means++ Silhouettes,https://www.reddit.com/r/MachineLearning/comments/3j80o1/kmeans_silhouettes/,seabass,1441117489,,0,10
9,2015-9-2,2015,9,2,1,3j8fev,Comparing Artificial Artists,https://www.reddit.com/r/MachineLearning/comments/3j8fev/comparing_artificial_artists/,x2342,1441123497,,7,67
10,2015-9-2,2015,9,2,1,3j8gj9,What limits have you experienced training models on 1 GPU?,https://www.reddit.com/r/MachineLearning/comments/3j8gj9/what_limits_have_you_experienced_training_models/,[deleted],1441123953,"I'm training a Keras stacked Bi-directional LSTM encoder/decoder on GPU. The model has a total of four layers including input -&gt; hidden -&gt; output. My inputs are 50d vectors and outputs are 20d vectors, and my sequences are truncated at length 60.

I have found I can't really push above 512 for the hidden layer size without getting a memory error. I'm running on a GeForce GT 750M.

Has anyone met similar limits training recurrent networks on a single GPU? Do you have any strategy tips to share?

",12,12
11,2015-9-2,2015,9,2,4,3j9bg2,You need seex? I am Angelina I need seex! Help!,https://www.reddit.com/r/MachineLearning/comments/3j9bg2/you_need_seex_i_am_angelina_i_need_seex_help/,branef6420,1441135973,,0,1
12,2015-9-2,2015,9,2,6,3j9sdj,Experiment Log: matching GRU performance with simple RNNs,https://www.reddit.com/r/MachineLearning/comments/3j9sdj/experiment_log_matching_gru_performance_with/,bluecoffee,1441142410,,24,11
13,2015-9-2,2015,9,2,6,3j9uw0,Neural Network to learn to play Video Games [x-post from /r/mlquestions],https://www.reddit.com/r/MachineLearning/comments/3j9uw0/neural_network_to_learn_to_play_video_games_xpost/,jesmaail,1441143416,"Is it possible to use a Deep Belief Network to learn and play a video game (e.g Tetric, Pacman, Snake)?

I've read about Deep Reinforcement Learning being used for it, but I'm unsure how far this is from a DBN.

[I originally posted this on r/mlquestions but had very little response.]",4,4
14,2015-9-2,2015,9,2,6,3j9y15,Understanding ROC graphs and F1 score,https://www.reddit.com/r/MachineLearning/comments/3j9y15/understanding_roc_graphs_and_f1_score/,Poydflink,1441144690,"I want to confirm my understanding of these two graphs and their seemingly contradictory F1 results. In [Example 1](http://i.imgur.com/6g29tHN.png) I have the linked ROC graph with an AUC of 0.67, but the F1 score is of 0.76 . In [Example 2](http://i.imgur.com/A5ginjK.png) I have a AUC of 0.79 but an F1 score of 0.69 . I guess this happens because the two scores are not completely related: one is related to how much true positives and false positives vary with a certain parameter, while the other is a formula taking into account not only the true and false positives but also that the certain parameter is fixed. Is my interpretation correct? Thank you",4,3
15,2015-9-2,2015,9,2,7,3ja3b3,Useful video tutorial explaining the richness of Maximum Entropy Methods in data modelling,https://www.reddit.com/r/MachineLearning/comments/3ja3b3/useful_video_tutorial_explaining_the_richness_of/,napsternxg,1441146888,,1,17
16,2015-9-2,2015,9,2,7,3ja454,Mocha.jl: Deep Learning for Julia,https://www.reddit.com/r/MachineLearning/comments/3ja454/mochajl_deep_learning_for_julia/,harrism,1441147330,,7,36
17,2015-9-2,2015,9,2,8,3ja8um,Where can I find applications of machine learning that benefit users right now?,https://www.reddit.com/r/MachineLearning/comments/3ja8um/where_can_i_find_applications_of_machine_learning/,Hans-German,1441149342,"Hey together,
I am relatively new to the field of machine learning and not a coder. My question is where can I find examples of machine learning that benefit me directly in the real world? The hype about machine learning and AI makes me want to test and experience it, do you know any companies/start-ups that have launched their AI/machine learning product/service that I can experience first hand?

I am really excited about this topic since I had the pleasure to teach a little robot the game of black jack last week in university. The experience was totally mind blowing. Also I am interning at a start-up that uses some kind of machine learning/AI and we are having a small survey about users opinion on AI and online shopping, if you like you can give us your opinion [here.](https://shopcos.typeform.com/to/JHcErp)",1,2
18,2015-9-2,2015,9,2,9,3jafzh,How do you deal with continuous inputs/outputs in neural networks?,https://www.reddit.com/r/MachineLearning/comments/3jafzh/how_do_you_deal_with_continuous_inputsoutputs_in/,[deleted],1441152996,[deleted],14,7
19,2015-9-2,2015,9,2,12,3jb587,[NIPS 2015 Workshop] Black Box Learning and Inference,https://www.reddit.com/r/MachineLearning/comments/3jb587/nips_2015_workshop_black_box_learning_and/,mrkulk,1441164365,,0,3
20,2015-9-2,2015,9,2,12,3jb8hi,Album Review Sentiment Analysis (using pre-trained API) indicates grading curve,https://www.reddit.com/r/MachineLearning/comments/3jb8hi/album_review_sentiment_analysis_using_pretrained/,h20lac,1441166025,,0,2
21,2015-9-2,2015,9,2,13,3jbb9c,Signal processing/anomaly detection to find important spikes in social data,https://www.reddit.com/r/MachineLearning/comments/3jbb9c/signal_processinganomaly_detection_to_find/,rjbullet,1441167330,,0,2
22,2015-9-2,2015,9,2,13,3jbek3,Mathematics for Machine Learning,https://www.reddit.com/r/MachineLearning/comments/3jbek3/mathematics_for_machine_learning/,john_philip,1441169019,,0,6
23,2015-9-2,2015,9,2,13,3jbew1,You want seex! I am Samantha I want seex! Please! Help!,https://www.reddit.com/r/MachineLearning/comments/3jbew1/you_want_seex_i_am_samantha_i_want_seex_please/,grosom4395,1441169169,,0,1
24,2015-9-2,2015,9,2,13,3jbfg3,Video: Live stock market hedge powered by machine learning algorithms,https://www.reddit.com/r/MachineLearning/comments/3jbfg3/video_live_stock_market_hedge_powered_by_machine/,john_philip,1441169439,,3,0
25,2015-9-2,2015,9,2,15,3jbnpi,List of Machine Learning Certifications and Best Data Science Bootcamps,https://www.reddit.com/r/MachineLearning/comments/3jbnpi/list_of_machine_learning_certifications_and_best/,saileshg,1441174580,,0,0
26,2015-9-2,2015,9,2,16,3jbtur,Interesting 99.3% accuracy on MNIST with no elastic distorsions and no committee,https://www.reddit.com/r/MachineLearning/comments/3jbtur/interesting_993_accuracy_on_mnist_with_no_elastic/,mioan,1441178568,,20,20
27,2015-9-2,2015,9,2,17,3jbyi2,200L BREWERY PROJECT - Beer Equipment Details - yolong Industrial,https://www.reddit.com/r/MachineLearning/comments/3jbyi2/200l_brewery_project_beer_equipment_details/,Rachel-Han,1441182175,,0,1
28,2015-9-2,2015,9,2,17,3jc1d0,What Are Essential Parts of a Standard Trabon Lube System?,https://www.reddit.com/r/MachineLearning/comments/3jc1d0/what_are_essential_parts_of_a_standard_trabon/,jackerfrinandis,1441184099,,0,1
29,2015-9-2,2015,9,2,18,3jc41k,Slow PCI bus holding back faster GPU? (980 Ti vs 970),https://www.reddit.com/r/MachineLearning/comments/3jc41k/slow_pci_bus_holding_back_faster_gpu_980_ti_vs_970/,spurious_recollectio,1441185848,"I develop on two desktops: one's from 2011 and has a i7 2700k with a gen2 PCI (5 GT/s) bus while the second is a small form factor but with a much more recent i7 4790k and a gen3 PCI (8 GT/s) bus as well as a GTX 970 card.  I decided to get a 980 Ti for my larger, older desktop so I could also use it to run my own neural network code (mostly RNNs at this point).  I expected the slower 2nd gen PCI bus (16 bit) on the older machine would prevent me from running two GPUs (so I removed the machine's older GPU) but what I found was surprisingly much worse.

When I compare performance of the GTX 970 on a PCI 3 bus (on the 4790k) with that of the 980 Ti on the PCI 2 bus (on the 2700k) I found the former to be ~2 times faster!  This is running my own RNN code which uses cudarray to access the GPU.  To make sure this wasn't just random timing variations for different sequence lengths in the RNN I tested it with an simple feed forward net on MNIST and it was even more clear - the 970 finishes an epoch in 1/2 the time it takes the 980 Ti.  

Its clear the 980 Ti is not being used to full capacity. What I see in Nvidia's GUI is (unfortunately qualitatively rather than quantitatively cause I can't get these numbers in the terminal to plot them):

- 970 usage is 20-95% averaging high 50s while 980 ti's is 20-80% averaging mid-20s.
- Surprisingly in both cases PCI-bus utilization is mostly ~1% and occasionally shoots up to 20% (which is surprising if the bus is the problem).

OTOH I checked that as expected:

- transfering a large matrix to GPU is ~2x faster on the PCI-3 bus (the new machine).
- doing large matrix dot products is ~2x faster on the 980 ti.

So the cards are behaving as expected and this problem could very well highlight flaws in my code -- clearly a lot of time is spent on the CPU and transfering data to the GPU rather than just doing GPU math.

I'm now pretty convinced to send back the 980 Ti (since I don't feel like upgrading my whole system now and I'm not convinced improving my code will fix it) but I was just posting here to see if anyone can suggest ways to determine the exact culprit of the slowdown or share their thoughts on whether the problem is resolvable at all (without splurge for a new motherboard and CPU).  Unfortunately I cannot just swap the GPUs because the 4790k's case is too small for the 980 Ti (I built it as a small form-factor PC).

I'd like to ask if anyone has advice for anything to run/check to determine the exact cause of the problem (i.e. CPU vs bus).  Also is there another trivial-to-install/run implementation of an NN that does not use cudarray so I can see how that fares?  All hardware posts generally suggest the GPU to be the most important component in running NN's but its clear here that something else is a bottleneck.  This might be an important lesson for those building a ""deep-learning workstation"" cause my old 2700k system is still a pretty reasonable workstation but its apparently clobbering the performance of a new GPU.",56,4
30,2015-9-2,2015,9,2,19,3jcb9o,BIOES instead of BIO the tagging scheme?,https://www.reddit.com/r/MachineLearning/comments/3jcb9o/bioes_instead_of_bio_the_tagging_scheme/,[deleted],1441190723,[deleted],2,0
31,2015-9-2,2015,9,2,22,3jcrif,Maxima Core technical details,https://www.reddit.com/r/MachineLearning/comments/3jcrif/maxima_core_technical_details/,superrjb,1441199969,,19,2
32,2015-9-3,2015,9,3,0,3jd5u6,"DEEP LEARNING REVOLUTION, summer 2015, state of the art &amp; topnotch links",https://www.reddit.com/r/MachineLearning/comments/3jd5u6/deep_learning_revolution_summer_2015_state_of_the/,vznvzn,1441206198,,1,0
33,2015-9-3,2015,9,3,0,3jd6qo,Machine Learning Methods - Computerphile,https://www.reddit.com/r/MachineLearning/comments/3jd6qo/machine_learning_methods_computerphile/,jackbrucesimpson,1441206572,,18,83
34,2015-9-3,2015,9,3,1,3jdlwo,Find your Dream Job! Recurrent Neural Network learns Indeed.com Job Postings,https://www.reddit.com/r/MachineLearning/comments/3jdlwo/find_your_dream_job_recurrent_neural_network/,mwakanosya,1441212737,,0,4
35,2015-9-3,2015,9,3,2,3jdrds,"Fact Extraction from Wikipedia text, a Google Summer of Code project: check out the new datasets released by DBpedia",https://www.reddit.com/r/MachineLearning/comments/3jdrds/fact_extraction_from_wikipedia_text_a_google/,hell_j,1441214814,,8,49
36,2015-9-3,2015,9,3,2,3jdsgp,How to design Lyapunov functions for obtaining Convergence rate of an Algorithm?,https://www.reddit.com/r/MachineLearning/comments/3jdsgp/how_to_design_lyapunov_functions_for_obtaining/,mr_robot_elliot,1441215253,"I was wondering how to design the lyapunov functions before knowing the convergence analysis. Is it hit and trail? If you see [SAGA paper](http://www.aarondefazio.com/adefazio-nips2014.pdf ) of NIPS 2014, the authors come up a lyapunov function in Theorem 1 of Page 7 , though the convergence analysis turns out to be good but how did they know beforehand that the analysis will work? Is there any procedure to design the functions. Thanks",4,6
37,2015-9-3,2015,9,3,3,3jdxip,Data sets for using Spiking Neural Networks for classification?,https://www.reddit.com/r/MachineLearning/comments/3jdxip/data_sets_for_using_spiking_neural_networks_for/,[deleted],1441217230,[deleted],24,2
38,2015-9-3,2015,9,3,3,3jdy43,#Spark Vs #Storm: Applying #MachineLearning in real-time #data (streams of Data) #ML #DataScience,https://www.reddit.com/r/MachineLearning/comments/3jdy43/spark_vs_storm_applying_machinelearning_in/,__learningCS,1441217448,"I am have limited time and I wanna invest in one of the both tools, Spark Vs Storm. I am working on project where I should applying machine learning on stream of data (real-time data), I implemented machine learning model. Now, I would like to build the infrastructure, which platform is better in such scenarios. 

When I said real-time, it should be text data coming from different online streams. ",9,0
39,2015-9-3,2015,9,3,3,3je4pg,RNN resources curated,https://www.reddit.com/r/MachineLearning/comments/3je4pg/rnn_resources_curated/,[deleted],1441220065,[deleted],0,1
40,2015-9-3,2015,9,3,4,3jea4d,What is the best real world analogy you can come up with to describe hyperparameters?,https://www.reddit.com/r/MachineLearning/comments/3jea4d/what_is_the_best_real_world_analogy_you_can_come/,[deleted],1441222207,[deleted],4,0
41,2015-9-3,2015,9,3,5,3jeh37,"As a rule of thumb, what size of dataset would you consider big enough to train a recurrent neural network?",https://www.reddit.com/r/MachineLearning/comments/3jeh37/as_a_rule_of_thumb_what_size_of_dataset_would_you/,[deleted],1441225181,"I have a dataset of around 320,000 instances. I realized this isn't actually that big compared to the stat machine translation datasets, which can be in the size of tens of millions.

Am I wasting my time trying to train an LSTM network with this data? Are we converging on a 'rule of thumb' for the quantity of data required to achieve success with RNNs?",9,3
42,2015-9-3,2015,9,3,5,3jei62,Semantic Segmentation vs Object Detection ?,https://www.reddit.com/r/MachineLearning/comments/3jei62/semantic_segmentation_vs_object_detection/,ganarajpr,1441225635,"Was just wondering what is the major difference between the two methods ? What would be the ideal situations to use one over the other ? Do either of the methods have any advantages over another in terms of performance and prediction accuracy ? 

Also any pointers in terms of ideal applications of either methods would be awesome. ",2,1
43,2015-9-3,2015,9,3,5,3jel8c,Meet The Man Who Gamed Reddit With A Bot,https://www.reddit.com/r/MachineLearning/comments/3jel8c/meet_the_man_who_gamed_reddit_with_a_bot/,hooked_dev,1441226885,,4,0
44,2015-9-3,2015,9,3,7,3jext0,Great Bloomberg feature on using machine learning in robots,https://www.reddit.com/r/MachineLearning/comments/3jext0/great_bloomberg_feature_on_using_machine_learning/,Buck-Nasty,1441232025,,0,5
45,2015-9-3,2015,9,3,9,3jfgc5,How to tune data on ML model,https://www.reddit.com/r/MachineLearning/comments/3jfgc5/how_to_tune_data_on_ml_model/,tim_schaaf,1441240200,"So, I did it. Step 1: got a ML model (LogLoss, scikit-learn) to run on a training set and cross validate over multiple folds with a hold out set (20%). The LogLoss validation was 0.37 (not terrible, probably not great), and then ran the end model to predict a set of values .... and the results weren't great.

What ways do you look to tune such a model? For a common well known example, we can say the Titanic Kaggle problem.",0,0
46,2015-9-3,2015,9,3,12,3jfzz3,Steps Toward Deep Kernel Methods from Infinite Neural Networks,https://www.reddit.com/r/MachineLearning/comments/3jfzz3/steps_toward_deep_kernel_methods_from_infinite/,iidealized,1441249493,,0,10
47,2015-9-3,2015,9,3,12,3jg446,question about oxford rnn implementation?,https://www.reddit.com/r/MachineLearning/comments/3jg446/question_about_oxford_rnn_implementation/,logrech,1441251723,"[Here](https://github.com/oxford-cs-ml-2015/practical6/blob/master/train.lua) is the oxford deep learning class rnn torch implementation. 

I have a question about how the training is done. 

During the training, in the forward pass, the output of the rnn is not carried over to the input of the next time step, as shown in line 87. Why is this? Are there advantages to training a net in this way? 

What confused me more is that during the backward pass, they assume that the final output IS carried over to the next time step, coded in line 114. 

So whats going on here? ",6,7
48,2015-9-3,2015,9,3,15,3jgjkw,FKR 300A tong sealer,https://www.reddit.com/r/MachineLearning/comments/3jgjkw/fkr_300a_tong_sealer/,dongfengpacking,1441262552,,3,0
49,2015-9-3,2015,9,3,16,3jgn8m,Bijur Delimon: How Lubrication is Essential For Your Machines?,https://www.reddit.com/r/MachineLearning/comments/3jgn8m/bijur_delimon_how_lubrication_is_essential_for/,jackerfrinandis,1441265207,,0,1
50,2015-9-3,2015,9,3,19,3jgyxv,"HTSL2, Evolving the Neocortex, and Human-Like Reinforcement Learning",https://www.reddit.com/r/MachineLearning/comments/3jgyxv/htsl2_evolving_the_neocortex_and_humanlike/,CireNeikual,1441274476,,2,6
51,2015-9-3,2015,9,3,20,3jh41t,IPython notebook for Neural Artistic Style using Theano + Lasagne,https://www.reddit.com/r/MachineLearning/comments/3jh41t/ipython_notebook_for_neural_artistic_style_using/,em0lson,1441278495,,11,64
52,2015-9-3,2015,9,3,20,3jh5ce,Best Quality Injection Molding Machine  Moldfabrication,https://www.reddit.com/r/MachineLearning/comments/3jh5ce/best_quality_injection_molding_machine/,Moldfabrica,1441279341,,0,0
53,2015-9-3,2015,9,3,20,3jh7yy,How to build a classification model which can classify Product Reviews into a set of labels?,https://www.reddit.com/r/MachineLearning/comments/3jh7yy/how_to_build_a_classification_model_which_can/,[deleted],1441281144,"I want to build a classifier which has the ability to classify freetext into a set of pre-selected labels. Let's say I have access to all the feedback (Product Reviews) left by consumers on a particular e-commerce website.

I want to classify all the reviews under 10 pre-determined labels. Example: Expensive, Fake, Prompt Service, Good Quality ..etc.

What is the best way to go about making this classifier? I am a complete Machine learning Modelling noob. I don't want you to do this for me, but please point me in the right direction so that I start exploring ways to do this. Also, if possible please elaborate on how to clear out junk/noise from data.
",3,1
54,2015-9-3,2015,9,3,20,3jh8ig,Check out these tips for cleaning your machinery.,https://www.reddit.com/r/MachineLearning/comments/3jh8ig/check_out_these_tips_for_cleaning_your_machinery/,easternoil48341,1441281510,,0,1
55,2015-9-3,2015,9,3,21,3jhe26,Artistic Style Network in Python (yet another),https://www.reddit.com/r/MachineLearning/comments/3jhe26/artistic_style_network_in_python_yet_another/,abll,1441284798,"Hi there!

I have played around with the new convnet craze. My code is [on Github](https://github.com/andersbll/neural_artistic_style). Some comments:

- The method works really well - it's like Instagram on speed! Even though the method does not always map the style between semantically similar objects correctly (e.g. transfer an eye style to the eyes of the subject), the results are very pleasant in surprising ways.
- The method fails if the style is too subtle. It's easier to spot poorly transferred styles when there are no crude pen strokes to cover them up.
- The original method assumes same size subject and style images. If the image sizes differ, the gradient contributions are uneven. I have tried to compensate for this in my implementation (and so far, it seems to work).
- The original method uses a normalized version of the VGG net 19 features. This version can be downloaded from their [website](http://bethgelab.org/deepneuralart/). I wasn't aware of this when I did my implementation, so I had to normalize the gradient contributions layer-wise.
- The original method performs optimization using L-BFGS. I get pretty good results using ADAM. Ordinary SGD is a bit sensitive wrt. learning rate.
- For the best results, make sure both your subject and style image dimensions are a multiple of 32. Otherwise, you might observe strange artifacts around the image borders. This is due to the size of the features in the upper layers of the convnet.
- It's pretty fast. A 800x600 images takes 1-2 minutes using CUDA. Typically, the image is 99 % done after 30 seconds.
- There is ample room for tweaking and extending the method! :)",22,19
56,2015-9-3,2015,9,3,22,3jhfsc,CNN produces flat filters. Interpretation?,https://www.reddit.com/r/MachineLearning/comments/3jhfsc/cnn_produces_flat_filters_interpretation/,leonard_o_,1441285679,"For my problem I train a convolutional neural network with 4 convolutional layers, 2 fully connected layers and ReLU nonlinearities. The network achieves excellent performance, however, some of the filters in the first two layers (filter sizes 5x5) are completely flat (constant non-zero value across the filter. they do not resemble edge detectors). Unfortunately I could not find the cause of this. Any ideas?",6,2
57,2015-9-3,2015,9,3,22,3jhk8n,How useful are Topic Models in practice?,https://www.reddit.com/r/MachineLearning/comments/3jhk8n/how_useful_are_topic_models_in_practice/,ginger_beer_m,1441288027,"Since the quality of discussion in this sub is often better, I'd be interested to know what people here think about the following question posted at researchgate: http://www.researchgate.net/post/How_useful_are_Topic_Models_in_practice

&gt; On the face of it, topic modelling, whether it is achieved using LDA, HDP, NNMF, or any other method, is very appealing. Documents are partitioned into topics, which in turn have terms associated to varying degrees. However in practice, there are some clear issues: the models are very sensitive to the input data small changes to the stemming/tokenisation algorithms can result in completely different topics; topics need to be manually categorised in order to be useful (often arbitrarily, as the topics often contain mixed content); topics are ""unstable"" in the sense that adding new documents can cause significant changes to the topic distribution (less of an issue with large corpi). 

&gt; In the light of these issues, are topic models simply a toy for NLP/ML researchers to show what can be done with data, or are they really useful in practice? Do any websites/products make use of them?",5,7
58,2015-9-3,2015,9,3,22,3jhkt3,NLTK: Tuning SVMc classifier accuracy? - Looking for better approaches/advices,https://www.reddit.com/r/MachineLearning/comments/3jhkt3/nltk_tuning_svmc_classifier_accuracy_looking_for/,Chuckytah,1441288308,"Problem/Main objective/TLDR: Train a classifier,  then feed it a random review and get the correspondent predicted review  rating (number of stars from 1 to 5) - only 60% accuracy! :(

To see some code follow SO link: http://datascience.stackexchange.com/questions/6995/nltk-tuning-svmc-classifier-accuracy-looking-for-better-approaches-advices

I have a big dataset with around 48000 tech product reviews (from  many different writers and from different products - here this is not so  important (?)) and corresponding ratings (1 to 5 stars) I randomly selected some reviews within each class:

    1 star: 173 reviews (could not pick 1000 because there were 173)
    2 stars: 1000 reviews
    3 stars: 1000 reviews
    4 stars: 1000 reviews
    5 stars: 1000 reviews

Total: 4173 reviews - this data is organized in one file  (all_reviews_labeled.txt) in tuple format, one review and rating for  line:

    (review text, x star)
    (review text, x star)
    (review text, x star)
    (review text, x star)
    

My 1st dummie aproach was:

    Tokenize review text
    POS tagging

    Get most frequent bigrams that folowing some POS tags rules for most frequent trigrams (I have seen this rules - using this POS patterns in Automatic Star-rating Generation from Text Reviews - pag.7 - paper from Chong-U Lim, Pablo Ortiz and Sang-Woo Jun).
    Extract features (here is where I have more doubts - should I only look for this two features?).
    Splits the training data into training size and testing size (90% training - 10% testing)
    Train SVMc
    Evaluate the classifier

So far I get only 60% accuracy  What can I do to improve my prediction results? Is something before,  some text/reviews preprocessing (like removing stopwords/punctuation?)  that is missing? Could you suggest me some other approaches? I am still a  bit confused if is really a classification problem or a regression  one... :/
Please simple explanations, or give me a link to machine learning  for dummies, or be my mentor, I promise to learn fast! My background in machine learning/language processing/data mining is  very light, I have played a couple of times with weka (Java), but now I  need to stick with Python (nltk + scikit-learn)! to see some code follow SO link: http://datascience.stackexchange.com/questions/6995/nltk-tuning-svmc-classifier-accuracy-looking-for-better-approaches-advices",0,4
59,2015-9-4,2015,9,4,1,3ji98g,Showing quote not consistent with author,https://www.reddit.com/r/MachineLearning/comments/3ji98g/showing_quote_not_consistent_with_author/,[deleted],1441298583,[deleted],0,1
60,2015-9-4,2015,9,4,2,3jicxr,"Neural Networks, Types, and Functional Programming",https://www.reddit.com/r/MachineLearning/comments/3jicxr/neural_networks_types_and_functional_programming/,rasbt,1441300131,,17,138
61,2015-9-4,2015,9,4,2,3jie6u,when you know how the brain works,https://www.reddit.com/r/MachineLearning/comments/3jie6u/when_you_know_how_the_brain_works/,ciolaamotore,1441300619,,1,0
62,2015-9-4,2015,9,4,3,3jikz8,The Yahoo Behind Deep Learning Approaches at Flickr,https://www.reddit.com/r/MachineLearning/comments/3jikz8/the_yahoo_behind_deep_learning_approaches_at/,[deleted],1441303356,[deleted],0,0
63,2015-9-4,2015,9,4,3,3jip1q,Roti - Chapati - Paratha - Papad Making Machine Working Demo,https://www.reddit.com/r/MachineLearning/comments/3jip1q/roti_chapati_paratha_papad_making_machine_working/,suratexim,1441304991,,1,0
64,2015-9-4,2015,9,4,3,3jirbu,Why is my LSTM behaving erratically?,https://www.reddit.com/r/MachineLearning/comments/3jirbu/why_is_my_lstm_behaving_erratically/,ma2rten,1441305933,"I implemented Sequence to Sequence learning in python using numpy.

After I verified that the gradient check passes, I ran it on a toy dataset with only 10 training examples to verify that it can learn those (overfit).

Depending on the random initialization the network behaves quite strange. Sometimes it's able to coverage just fine. Other times the error keeps going down, but then shoots up for several iterations. Sometimes it doesn't converge at all. I tried adjusting the different initializations, adjusting the learning rate and gradient clipping.

I also implemented another toy task which my implementation is able to handle well.

I am wondering if there is some kind of bug in my code, that the gradient check is unable to catch. ",13,2
65,2015-9-4,2015,9,4,3,3jit0m,Neural Animation,https://www.reddit.com/r/MachineLearning/comments/3jit0m/neural_animation/,crypto474747,1441306659,,5,21
66,2015-9-4,2015,9,4,5,3jj2zs,How Machine Learning &amp; Artificial Intelligence Affect SEO : Machine Learning &amp; Google,https://www.reddit.com/r/MachineLearning/comments/3jj2zs/how_machine_learning_artificial_intelligence/,thisisamachine,1441310825,,0,0
67,2015-9-4,2015,9,4,6,3jjbmy,"Top Machine Learning, Data Mining, &amp; Natural Language Processing Books",https://www.reddit.com/r/MachineLearning/comments/3jjbmy/top_machine_learning_data_mining_natural_language/,kjahan,1441314347,,1,0
68,2015-9-4,2015,9,4,6,3jjbra,An example machine learning notebook,https://www.reddit.com/r/MachineLearning/comments/3jjbra/an_example_machine_learning_notebook/,piterpolk,1441314393,,0,9
69,2015-9-4,2015,9,4,6,3jjbwb,Analyzing Big Data with Dynamic Quantum Clustering,https://www.reddit.com/r/MachineLearning/comments/3jjbwb/analyzing_big_data_with_dynamic_quantum_clustering/,_spreadit,1441314452,,1,3
70,2015-9-4,2015,9,4,6,3jjh4f,Learn by Implementation: K-Nearest Neighbor,https://www.reddit.com/r/MachineLearning/comments/3jjh4f/learn_by_implementation_knearest_neighbor/,vonnik,1441316646,,0,7
71,2015-9-4,2015,9,4,8,3jjvgg,Dealing with multiple variable inputs,https://www.reddit.com/r/MachineLearning/comments/3jjvgg/dealing_with_multiple_variable_inputs/,VivaLaPandaReddit,1441322949,"Hello! I am a ML novice, and am currently working on a non - machine learning project to estimate the future sales of a given product using historic data. I am using a traditional statistical /algorithmic approach but have ran into a large problem. I can access sales data and price data, as well as date, but I have no way of isolating what sales changes are due to price and what are due to date. I have some ideas, but am wondering whether a neural network might be able to help me with this. ",0,0
72,2015-9-4,2015,9,4,9,3jk0cb,Dealing with multiple variable inputs,https://www.reddit.com/r/MachineLearning/comments/3jk0cb/dealing_with_multiple_variable_inputs/,VivaLaPandaReddit,1441325267,"Hello! I am a ML novice, and am currently working on a non - machine learning project to estimate the future sales of a given product using historic data. I am using a traditional statistical /algorithmic approach but have ran into a large problem. I can access sales data and price data, as well as date, but I have no way of isolating what sales changes are due to price and what are due to date. I have some ideas, but am wondering whether a neural network based approach might be able to deal with this confounding of variables better. ",2,1
73,2015-9-4,2015,9,4,11,3jkeob,[xpost] /r/DIYHealth : Microbiome,https://www.reddit.com/r/MachineLearning/comments/3jkeob/xpost_rdiyhealth_microbiome/,Atrix621,1441332023,,0,1
74,2015-9-4,2015,9,4,12,3jkorw,K thut nh  lun em li li nhun,https://www.reddit.com/r/MachineLearning/comments/3jkorw/k_thut_nh__lun_em_li_li_nhun/,hoangvu1901,1441337065,[removed],0,1
75,2015-9-4,2015,9,4,12,3jkpmi,on Helen Keller and computer intelligence,https://www.reddit.com/r/MachineLearning/comments/3jkpmi/on_helen_keller_and_computer_intelligence/,toisanji,1441337529,,0,0
76,2015-9-4,2015,9,4,13,3jku1w,I'm taking a machine learning course this fall. I'd like to watch a whole series of lectures in advance. What's the best and most modern free online course for machine learning?,https://www.reddit.com/r/MachineLearning/comments/3jku1w/im_taking_a_machine_learning_course_this_fall_id/,[deleted],1441339993,[deleted],0,1
77,2015-9-4,2015,9,4,13,3jkwz1,What's the best and most modern free online course for machine learning?,https://www.reddit.com/r/MachineLearning/comments/3jkwz1/whats_the_best_and_most_modern_free_online_course/,Zulban,1441341693,I'm taking a machine learning course this fall. I'd like to watch a whole series of lectures in advance because I'm cool like that. Suggestions? Or is 2008 Andrew Ng the winner?,7,4
78,2015-9-4,2015,9,4,15,3jl6re,Machine Learning Trick of the Day: Gaussian Integral Trick,https://www.reddit.com/r/MachineLearning/comments/3jl6re/machine_learning_trick_of_the_day_gaussian/,iori42,1441348147,,5,56
79,2015-9-4,2015,9,4,15,3jl7tf,Mid and Lower Tier Graduate Schools,https://www.reddit.com/r/MachineLearning/comments/3jl7tf/mid_and_lower_tier_graduate_schools/,Hobit103,1441348948,"Hello,

I am a prospective grad student applying to Phd programs this fall. I've easily figured out which schools are the best within ML, but am having a bit of a harder time figuring out which schools fall into the middle and lower tiers. I have mostly been using [phds.org](http://phds.org) so far. If you have any suggestions for schools, or any places that I can look at that are more tailored to ML than [phds.org](http://phds.org) is, that would be a great help.

Along those same lines, if you have any advice to applying to Phd programs, I'd be much appreciative to hear it.",7,8
80,2015-9-4,2015,9,4,17,3jlho3,AdaBoost: Why does test error decrease even after training error hits zero?,https://www.reddit.com/r/MachineLearning/comments/3jlho3/adaboost_why_does_test_error_decrease_even_after/,TURBO_HULK,1441356994,Can anyone enlighten me? I don't get it. Thanks in advance.,5,3
81,2015-9-4,2015,9,4,18,3jligo,"Top Machine Learning, Data Mining, &amp; NLP Books",https://www.reddit.com/r/MachineLearning/comments/3jligo/top_machine_learning_data_mining_nlp_books/,john_philip,1441357639,,0,2
82,2015-9-4,2015,9,4,18,3jlj9i,Topmodel: Standard evaluations for binary classifiers so you don't have to (by Stripe),https://www.reddit.com/r/MachineLearning/comments/3jlj9i/topmodel_standard_evaluations_for_binary/,alexcasalboni,1441358284,,0,5
83,2015-9-4,2015,9,4,19,3jlrj2,Machine learning for Recognizing Car Maneuvers,https://www.reddit.com/r/MachineLearning/comments/3jlrj2/machine_learning_for_recognizing_car_maneuvers/,Kaptajn_Snaps,1441364397,"I am trying to figure out a good ML model/method for recognizing car maneuvers(e.g. lane changes) from sensor data. Some quick searching showed people suggesting Recurrent Neural Networks(and other NNs), Hidden Markov Models, Conditional Random Field, Support Vector Machines and Bayesian Networks. Unfortunately i only have a vague understanding any of them for my problem. Any suggestions for which is best to start learning in depth and use? or other general suggestions?",4,9
84,2015-9-4,2015,9,4,20,3jlvd0,[1509.00519] Importance Weighted Autoencoders,https://www.reddit.com/r/MachineLearning/comments/3jlvd0/150900519_importance_weighted_autoencoders/,sidsig,1441366770,,0,11
85,2015-9-4,2015,9,4,21,3jlynh,denoising auto-encoders question,https://www.reddit.com/r/MachineLearning/comments/3jlynh/denoising_autoencoders_question/,bhmoz,1441368659,"I refer to the paper Extracting and Composing Robust Features with
Denoising Autoencoders, Vincent, Larochelle, Bengio, Manzagol (2008) Technical Report 1316 (more detailed than the ICML paper)

I am looking for information about the justification of denoising auto-encoders. I think I could understand the calculations which are not too complicated, but I don't have the intuition behind the proofs and probabilistic modelling in general (I only have basic information theory and bayesian statistics knowledge). I don't understand the proofs in 4.3 (bottom-up filtering, information theoretic perspective) and 4.4 (top-down, generative model perspective).

In particular:

- question about notations: I thought that in general p is the true generating process while q is an approximation (example: variational methods). Isn't it? q is the ""true generating process"". What is p? what is q^0?

As underlined in 4.4, ""Keep in mind that whereas we had the dependency structure X -&gt; X^tilde -&gt; Y for q or q0, we have Y -&gt; X -&gt; X^tilde for p."". 

- Why do they introduce two models? Is it that one is for modelling the encoder and the other one the decoder?

- Any good introductions/pages/courses?

Finally, a more general question: I think I lack practice and I would like to find a book with good examples, complete maths, exercices and importantly, corrections. I hesitate between:

- Bishop - pattern recognition and machine learning

- MacKay - Information theory, inference and learning algorithms

- Hastie, Tibshirani, Friedman - the elements of statistical learning

- Murphy - Machine Learning: A Probabilistic Perspective

The goal is to be able to understand proofs such as this one, and also to be able to understand different families of Bayes nets, how to design them, etc. Particularly interested also in the parallels between IT and Bayesian modelling (work of Hinton, Valpola, ...)
What do you think would suit me?

thanks",2,2
86,2015-9-4,2015,9,4,22,3jm4jg,Promising new open source toolkit for end-to-end learning in speech recognition (with RNNs),https://www.reddit.com/r/MachineLearning/comments/3jm4jg/promising_new_open_source_toolkit_for_endtoend/,quirm,1441371623,,2,27
87,2015-9-4,2015,9,4,22,3jmb8z,Knowm claims breakthrough in memristors,https://www.reddit.com/r/MachineLearning/comments/3jmb8z/knowm_claims_breakthrough_in_memristors/,asymptotics,1441374953,,20,33
88,2015-9-4,2015,9,4,23,3jmcca,An artificial machine learning program that attempts to impersonate the writing style of any given text training set,https://www.reddit.com/r/MachineLearning/comments/3jmcca/an_artificial_machine_learning_program_that/,julian88888888,1441375477,,2,1
89,2015-9-4,2015,9,4,23,3jmedy,/r/Elastic - Elasticsearch community on Reddit!,https://www.reddit.com/r/MachineLearning/comments/3jmedy/relastic_elasticsearch_community_on_reddit/,thesameoldstories,1441376434,,0,2
90,2015-9-5,2015,9,5,1,3jmsu5,The Morality Of Machine Learning,https://www.reddit.com/r/MachineLearning/comments/3jmsu5/the_morality_of_machine_learning/,James_Barton1,1441382424,,0,0
91,2015-9-5,2015,9,5,1,3jmt51,Getting into Yoshua Bengio Lab,https://www.reddit.com/r/MachineLearning/comments/3jmt51/getting_into_yoshua_bengio_lab/,yoshuaBengioLab,1441382525,"Hello. I need some information regarding getting admitted to  Yoshua Bengio Lab. How competitive it is? I am sorry if this out of place or it belongs to any other forum. I am thinking about applying for master there and I don't have a undergraduate degree in Maths or CS. I am a chemical engineering undergrad. Is that going to be a problem?

Thanks mates !",11,0
92,2015-9-5,2015,9,5,1,3jmv43,Is there a [paragraph vector](http://cs.stanford.edu/~quocle/paragraph_vector.pdf) implementation that can take an arbitrary vector and generate a paragraph from it?,https://www.reddit.com/r/MachineLearning/comments/3jmv43/is_there_a_paragraph/,[deleted],1441383179,[deleted],0,1
93,2015-9-5,2015,9,5,1,3jmvcd,"Is there a ""paragraph vector"" implementation that can take an arbitrary vector and generate a paragraph from it?",https://www.reddit.com/r/MachineLearning/comments/3jmvcd/is_there_a_paragraph_vector_implementation_that/,sanity,1441383257,"Based on my understanding of [the approach](http://cs.stanford.edu/~quocle/paragraph_vector.pdf) (pdf) it seems like it should be possible to progressively generate a paragraph from a vector.
Is anyone aware of anyone that has tried this?",3,1
94,2015-9-5,2015,9,5,2,3jn5xf,[Question] What interesting machine learning project can be completed within 1000 hours?,https://www.reddit.com/r/MachineLearning/comments/3jn5xf/question_what_interesting_machine_learning/,unlogicalgames,1441387300,"**Context**

We are two high school (special high school that takes 5 years with focus on programming) students approaching 12th grade. In this school, in 12th and 13th grade students have to work on a theoretical or practical thesis (related to programming) in small groups with a supervising teacher. Both being somewhat experienced in programming (we both started when we were around 10 years old and have now been working on our own game projects for over a year) we are fascinated by machine learning and want to make that the focus of our thesis. Over the course of 10 months, starting in like 8 months, we are each willing to invest around 500 hours into this project (meaning a total of 1000 hours).    

**The question**

Having looked up the basics of machine learning, we kind of know what to expect, but only kind of. We have no idea what the scope of such a project could be or what would be reasonable to achieve in the given time frame (1000 hours total). The videos about machine learning applied to 2D games have interested us very much and we would like to go in a similar direction, where the project has some interesting visual output to look at, not just plain text. 

What would be programming languages to do this in (we have experience in C, C# and Java but are quick to learn)? We read the FAQ of course, but as mentioned earlier we would like to apply this to something like 2D games or anything similar, so what would be the best programming language for that (R, Python, ..?)? 

What can we expect to get done but that still takes us a few houndred hours to do?

This questions is very broad and we realize that, but we really are at a loss of where to start and what to do. Any advice or help are greatly appreciated. 

**TLDR**:
Two students who are willing to invest 1000 hours in total, try to find a (visually) interesting topic for their 13th grade thesis. (Maybe a self learning 2D AI?) We know that Python and R are the ways to go, but don't know whether languages like Java, C# may be better since we are confident with these languages already (and C) (and there are probably good libraries out there). We would like some advice for interesting topics, with suitable programming languages and libraries.",37,34
95,2015-9-5,2015,9,5,3,3jndl7,Appropriate technique for this problem?,https://www.reddit.com/r/MachineLearning/comments/3jndl7/appropriate_technique_for_this_problem/,generalT,1441390452,"I worked for a small company that does a lot of video processing.  Time after time, the problem of estimating the processing time of a video arises.  I always push back on this request, saying that creating an estimation model for video processing time will never be accurate.  

I was wondering, though- could machine learning techniques help here?  We have collected A LOT of data about input video properties (size, duration, bitrate, resolution, framerate, etc), and the total time elapsed for processing.

Last night, I thought that a neural network may help with this, but I really have no idea.  

What do you guys think?",4,2
96,2015-9-5,2015,9,5,3,3jngr6,Where can I download a comprehensive list of English words and their frequencies in normal English usage?,https://www.reddit.com/r/MachineLearning/comments/3jngr6/where_can_i_download_a_comprehensive_list_of/,sanity,1441391776,Seems like thousands of people must have generated this dataset but I can't find any comprehensive free list (I've found a few that are limited to 5000 words which seems like a low word-count).,9,4
97,2015-9-5,2015,9,5,4,3jnn2l,FeatureFu: A Machine Learning Toolkit Released as Open Source by LinkedIn,https://www.reddit.com/r/MachineLearning/comments/3jnn2l/featurefu_a_machine_learning_toolkit_released_as/,youtoobinallday,1441394521,,5,17
98,2015-9-5,2015,9,5,5,3jnzg6,Nips 2015 accepted papers,https://www.reddit.com/r/MachineLearning/comments/3jnzg6/nips_2015_accepted_papers/,hackscience,1441399710,,11,30
99,2015-9-5,2015,9,5,6,3jo3c9,Are sigmoid DNNs dead?,https://www.reddit.com/r/MachineLearning/comments/3jo3c9/are_sigmoid_dnns_dead/,NovaRom,1441401177,"I see most of the deep models are ReLu based today. They are faster to train, no pre-training is needed, no saturation happens. So, I wonder if Sigmoid based networks are still justified for any problem?",8,11
100,2015-9-5,2015,9,5,6,3jo5e6,Applying StyleNet to Audio,https://www.reddit.com/r/MachineLearning/comments/3jo5e6/applying_stylenet_to_audio/,samim23,1441402121,,4,0
101,2015-9-5,2015,9,5,6,3jo968,What are the limits of Deep Learning?,https://www.reddit.com/r/MachineLearning/comments/3jo968/what_are_the_limits_of_deep_learning/,Whitey_Knightey,1441403798,"I recently watched [this video](https://www.youtube.com/watch?v=XmWneGydtng) and read [this article](http://www.newyorker.com/news/news-desk/is-deep-learning-a-revolution-in-artificial-intelligence) by Gary Marcus. He is  Professor of Psychology at N.Y.U. and does some work in A.I. In the video he says that Andrew Ng seems to think that Deep Learning can do anything, given enough time and power. He seems to think that Andrew Ng believes that deep learning is a fundamental tool to creating human level artificial intelligence. Gary Marcus seems to believe that deep neural networks still lack ""common sense"" about the world. For example, there was a car detector that saw a car in a ripple in the pond in the video. 

Is machine learning logically constrained in any way that makes it impossible to do certain kinds of operations with machine learning? If machine learning, and therefore deep learning, is not constrained, then why have we not already used it to solve problems like speech recognition or language comprehension to human level yet? ",53,9
102,2015-9-5,2015,9,5,7,3jodbq,Current State of Recommendation Systems by Xavier Amatriain of Netflix (2014 Talk),https://www.reddit.com/r/MachineLearning/comments/3jodbq/current_state_of_recommendation_systems_by_xavier/,[deleted],1441405639,[deleted],2,1
103,2015-9-5,2015,9,5,12,3jpam3,Google patents Neural Network based approaches to make document vectors from word embeddings.,https://www.reddit.com/r/MachineLearning/comments/3jpam3/google_patents_neural_network_based_approaches_to/,muktabh,1441423029,,8,2
104,2015-9-5,2015,9,5,12,3jperj,[Question]Are there any context/semantics aware spell checkers ?,https://www.reddit.com/r/MachineLearning/comments/3jperj/questionare_there_any_contextsemantics_aware/,merlin0501,1441425258,"I find that I often make spelling errors that conventional spell checkers can't detect because they transform one valid English word into another which is incorrect for the context (ie. there/their, its/it's, know/no etc.).  I'm curious if there are currently any spell checkers that do a good job of detecting these types of errors.  If not I'm thinking this could be an interesting NLP/machine learning application.  I'm guessing this problem has already been solved but that the applications are not wide-spread but thought I'd check.",5,2
105,2015-9-5,2015,9,5,17,3jq21z,Inferring Algorithmic Patterns with Stack,https://www.reddit.com/r/MachineLearning/comments/3jq21z/inferring_algorithmic_patterns_with_stack/,pilooch,1441441437,,12,29
106,2015-9-5,2015,9,5,17,3jq44s,"""Dither is Better than Dropout for Regularising Deep Neural Networks""",https://www.reddit.com/r/MachineLearning/comments/3jq44s/dither_is_better_than_dropout_for_regularising/,ajrs,1441443215,,24,16
107,2015-9-5,2015,9,5,18,3jq4li,[Question] Papers and Books: Advanced graph theory,https://www.reddit.com/r/MachineLearning/comments/3jq4li/question_papers_and_books_advanced_graph_theory/,__learningCS,1441443629,"I have two questions please:
1- What are some applications of advanced graph theory 
2- Can you recommend some good resources/papers that introduce graph theory in objects activity ?",1,2
108,2015-9-5,2015,9,5,18,3jq58z,[Question] Best Machine Learning approach for matching documents?,https://www.reddit.com/r/MachineLearning/comments/3jq58z/question_best_machine_learning_approach_for/,kar321,1441444195,"I have a set of around 500 consultant profiles (free text description of their skills). Then I have a set of projects (free text description what the project is about). Now I want to find the best matching consultant for a project.
I already have around 1000 of those matches (project-consultant) as ""labeled"" training data.

How would you approach that problem?

You could simply deal with it as an unsupervised information retrieval problem. But how could you use the the labeled training data and do a machine learning approach? I was wondering if using an RNN with the free text description as input and the 500 consultants as 500 classes might make sense?
",4,4
109,2015-9-5,2015,9,5,23,3jqwtk,Which mathematical course is more / most relevant to attend for machine learning.,https://www.reddit.com/r/MachineLearning/comments/3jqwtk/which_mathematical_course_is_more_most_relevant/,rishok,1441463462,"Which mathematical course is more / most relevant to attend for machine learning.

1) Computational Discrete Mathematics

2) Dynamical Systems

3) Functional Analysis

I am not referring here to knowledge in relation to use ML algorithms, but a deeper understanding of mathematics to the field.

Thanks for the help.",6,2
110,2015-9-5,2015,9,5,23,3jr02d,How is word2vec different from Vector space model used in inverted indexing? [newbie question],https://www.reddit.com/r/MachineLearning/comments/3jr02d/how_is_word2vec_different_from_vector_space_model/,phenkdo02139,1441465155,Aren't inverted indexing using term vectors as well - TF-IDF to generate scores for each term? I am not entirely sure what word2vec does that is novel and blowing people away? Could someone explain the difference/novelty in some detail ? appreciate your responses.,6,24
111,2015-9-6,2015,9,6,2,3jrhsp,[Question] ML Research career advice for non-affiliated polymath with proprietary experience,https://www.reddit.com/r/MachineLearning/comments/3jrhsp/question_ml_research_career_advice_for/,bubaonaruba,1441473426,"Hi,

I'm a CS+Math graduate with pretty strong stats skills in the EU, have 9 years of experience doing algo trading, risk management, AdTech and Retail predictive models &amp; optimization.

Often did major parts of bleeding edge research in ML on my own (generative models, global search search metaheuristics, active learning, new kinds of deep nets), from idea to deployment, but also worked with proper researchers. Those topics were all proprietary, so I don't have any official publications.

Now I'd like to focus more on the research part of the job, instead of focus on building systems. The problem is that virtually all research roles require a PhD and an academic track record. While easily getting accepted for quant dev or data science roles, it seems impossible to even be considered for ML research scientist jobs.

What do I do to be able to do what I love doing? Which is research + seeing the results applied? I see the following options:

A. start a PhD - and waste years of life 'for free', before I land a dream job

B. publish a couple papers on my own - is this even possible to get published that way?

C. start my own company and do research there

Suggestions?",2,1
112,2015-9-6,2015,9,6,2,3jriqb,22 easy-to-fix worst mistakes for data scientists,https://www.reddit.com/r/MachineLearning/comments/3jriqb/22_easytofix_worst_mistakes_for_data_scientists/,rhiever,1441473869,,2,0
113,2015-9-6,2015,9,6,3,3jroy0,"On chain rule, computational graphs, and backpropagation",https://www.reddit.com/r/MachineLearning/comments/3jroy0/on_chain_rule_computational_graphs_and/,outlacedev,1441476792,,11,34
114,2015-9-6,2015,9,6,4,3jrwfw,LSTM implementation explained,https://www.reddit.com/r/MachineLearning/comments/3jrwfw/lstm_implementation_explained/,[deleted],1441480173,[deleted],0,1
115,2015-9-6,2015,9,6,4,3js08y,"BREAKING NEWS: Kaggle sucks at data science and running competitions (well-founded rant in forums by fchollet, author of Keras)",https://www.reddit.com/r/MachineLearning/comments/3js08y/breaking_news_kaggle_sucks_at_data_science_and/,[deleted],1441481938,[deleted],0,2
116,2015-9-6,2015,9,6,6,3jseg2,College Student Interested in Machine Learning,https://www.reddit.com/r/MachineLearning/comments/3jseg2/college_student_interested_in_machine_learning/,austinc3030,1441488356,"Hello Reddit
I am a college student about to graduate with an Associates in Software Engineering with the intent of continuing on to get my Bachelors and Masters. I am fascinated by the concepts of machine learning and have ""tried"" to learn as much as I can. However, Machine Learning is a broad field itself. My question(s) are:

Is there such a thing as a degree (Software, CompSci, etc.) with a focus in Machine Learning?

Are there any schools with a focus in this field or that are known for programs in this field?

What should I be doing in my own time to learn the science and concepts? Writing Code and come up with my own concepts? Read articles on Machine Learning? Find out what aspect of ML I want to focus on? What should I be doing to better educate my self on the topic. 

I thank you for your time and have fallen in love with this sub. Thanks",7,0
117,2015-9-6,2015,9,6,8,3jsvkt,Looking for movie scripts with speakers identified?,https://www.reddit.com/r/MachineLearning/comments/3jsvkt/looking_for_movie_scripts_with_speakers_identified/,oowowaee,1441496711,"A little bit off topic (sorry!), but I have had an idea in mind for a project for a while, and this afternoon when I sat down to get started again I realized why I had stopped previously - my brilliant idea of using movie subtitle files won't work in this case because of the lack of speaker identification.

I've tried looking for scripts, but these seem to be a scarce resource (I couldn't find any for any of the movies I need specifically).

I was wondering if by any chance someone had come up against this problem before and knew of an easy solution.  It seems like closed captions might by definition identify the speaker, but I didn't have a lot of luck finding anything more in that direction.

In terms of what I need, it's only the dialogue, but the speaker needs to be identified.

Thanks!",6,2
118,2015-9-6,2015,9,6,11,3jtage,Deep Forger paints Gabe Newell,https://www.reddit.com/r/MachineLearning/comments/3jtage/deep_forger_paints_gabe_newell/,LetaBot,1441504945,,17,26
119,2015-9-6,2015,9,6,21,3julsd,Popularity scoring for arXiv publications,https://www.reddit.com/r/MachineLearning/comments/3julsd/popularity_scoring_for_arxiv_publications/,nebw,1441541679,,4,17
120,2015-9-7,2015,9,7,0,3jv7cp,word2vec usage questions,https://www.reddit.com/r/MachineLearning/comments/3jv7cp/word2vec_usage_questions/,swampsofjersey,1441554733,"I am writing a term paper for a linguistics course and am using a word2vec implementation to observe semantic (word sense) change over a set period of time. I have built my models in decade long chunks so I can map the difference between cosine similarities throughout the entirety of my corpus. Furthermore I was wondering if there is a way with word2vec to see how word A relates to B in each decade, while taking into account the collocations of both and how those collocations might be influencing both terms; ie. A collocates look like Aa Ab Ac while B collocates to Bd Be Bf at the start of the corpus. Over time A collocates look like Aa Ac Af and B collocates look like Bd Bf Bg.

Since I am approaching this topic from a more linguistic angle I am a bit unfamiliar with some of the equations that may be obvious to others for doing what I would like to do. Does anyone have any suggestions that might help me concisely illustrate this with my implementation? Thanks!",5,6
121,2015-9-7,2015,9,7,3,3jvq08,I made a free program that generates music.,https://www.reddit.com/r/MachineLearning/comments/3jvq08/i_made_a_free_program_that_generates_music/,[deleted],1441563626,[deleted],1,0
122,2015-9-7,2015,9,7,3,3jvs99,Neural network code,https://www.reddit.com/r/MachineLearning/comments/3jvs99/neural_network_code/,DarkPhalanx,1441564686,"I asked this question over at /r/learnpython and I was told to post it over here.

I'm trying to program a neural network with the help of Marsland's Machine Learning: An Algorithmic Perspective since the code is available online. While programming the multi layered perceptron, I was working through the code online available [here](http://seat.massey.ac.nz/personal/s.r.marsland/Code/Ch4/mlp.py).

The way I set up my network was to have the bias weights as 0 index and the input weights from 1 to N. However, if I edit the `concatenate` function on lines 56, 97 and 116 to have the `-1` nodes before the rest and run the code, I sometimes get: `RuntimeWarning: overflow encountered in exp`

This happens on a few runs and the classification ends up being 75% correct for the `XOR` gate. On other runs, I do not get the error and I get a 100% correct classification.
When I run the code as it is, this warning does not occur and it classifies the `XOR` inputs with 100% accuracy every time.

I can't figure out why this is happening.",6,0
123,2015-9-7,2015,9,7,7,3jwi60,"Any suggestions for a first ML project? (more complex than simple regression, but not too ambitious for an intermediate programmer)",https://www.reddit.com/r/MachineLearning/comments/3jwi60/any_suggestions_for_a_first_ml_project_more/,isawahill,1441577117,"I'm planning on jumping into the field of ML with an independent project before I head off to school in two weeks. I'd prefer for it to be more than just a basic multivariate regression-based program (e.g. predicting the height of an adult based on gender and the height of his/her parents), but I'm probably not skilled enough to do anything at the level of, say, natural language processing.  ",28,27
124,2015-9-7,2015,9,7,7,3jwi80,Discover Social Circles in Twitter Ego Network | https://balaca.shinyapps.io/Twitter_Graph,https://www.reddit.com/r/MachineLearning/comments/3jwi80/discover_social_circles_in_twitter_ego_network/,bala_io,1441577147,,0,0
125,2015-9-7,2015,9,7,7,3jwks9,Has anyone used hierarchical temporal memory or Jeff Hawkins work?,https://www.reddit.com/r/MachineLearning/comments/3jwks9/has_anyone_used_hierarchical_temporal_memory_or/,Whitey_Knightey,1441578394,"https://en.wikipedia.org/wiki/Hierarchical_temporal_memory

https://www.youtube.com/watch?v=6ufPpZDmPKA

Jeff Hawkins seems to have a different approach than most AI researchers. He seems to want to follow the principles of the human neocortex as close as possible. I was wondering if any of you could explain what the point of this strategy is or why it might be better or worse than deep learning, the current most popular method in AI. I have also read about neuromorphic chips and architectures. Could combining neuromorphic architectures with brain like AI methods like HTM make better AI? ",25,12
126,2015-9-7,2015,9,7,10,3jx83i,Donating your google history?,https://www.reddit.com/r/MachineLearning/comments/3jx83i/donating_your_google_history/,[deleted],1441590605,[deleted],3,0
127,2015-9-7,2015,9,7,12,3jxhv3,Using ML to identify cases of insurance fraud.,https://www.reddit.com/r/MachineLearning/comments/3jxhv3/using_ml_to_identify_cases_of_insurance_fraud/,AintNoFortunateSon,1441596048,I'm working with an fraud investigator for a major insurance company. Her focus is on identifying and prosecuting cases of insurance fraud involving behaviour health issues. These are non-obvious conditions that can be difficult to diagnose and even more difficult to identify as fraudulent. We were speaking about advances in ML and how it might be a useful tool for identifying possible cases of fraud. Does anyone have any advice on how to move forward to this this? Any resources or ideas on how to get started? T,5,0
128,2015-9-7,2015,9,7,16,3jy3tm,Roll Forming Machine Company High Frequency | H-beam welding pipe line Manufacturers,https://www.reddit.com/r/MachineLearning/comments/3jy3tm/roll_forming_machine_company_high_frequency_hbeam/,maxyou,1441609980,,1,1
129,2015-9-7,2015,9,7,16,3jy75m,From ugly reality to biutiful art. Must see before you die best examples of artistic style. x-post of x-post.,https://www.reddit.com/r/MachineLearning/comments/3jy75m/from_ugly_reality_to_biutiful_art_must_see_before/,derRoller,1441612512,,1,0
130,2015-9-7,2015,9,7,17,3jy8xr,Implementing a Neural Network from Scratch,https://www.reddit.com/r/MachineLearning/comments/3jy8xr/implementing_a_neural_network_from_scratch/,john_philip,1441613929,,5,9
131,2015-9-7,2015,9,7,18,3jydb1,self organizing map for anomalous human action detection,https://www.reddit.com/r/MachineLearning/comments/3jydb1/self_organizing_map_for_anomalous_human_action/,TatuOinam,1441617453,An elaborate explanation on Kohonen's Self Organizing Map and how well can it be used in the research and development of human anomaly action detection. ,3,0
132,2015-9-7,2015,9,7,18,3jydr6,"Autoencoders with very high dimensional, dense data",https://www.reddit.com/r/MachineLearning/comments/3jydr6/autoencoders_with_very_high_dimensional_dense_data/,bigBangBartend,1441617811,"Hello reddit,
I'm working on a project where I need to reduce the dimensionality of my observations and still have a significative representation of them. The use of Autoencoders was strongly suggested for many reasons but I'm not quite sure it's the best approach.

I have 1.400 samples of dimension ~60.000 which is far too high, I am trying to reduce their dimensionality to a 10% of the original one. I'm using theano autoencoders and it seems like the cost keeps being around 30.000 (which is very high). I tried raising the number of epochs or lowering the learning rate with no success. I'm not a big expert on autoencoders so I'm not sure how to proceed from here or when to just stop trying.

There are othere tests I can run but, before going any further, I'd like to have an input from you.

* Do you think the dataset is too small (I can add another 600 samples for a total of ~2000) ?
* Do you think using stacked autoenoders could help ?
* Should I keep tweaking the parameters (epochs and learning rate) ?
* Incresing the number of hidden units raises the cost, is that normal ?

Since the dataset is an ensamble of pictures I tried to visualize the reconstructions from the autoencoders and all I got was the same output for every sample. This means that given the input the autoencoder tries to rebuild the input but what I get instead is the same (almost exactly) image for any input(which kind of looks like an average of all the images in the dataset). This means that the inner representation is not good enough since the autoencoder can't reconstruct the image from it.

**The dataset**: 1400 - 2000 images of scanned books (covers included, not labled) of around ~60.000 pixels each (which translates to a feature vector of 60.000 elements). Each feature vector has been normalized in [0,1] and originally had values in [0,255].

**The problem**: Reduce their dimensionality with Autoencoders (if possible)

As you noticed there are several questions, I'm not directly looking for a solution to my problem from you, *I'm more interested in understanding* the reason why this approach is not really working as intendend and what I am missing about autoencoders that prevents me from using them in this specific problem. If you need any extra info to better understand the scenario, I'll be glad to help you help me =D


Note: I'm currenty running a test with a higher number of epochs on the whole dataset and I will update my post accordingly to the result, it might take a while though.",18,8
133,2015-9-7,2015,9,7,18,3jydt6,Topic Modeling of Twitter Followers using LDA,https://www.reddit.com/r/MachineLearning/comments/3jydt6/topic_modeling_of_twitter_followers_using_lda/,cast42,1441617866,,1,8
134,2015-9-7,2015,9,7,18,3jyf3v,churn definition for non subscription-based services with high variance,https://www.reddit.com/r/MachineLearning/comments/3jyf3v/churn_definition_for_non_subscriptionbased/,edoven,1441618887,"I'm trying to create a prediction model for an on-line game betting/gambling service.
The problem here is that there isn't a ""hard churn event"" as in telco companies (i.e. a ""I want to cancel my subscription"" letter).

I've read some papers and, in video-gaming industry, they usually study the max days of inactivity between two distinct days with activity. Then they see that for 90% of users that distance is lower than 8 days (this is just an example) and they make the assumption that if a user don't come back for more than 8 days she's a churner.

In my industry, that 90th percentile is something like 100 days and if I tell to the business team that if a user doesn't play for 100 days she's a churner, I get fired. :)
So what I'm doing is to analyze, for every user, her max distance, than I say ""if she's not coming for 1.1*max_distance days, she's a churner"".
User A, who has the max number of days of inactivity equals to 10, is a churner if she's not coming for more than 11 days.
User B, who has the max number of days of inactivity equals to 30, is a churner if she's not coming for more than 33 days.

Does it make sense? Any suggestion?


",0,1
135,2015-9-7,2015,9,7,19,3jyh91,[Question] NNs with multiple outputs of multiple classes?,https://www.reddit.com/r/MachineLearning/comments/3jyh91/question_nns_with_multiple_outputs_of_multiple/,olBaa,1441620601,"The toy example of this problem would be predicting both sex (m/f) and race (say, 4 classes) from a persons photo. How one would design the loss function for the NN to do that?

My main concern is that if one will do 2 softmaxes, the gradients will be unbalanced. Is there any common solution to this?
Thanks!",10,10
136,2015-9-7,2015,9,7,20,3jylz1,[1508.03790] Depth-Gated LSTM,https://www.reddit.com/r/MachineLearning/comments/3jylz1/150803790_depthgated_lstm/,egrefen,1441624438,,5,19
137,2015-9-7,2015,9,7,20,3jyo4e,Motor Lubrication: Keep Your Machinery Running,https://www.reddit.com/r/MachineLearning/comments/3jyo4e/motor_lubrication_keep_your_machinery_running/,jackerfrinandis,1441626083,,0,1
138,2015-9-7,2015,9,7,21,3jyuxd,Data preprocessing for unsupervised learning using Auto-encoders,https://www.reddit.com/r/MachineLearning/comments/3jyuxd/data_preprocessing_for_unsupervised_learning/,kmul00,1441630679,"I want to train an autoencoder on my grayscale video data. What are the typical preprocessing steps that my data should undergo before I learn the auto-encoder on them ?

While using CNN previously I used only zero mean, unit std and that gave satisfactory results. Should I use the same for auto-encoders also ?

Specifically, I am using Torch, and in the demo that they have provided (https://github.com/torch/tutorials/tree/master/3_unsupervised) they are downloading an already preprocessed data from Amazon servers (http://torch7.s3-website-us-east-1.amazonaws.com/data/tr-berkeley-N5K-M56x56-lcn.ascii) and using them.  

Anyone having any idea regarding how it is preprocessed ?",0,1
139,2015-9-7,2015,9,7,22,3jyy6s,"Help me identify this quote: ""Perhaps we worked on MNIST for too long""",https://www.reddit.com/r/MachineLearning/comments/3jyy6s/help_me_identify_this_quote_perhaps_we_worked_on/,dunnowhattoputhere,1441632561,"I recall one of the prominent deep learning authors (I think it was either LeCun or Hinton) saying something along the lines of ""perhaps we spent too long using MNIST as our benchmark."" The general gist was that MNIST was limited as a dataset, and that bigger, richer datasets like ImageNet have led to substantial increases in a number of CV tasks.

Does anyone else recall that? If so, do you remember who it was and where they said it?

As a further discussion point, I completely agree. If ImageNet hadn't come around, I don't think we would have made the same progress at large-scale recognition and the field-wide convergence on CNN's wouldn't have happened as quickly. Thoughts?",14,6
140,2015-9-7,2015,9,7,22,3jyyts,What technologies do you recomend me to make the web interface for a machine learning portfolio?,https://www.reddit.com/r/MachineLearning/comments/3jyyts/what_technologies_do_you_recomend_me_to_make_the/,mareram,1441632965,"Hi. First of all, saying that I'm not sure if this would be the right place for a question like this. If you think it isn't, please, move it.

I'm looking for a new job and I've got the problem that in my actual job we use own technologies which are not standard. I have studied and practiced Python, Matlab/Octave, Hadoop, Spark, etc, etc... but on my own. And It's quite a problem in the job interviews when they know that I have not used that professionally.

So, I've thought about, using a VPS that I already have, programming code that run on that VPS and which can be accessed from a web interface, so that they can check that I do know all the things that I have in my CV and that I'm not overstating.

For example, a user may be able to load an small dataset of a given format from a list of available datasets (or loading an small dataset given by the user), set a list of options, launch an small machine learning process and see results in an small report.

My main doubt is twofold:

  - Do you think this is a good approach? is there any easier, more straightforward way to showcase skills?
  - What tools can I use to write the web interface? 

My main problem is that I don't know web technologies and I would like to use as few bullets as possible. This is, I'm perfectly able to learn whatever is necessary, but I'd like results as soon as possible. I don't want to spend weeks or months learning something only to realize that it is not what I need, that there was a better/easier/direct option or that it is outdated.",12,4
141,2015-9-8,2015,9,8,0,3jza12,Machine Learning Specialization - Coursera.org,https://www.reddit.com/r/MachineLearning/comments/3jza12/machine_learning_specialization_courseraorg/,fsqcds,1441638876,,19,52
142,2015-9-8,2015,9,8,3,3jzz5t,LSTM implementation explained,https://www.reddit.com/r/MachineLearning/comments/3jzz5t/lstm_implementation_explained/,wesolyromek,1441650020,,2,22
143,2015-9-8,2015,9,8,3,3jzzj2,Neural Networks: Closing in on the Big Black Box Problem,https://www.reddit.com/r/MachineLearning/comments/3jzzj2/neural_networks_closing_in_on_the_big_black_box/,[deleted],1441650181,[deleted],3,19
144,2015-9-8,2015,9,8,3,3k02ku,Running DistributedDataMining or MindModeling@Home? Offset electricity costs with Gridcoin!,https://www.reddit.com/r/MachineLearning/comments/3k02ku/running_distributeddatamining_or_mindmodelinghome/,grctest,1441651508,,12,9
145,2015-9-8,2015,9,8,5,3k0dp1,Dramatic performance improvements with smaller batch sizes?,https://www.reddit.com/r/MachineLearning/comments/3k0dp1/dramatic_performance_improvements_with_smaller/,[deleted],1441656683,"I'm training stacked LSTM encoder-decoder network with 300,000 input sequences. Although my results are reasonable, I decided to go back to basics and reconsider the design of my model.

I took a basic dataset of 2,000 sequences to run tests on and made a few improvements. Then I decreased the batch size to 10 and something a little startling has happened. Training only on these 2,000 sentences, with the smaller batch size, I am seeing an accuracy of around 60% on NEW DATA after ONE epoch. With the larger batch size and before the various tweaks I made, it is around 18%. I cannot fathom how this network could learn so much about new data from 2,000 sequences in one epoch.

This seemed improbable to me, and I have checked the usual things - I'm definitely initialising on new weights, and I have run it on different test data to make sure it wasn't a fluke set with similarities to the 2,000 training sentences.

Can changing the batch size really make so much of a difference with an RNN? I'm aware that batching training data is a murky activity at the best of times, but this seems really dramatic - although I am probably biased from having become accustomed to the poorer results!

",17,5
146,2015-9-8,2015,9,8,6,3k0m0w,Yann LeCun's talk at HC27: Convolutional Neural Networks,https://www.reddit.com/r/MachineLearning/comments/3k0m0w/yann_lecuns_talk_at_hc27_convolutional_neural/,Buck-Nasty,1441661308,,25,49
147,2015-9-8,2015,9,8,7,3k0qa2,Amazon Machine Learning: use cases and a real example in Python (w/ code),https://www.reddit.com/r/MachineLearning/comments/3k0qa2/amazon_machine_learning_use_cases_and_a_real/,caimanodistratto,1441663550,,0,23
148,2015-9-8,2015,9,8,9,3k194s,Documentation for Linear C-Support and Nu-Support Vector Classification,https://www.reddit.com/r/MachineLearning/comments/3k194s/documentation_for_linear_csupport_and_nusupport/,raduqq,1441672824,"I'm trying to find documentation for how the math works for these algorithms, or at least the concepts about how the algorithm works.

The algorithms are part of sklearn Python library. So far, unfortunately, I couldn't find anything on the web related to these. Maybe they are called differently in the science community?

Any help is appreciated.",3,2
149,2015-9-8,2015,9,8,10,3k1du0,What do you think about SPAUN and its methods?,https://www.reddit.com/r/MachineLearning/comments/3k1du0/what_do_you_think_about_spaun_and_its_methods/,Whitey_Knightey,1441675289,"http://nengo.ca/

http://www.extremetech.com/extreme/141926-spaun-the-most-realistic-artificial-human-brain-yet

I am quite excited by SPAUN. I just found out about it yesterday. SPAUN is able to do several intelligence tests, giving it a much more general ability that any other AIs I have ever seen. It uses 2.5 million simulated neurons to think. I am wondering why no one has put SPAUN on a better super computer or scaled it up. Currently, it takes hours to make a few seconds of simulation on this hardware. 

SPAUN looks like it has the right general architecture for general intelligence, so why aren't more people making stuff like SPAUN? I think a larger model like SPAUN could create human level intelligence.",4,1
150,2015-9-8,2015,9,8,11,3k1o8v,Zaid Harchaoui@MLSS2015: Our Convolutional Kernel Networks have outperformed ConvNets on ImageNet,https://www.reddit.com/r/MachineLearning/comments/3k1o8v/zaid_harchaouimlss2015_our_convolutional_kernel/,whirrrlybird,1441680542,"Harchaoui mentioned at this year's MLSS2015 that their [Convolutional Kernel Networks](http://www.harchaoui.eu/zaid/publications/mkhs_2014_ckn.pdf) have outperformed ConvNets on Imagenet. Also mentioned that they're 100x faster to train.

Are these signs of the second return of kernel methods?

Anyone have any more details about this?",8,9
151,2015-9-8,2015,9,8,12,3k1uzl,The ImageNet for NLP?,https://www.reddit.com/r/MachineLearning/comments/3k1uzl/the_imagenet_for_nlp/,asymptotics,1441684024,"Is there a NLP benchmark much like ImageNet for CV? If so, what are the most successful algorithms?",8,10
152,2015-9-8,2015,9,8,14,3k264r,Understanding Machine Learning: From Theory to Algorithms,https://www.reddit.com/r/MachineLearning/comments/3k264r/understanding_machine_learning_from_theory_to/,john_philip,1441690526,,4,24
153,2015-9-8,2015,9,8,17,3k2jav,Why Big Data Machine Learning is over solving a non-existent problem? - Hopdata,https://www.reddit.com/r/MachineLearning/comments/3k2jav/why_big_data_machine_learning_is_over_solving_a/,imshashank,1441700067,,0,1
154,2015-9-8,2015,9,8,20,3k2x3x,Noisy data with SVMs,https://www.reddit.com/r/MachineLearning/comments/3k2x3x/noisy_data_with_svms/,j_mascis_is_jesus,1441710477,"I have been going back and forth over something and I just wanted to see if my thinking was correct. (Bare in mind my understanding of all this is pretty limited).

Using error prone or noisy data (e.g. from psychology etc), SVMs are possibly not ideal, as the sparse number of support vectors means the decision boundary can be influenced by outliers.

Incorporating Bayesian or Fuzzy logic into SVMs (e.g. relevance vector machines or fuzzy SVMs) could possibly improve predictive capability, as the effect of outliers are reduced?

Any help would be greatly appreciated, especially if I'm barking up the wrong (decision) tree. (Sorry, couldn't help myself).",8,5
155,2015-9-8,2015,9,8,20,3k30q4,Splitting 1D data into two unsupervised classes,https://www.reddit.com/r/MachineLearning/comments/3k30q4/splitting_1d_data_into_two_unsupervised_classes/,Thagor,1441712856,"I have a couple of data series like in the example image. It shows the value distribution of those series. What would be a good approach splitting data like this into two clusters?

[Example](http://i.imgur.com/Bfbm4em.png) ",6,3
156,2015-9-8,2015,9,8,21,3k3300,Deep Learning Libraries by Language,https://www.reddit.com/r/MachineLearning/comments/3k3300/deep_learning_libraries_by_language/,lavonnadentler,1441714296,,7,88
157,2015-9-8,2015,9,8,21,3k34yo,What's the worst part about learning Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/3k34yo/whats_the_worst_part_about_learning_machine/,LearnDataSci,1441715385,,6,2
158,2015-9-8,2015,9,8,21,3k36kd,CNN: A C++ neural network library as awesome as it is confusingly named (by CMU's clab),https://www.reddit.com/r/MachineLearning/comments/3k36kd/cnn_a_c_neural_network_library_as_awesome_as_it/,egrefen,1441716303,,7,4
159,2015-9-8,2015,9,8,22,3k3csl,"Unsupervised Feature Learning in Video: Learning to Linearize - understanding Goroshin, Mathieu &amp; LeCun's NIPS paper",https://www.reddit.com/r/MachineLearning/comments/3k3csl/unsupervised_feature_learning_in_video_learning/,fhuszar,1441719633,,2,7
160,2015-9-8,2015,9,8,23,3k3iah,How to evaluate the effect of gates in RNN like LSTM?,https://www.reddit.com/r/MachineLearning/comments/3k3iah/how_to_evaluate_the_effect_of_gates_in_rnn_like/,ihsgnef,1441722167,"Hi, I'd like to know how to determine if a gate in LSTM is important or not.
For example, I don't see the intuition behind the output gate in LSTM.

In paper [An Empirical Exploration of Recurrent Network Architectures](http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf), they conclude that in most tasks the output by comparing the performance of LSTM with / without a output gate. 

Is there other ways to determine a gate's importance? I could probably check how the weights of a gate varies with different inputs, to see if the gate is sensitive to the input. But a gate is affected by many factors, so this might not be useful.

Generally how do you determine the importance of a neuron/layer in neural network?",5,3
161,2015-9-8,2015,9,8,23,3k3kug,A Beginners Guide to Recurrent Networks and LSTMs,https://www.reddit.com/r/MachineLearning/comments/3k3kug/a_beginners_guide_to_recurrent_networks_and_lstms/,vonnik,1441723265,,1,7
162,2015-9-9,2015,9,9,0,3k3okn,Spatial Transformer Networks for Traffic Sign Recognition outperforms a committee of CNNs,https://www.reddit.com/r/MachineLearning/comments/3k3okn/spatial_transformer_networks_for_traffic_sign/,[deleted],1441724962,[deleted],0,1
163,2015-9-9,2015,9,9,0,3k3pzl,Spatial Transformer Networks for Traffic Sign Recognition outperform a committee of CNNs,https://www.reddit.com/r/MachineLearning/comments/3k3pzl/spatial_transformer_networks_for_traffic_sign/,negazirana,1441725565,,29,20
164,2015-9-9,2015,9,9,3,3k4ecb,Question about Batch Normalization,https://www.reddit.com/r/MachineLearning/comments/3k4ecb/question_about_batch_normalization/,enk100,1441735357,"So I read the article of BN - http://arxiv.org/abs/1502.03167.
and the following is mentioned there: 
""Note that simply normalizing each input of a layer may change what the layer can represent. For instance, normalizing the inputs of a sigmoid would constrain them to the linear regime of the nonlinearity. To address this, we make sure that the transformation inserted in the network can represent the identity transform."" 

I understand that the problem is that the normalization may change the features, but why do we want the net to have the capabilities to represent the identity transform, why not another transform?

thanks",1,3
165,2015-9-9,2015,9,9,3,3k4ib9,how could this run on just a raspberry pi? is it even real?,https://www.reddit.com/r/MachineLearning/comments/3k4ib9/how_could_this_run_on_just_a_raspberry_pi_is_it/,jj7205,1441736958,,2,2
166,2015-9-9,2015,9,9,3,3k4m5t,Tagged and Searchable Video - Caffe setup guide (for Deep learning),https://www.reddit.com/r/MachineLearning/comments/3k4m5t/tagged_and_searchable_video_caffe_setup_guide_for/,ojaved,1441738499,,0,1
167,2015-9-9,2015,9,9,3,3k4mfs,"Apple ups hiring, but Machine Learning experts tend to shy away over Privacy concerns",https://www.reddit.com/r/MachineLearning/comments/3k4mfs/apple_ups_hiring_but_machine_learning_experts/,dabshitty,1441738604,,7,30
168,2015-9-9,2015,9,9,4,3k4n34,"Free online textbook: ""Understanding Machine Learning: From Theory to Algorithms""",https://www.reddit.com/r/MachineLearning/comments/3k4n34/free_online_textbook_understanding_machine/,[deleted],1441738874,[deleted],0,1
169,2015-9-9,2015,9,9,5,3k4xxz,Let's Understand Amazon Machine Learning,https://www.reddit.com/r/MachineLearning/comments/3k4xxz/lets_understand_amazon_machine_learning/,caimanodistratto,1441743310,,1,0
170,2015-9-9,2015,9,9,6,3k57bk,Bayesian Data Analysis Python Demos,https://www.reddit.com/r/MachineLearning/comments/3k57bk/bayesian_data_analysis_python_demos/,cast42,1441747318,,1,17
171,2015-9-9,2015,9,9,6,3k581m,Help needed with Principal Component Analysis on MatLab,https://www.reddit.com/r/MachineLearning/comments/3k581m/help_needed_with_principal_component_analysis_on/,interstellar1990,1441747623,"Hi all,


I'm trying to run PCA analysis on a 80x13 dataset (80 time observations, 13 variables), to pull out the first principal component.

I also want to create residuals, which will be the result of taking away the standardised matrix (B below in the code) from the principal component (replicated across 13 variables of course). 


However, I need to impose the restriction that the first principal component has a scalar product that is equal to 1. By imposing that the first principal component is equal to 1 will ensure that my principal component will be uncorrelated to every residual.


Does anyone know what restriction or changes I need to make to my MatLab code to ensure I create this?



I would be very grateful for any help, no matter how large or small.
My Matlab code is attached below. I am getting the first component at the moment by taking the first column of the SCORE matrix.
With kind regards,


KB




Code:




%% Basic PCA

clc 
clear all

%% Load Data

filename = 'DataFinal.xlsx';
A = xlsread(filename);

% Principal Component Analysis

[n m] = size(A); %size of A

Amean = mean(A);

Astd = std(A);

% Now I seek to standardise the data normally. This will ensure that the
% data is centred and rescaled. I subtract the mean and divide by the
% standard deviation.

B = (A - repmat(Amean,[n 1])) ./ repmat(Astd,[n 1]);


%Calculating the coefficients of princ components and respective variances
%, done by finding eigenfunctions of sample covariance matrix

[V D] = eig(cov(B))

% V here contains the coefficients of principal components, diagonal
% elements of D store the variance of respective principal components.

%Alternatively the below is a better way of calculating our principal
%components and their variance.


[COEFF SCORE LATENT] = princomp(B)

%COEFF is a 13x13 square matrix
%LATENT is 13x1  matrix
%SCORE is 80x13 matrix (same dimensions as original time series set, with the first column of SCORE giving the first principal component)

% Cumulative variance contained in first so many principal components can
% be calculated as:

cumsum(var(SCORE)) / sum(var(SCORE))


% Also note all of the principal components are completely uncorrelated",6,2
172,2015-9-9,2015,9,9,6,3k5b3h,Modeling a time series for prediction (but not exactly prediction),https://www.reddit.com/r/MachineLearning/comments/3k5b3h/modeling_a_time_series_for_prediction_but_not/,answer-questions,1441748909,"I'm not quite sure on the correct terminology to use so I'll try to explain. In general, I am trying to create a model based on sets of training data which will be able to generate a new time series that has the characteristics of the set of training data is was created against.

My training data consists of 30 second long traces of (time, packetsize) pairs of network traffic. each 30 second long trace generally has about 1,000 - 100,000 pairs (depending on how many packets are sent). The traces are separated out into categories of traffic type (such as streaming video, skype, ftp download, etc).

So a snippet of a trace would look something like this:

    0.0.006825,134
    0.020398,131
    0.026335,40
    0.039872,140
    0.047299,138

Looking at each type of trace, they generally have fairly obvious characteristics (one type will have steady flow versus another type which has more of a wave of flow). I currently create a markov chain based on these parameters, but all of that is done manually. I have a few questions about what would be the appropriate way to train a model to do this.

I have been looking into creating a neural network to do this, but I'm not sure if this is the correct path to take. These are the steps I was going to be taking to format my problem into a NN problem:

* Get rid of the decimal times by choosing a fixed timeslice, and converting my input data into a single list of sizes, where the interleaving times are set to 0 size. (so for instance if I started with [(0.1, 10) (0.2, 12) (0.4, 50)] that would be converted to [10 12 0 50] by simply choosing a timeslice of 0.1 seconds.

* Create a NN with the number of inputs being a large enough number to see over all of the zeros which the above method would create. So if the precision is one value per millisecond, the window would probably have to cover a few thousand data points. The output of the NN would be the next size value directly after the window of inputs. I would have the window increment one position over until the entire dataset had been fed into the NN.

* To generate new traces, randomize the inputs and run the NN to get the next output, continue until the required number of timeslices are output.

Does that all make sense? Or is this not the approach one would take?",1,3
173,2015-9-9,2015,9,9,9,3k61t9,Videos from ML for Fintech Bloomberg event,https://www.reddit.com/r/MachineLearning/comments/3k61t9/videos_from_ml_for_fintech_bloomberg_event/,arshakn,1441759347,,0,5
174,2015-9-9,2015,9,9,11,3k6gt9,Your training error is always lower than your test error,https://www.reddit.com/r/MachineLearning/comments/3k6gt9/your_training_error_is_always_lower_than_your/,vkhuc,1441765276,,8,10
175,2015-9-9,2015,9,9,12,3k6rlx,Comparison between ConvNet filters and visual cortex?,https://www.reddit.com/r/MachineLearning/comments/3k6rlx/comparison_between_convnet_filters_and_visual/,ptrblck,1441769566,"Hi all,

I am currently reading about deep learning and especially ConvNets.
After reading some papers from Hubel and Wiesel, who studied the mammalian vision system, I remembered seeing a comparison of filters created by a ConvNet and filters ""derived"" from the visual cortex of a mammal.
Unfortunately, I cannot find anything about this comparison anymore in any paper.
So I am starting to doubt if there really was something like a visualization of filters from the visual cortex or if I just misunderstood some images.

Does someone know if there exist such a visualization?
It should look similar to this but comparing a ConvNet and the visual cortex:
http://cs231n.github.io/assets/cnn/weights.jpeg


Best regards,
ptrblck",4,6
176,2015-9-9,2015,9,9,16,3k7h86,Is there easy tool to make pictures look like paintings ?,https://www.reddit.com/r/MachineLearning/comments/3k7h86/is_there_easy_tool_to_make_pictures_look_like/,calomer,1441782124,"First of all, I was looking for photoshop filters, but I think art style transfer scheme would work just as well (if not better).

I think it should be more than possible to realize it on MatConvNet. Python libraries have heavy dependencies on Linux, which make them real pain to port to Windows.

Anyone knows of such a library build with Windows directly ?",7,0
177,2015-9-9,2015,9,9,18,3k7ws7,Xray machines manufacturers,https://www.reddit.com/r/MachineLearning/comments/3k7ws7/xray_machines_manufacturers/,anniejohar,1441791475,[removed],0,0
178,2015-9-9,2015,9,9,18,3k7y5z,Reading big data like a book,https://www.reddit.com/r/MachineLearning/comments/3k7y5z/reading_big_data_like_a_book/,AlexBogle,1441792301,,0,1
179,2015-9-9,2015,9,9,19,3k804p,Deep Learning Libraries by Language,https://www.reddit.com/r/MachineLearning/comments/3k804p/deep_learning_libraries_by_language/,john_philip,1441793459,,0,1
180,2015-9-9,2015,9,9,19,3k813p,Comparing Artificial Artists,https://www.reddit.com/r/MachineLearning/comments/3k813p/comparing_artificial_artists/,john_philip,1441794062,,2,32
181,2015-9-9,2015,9,9,20,3k86tq,Categorizing NIPS paper titles using LDA topic modeling,https://www.reddit.com/r/MachineLearning/comments/3k86tq/categorizing_nips_paper_titles_using_lda_topic/,likelihoodtprior,1441797363,,0,3
182,2015-9-9,2015,9,9,20,3k89gm,Nervana's Neon library with autodiff,https://www.reddit.com/r/MachineLearning/comments/3k89gm/nervanas_neon_library_with_autodiff/,pranv,1441798785,,0,25
183,2015-9-9,2015,9,9,21,3k8dl1,Hologram making machine,https://www.reddit.com/r/MachineLearning/comments/3k8dl1/hologram_making_machine/,sonalbisht101,1441800915,[removed],0,1
184,2015-9-9,2015,9,9,21,3k8gab,Audience Modeling With Spark ML Pipelines,https://www.reddit.com/r/MachineLearning/comments/3k8gab/audience_modeling_with_spark_ml_pipelines/,ezhulenev,1441802200,,0,1
185,2015-9-9,2015,9,9,21,3k8j8j,Choose the Best Vibratory Plate Compactors,https://www.reddit.com/r/MachineLearning/comments/3k8j8j/choose_the_best_vibratory_plate_compactors/,alienamarion11,1441803537,,0,1
186,2015-9-9,2015,9,9,22,3k8p3p,Need help with regression in Caffe,https://www.reddit.com/r/MachineLearning/comments/3k8p3p/need_help_with_regression_in_caffe/,faust_1706,1441806155,"I am having an extraordinary amount of trouble trying to figure out caffe. 

My problem: I have a bunch of pictures of totes (~200) (I know that is nowhere near enough to train on, but this is just a test run, and it outputs the rotation to line up with the center of the tote (found using opencv). 

There are a lot of things that could have gone wrong: the generation of the data, the model, the training script, or the python inference script. 

The python inference script does yield an output...out -150 something. 

There are a lot of moving parts to this and the resources already online aren't the most beginner friendly. 

Eventually what I want to do is train it on the pose of an object, that is it's position in x y z as well as the rotations (pitch roll and yaw) (also found using opencv)

If someone could give me anything as to what is going wrong, I'd be very grateful. 

Relevant files: https://www.dropbox.com/sh/0g51fnn4wejec64/AADLE8lHZcR-raKKmRiTIPjNa?dl=0",6,2
187,2015-9-9,2015,9,9,22,3k8r6h,Random Forest Regression and Classification in R and Python,https://www.reddit.com/r/MachineLearning/comments/3k8r6h/random_forest_regression_and_classification_in_r/,theglamp,1441807050,,0,5
188,2015-9-9,2015,9,9,23,3k8wp9,Talking Machines: a great podcast about machine learning.,https://www.reddit.com/r/MachineLearning/comments/3k8wp9/talking_machines_a_great_podcast_about_machine/,ecobost,1441809246,,4,101
189,2015-9-10,2015,9,10,0,3k93d2,When Big Data Becomes Bad Data,https://www.reddit.com/r/MachineLearning/comments/3k93d2/when_big_data_becomes_bad_data/,RinzeWind,1441811763,,0,0
190,2015-9-10,2015,9,10,1,3k9b0d,Essentials of Machine Learning Algorithms (with Python and R Codes),https://www.reddit.com/r/MachineLearning/comments/3k9b0d/essentials_of_machine_learning_algorithms_with/,vkvk724,1441814596,,0,12
191,2015-9-10,2015,9,10,2,3k9ljm,How do you explain your job to people outside the field?,https://www.reddit.com/r/MachineLearning/comments/3k9ljm/how_do_you_explain_your_job_to_people_outside_the/,changingourworld,1441818396,,9,10
192,2015-9-10,2015,9,10,4,3ka7so,Is there a market for trained machines/networks?,https://www.reddit.com/r/MachineLearning/comments/3ka7so/is_there_a_market_for_trained_machinesnetworks/,pointernil,1441826443,"Is there already a market for (pre)trained neural networks?
A working sharing community maybe?
Anything like this in the works?",8,5
193,2015-9-10,2015,9,10,5,3kaiif,Is there any theory which explains why dropout doesn't work well when combined with batch normalization?,https://www.reddit.com/r/MachineLearning/comments/3kaiif/is_there_any_theory_which_explains_why_dropout/,SunnyJapan,1441830144,,4,4
194,2015-9-10,2015,9,10,5,3kaj8v,Some Important Streaming Algorithms You Should Know About,https://www.reddit.com/r/MachineLearning/comments/3kaj8v/some_important_streaming_algorithms_you_should/,[deleted],1441830393,[deleted],0,1
195,2015-9-10,2015,9,10,5,3kajee,Some Important Streaming Algorithms You Should Know About,https://www.reddit.com/r/MachineLearning/comments/3kajee/some_important_streaming_algorithms_you_should/,[deleted],1441830438,[deleted],0,0
196,2015-9-10,2015,9,10,5,3kake8,RNN output problem in theano,https://www.reddit.com/r/MachineLearning/comments/3kake8/rnn_output_problem_in_theano/,Bhavishya1,1441830755,"I am working on developing an RNN in theano, python and I am currently facing a problem. This problem has been written in detail here - https://groups.google.com/forum/#!topic/theano-users/Hmk3OXB_pQ0 .

Any help is much appreciated.",0,5
197,2015-9-10,2015,9,10,5,3kakvl,pysterior: Bayesian Supervised learning in python,https://www.reddit.com/r/MachineLearning/comments/3kakvl/pysterior_bayesian_supervised_learning_in_python/,faming13,1441830931,,2,7
198,2015-9-10,2015,9,10,5,3kan17,Can recurrent neural network directly smooth the output?,https://www.reddit.com/r/MachineLearning/comments/3kan17/can_recurrent_neural_network_directly_smooth_the/,tuming1990,1441831636,"Now recurrent neural network consider time dependency in hidden layers. Can we also smooth the output of neural network by adding time dependency to output? Like the autoregressive method but with multiple-layer structure? 

I founf the toolkit CURRENNT can set output to LSTM nodes. But the output should be between -1 and 1 because of the nonlinear activation function.",0,1
199,2015-9-10,2015,9,10,5,3kaobl,autograd: Efficiently computes derivatives of numpy code,https://www.reddit.com/r/MachineLearning/comments/3kaobl/autograd_efficiently_computes_derivatives_of/,faming13,1441832098,,3,41
200,2015-9-10,2015,9,10,5,3kaovf,Anyone familiar with using Google compute engine?,https://www.reddit.com/r/MachineLearning/comments/3kaovf/anyone_familiar_with_using_google_compute_engine/,WTFseriously_,1441832254,"I've been looking over the docs for using it with python, and have worked through the tutorial of creating a cluster via a script. My goal is to have a pipeline between my desktop and the cluster. That is, sending data to the cluster, performing some heavy computation, getting the data back on my desktop for lighter computation, and repeat for however many times I need.

Is there any library which makes that streaming simple? The only way I can see how to do it so far is to create pickled objects / some kind of serialization of the data and store in a bucket on the cluster, and send / receive data via the bucket. This seems like a very roundabout way of doing it and would require a good bit of infrastructure to accomplish. 
",0,4
201,2015-9-10,2015,9,10,7,3kb0zp,[MachineLearning] I am Kaitlyn I wait you! Fck me now! My id: 62594382762,https://www.reddit.com/r/MachineLearning/comments/3kb0zp/machinelearning_i_am_kaitlyn_i_wait_you_fck_me/,akalk44057_qq,1441836588,,0,1
202,2015-9-10,2015,9,10,9,3kbot9,anyone using word2vec/GloVe in production?,https://www.reddit.com/r/MachineLearning/comments/3kbot9/anyone_using_word2vecglove_in_production/,[deleted],1441845002,[deleted],6,10
203,2015-9-10,2015,9,10,10,3kc1bg,A Visual Introduction to Machine Learning,https://www.reddit.com/r/MachineLearning/comments/3kc1bg/a_visual_introduction_to_machine_learning/,_alphamaximus_,1441849817,,0,16
204,2015-9-10,2015,9,10,11,3kc3p2,Possible way to generalize dropconnect?,https://www.reddit.com/r/MachineLearning/comments/3kc3p2/possible_way_to_generalize_dropconnect/,mr_yogurt,1441850740,"Just to get it out of the way: I don't have any good reason for why this would work. But I'm just curious if anybody thinks it's worth implementing.

Onto the generalization.
Dropconnect can be seen as doing an elementwise multiplication of all weights by a matrix (per layer) where each element is either 0 or 2 (determined randomly). After modifying the weights this way, the gradients of all of the weights are computed. After computing the gradients of all of the weights, we update the original unmodified weights using these gradients multiplied by the random matrix from earlier.

Would learning still be possible if we modified dropconnect so that the random matrix has each element be a random real number in the range [0, 2] with the same learning?

The one thing I'm not sure about on the generalization is the elementwise multiplication of the gradients by the random matrix. That should have the same effect as only training on the weights that weren't dropped (possibly times some constant), but I'm not entirely sure.

Easy on the eyes version of dropconnect and the generalization:

###Dropconnect
1. For each layer in the network, create a random matrix R that has the same dimensions as the weight matrix W where each element is either 0 or 2 with some probability.
2. Create a new weight matrix W' for each layer which is W  R where  is the elementwise product of two matrices.
3. Calculate the gradients W' for each layer using W'.
4. For each layer, compute W which is equal to W'  R for each layer.
5. For each layer, update the weights using W as the gradients, such as W = W - (  W) where  is a learning rate. ^^~~this~~ ^^all ^^steps ^^might ^^be ^^wrong

###Generalization
1. For each layer in the network, create a random matrix R that has the same dimensions as the weight matrix W where each element is a random real number with some distribution that might need to be constrained in a way I haven't figured out yet. I the mean might have to be 1. I'm pretty sure that a uniform distribution between 0 and 2 works. ^^note: ^^I ^^have ^^no ^^clue ^^how ^^this ^^still ^^works ^^with ^^distributions ^^that ^^can ^^give ^^negative ^^numbers ^^or ^^numbers ^^bigger ^^than ^^2.
2. Create a new weight matrix W' for each layer which is W  R where  is the elementwise product of two matrices.
3. Calculate the gradients W' for each layer using W'.
4. For each layer, compute W which is equal to W'  R for each layer.
5. For each layer, update the weights using W as the gradients, such as W = W - (  W) where  is a learning rate. ^^~~this~~ ^^all ^^steps ^^might ^^be ^^even ^^more ^^wrong

Dropconnect is a special case of the generalization where the distribution is a bernoulli distribution (scaled by two).

Disclaimer on the description of dropconnect (and generalization): there are probably some constant factors missing that I'm ignoring because we multiply the gradients by some arbitrarily picked learning rate anyway. And it's also likely that it's completely wrong.

I'd like to hear your opinions on whether or not it would work or if my description of dropconnect is wrong (or the generalization), or if there are any steps where my description is vague or inaccurate.

EDIT: making post more clear

EDIT 2: I forgot to mention this, but I have tried implementing this. I either implemented something wrong or chose my metaparameters incorrectly, because all of the learning methods I tried were getting around 10% error on MNIST. That was about the error for both simple gradient descent, gradient descent with momentum, gradient descent with Nesterov momentum, dropconnect, and this generalization.

EDIT 3: Made generalization an actual generalization.",3,2
205,2015-9-10,2015,9,10,11,3kc6g9,Speech Recognition data set,https://www.reddit.com/r/MachineLearning/comments/3kc6g9/speech_recognition_data_set/,CecilStan,1441851819,Im looking for a data sets of people speaking the digits 0 to 9. i want to use it for a simple 10 class classification problem. i have looked or hours and cant find anything. where can i find something like this?,8,3
206,2015-9-10,2015,9,10,12,3kce9z,"Scalable Nonparametrics workshop on GPs, DPs, IBPs, SVMs, kernels in NIPS 2015: http://t.co/QJ7sK1899T?ssr=true #MachineLearning",https://www.reddit.com/r/MachineLearning/comments/3kce9z/scalable_nonparametrics_workshop_on_gps_dps_ibps/,brussel_sprout_s,1441855043,,2,7
207,2015-9-10,2015,9,10,13,3kcmgp,"""We introduce a Perpetual Learning Machine; a new type of DNN that is capable of brain-like dynamic 'on the fly' learning""",https://www.reddit.com/r/MachineLearning/comments/3kcmgp/we_introduce_a_perpetual_learning_machine_a_new/,downtownslim,1441858592,,14,4
208,2015-9-10,2015,9,10,15,3kd2mm,EMG manufacturer,https://www.reddit.com/r/MachineLearning/comments/3kd2mm/emg_manufacturer/,anniejohar,1441867003,[removed],0,1
209,2015-9-10,2015,9,10,16,3kd8cz,Make the Most Productive Use of Tamping Rammers,https://www.reddit.com/r/MachineLearning/comments/3kd8cz/make_the_most_productive_use_of_tamping_rammers/,alienamarion11,1441870303,,0,1
210,2015-9-10,2015,9,10,17,3kdfwn,Need advice on statistical time series modeling,https://www.reddit.com/r/MachineLearning/comments/3kdfwn/need_advice_on_statistical_time_series_modeling/,domac,1441875217,"Hello everyone,

I am a graduate student and currently trying to work my way in the topic of time series modeling with statistical methods. I really cannot find a good introduction about

- basic methods
- advanced methods
- state of the art methods
- and generally basic ideas

Furthermore I thought about trying to teach myself about the topic by using a data set to test these methods in a bottom-up manner -- do you think this is a good way to achieve a fast learn process? (This doesn't mean that I try to avoid the theory by all means!)

Looking forward to your advices, thanks! I'd be happy to receive some reference about books, papers, (other subreddits?) etc.

----

EDIT:

Well, I thought about more clarification here why I post here; mainly because I try to dive into the topic in general and try not to be too specific. To my backgrounds: I have good machine learning knowledge (when it comes to basics) which imho is important in Machine Learning and a good understanding in the required math.

I googled and was not able to find anything that helped and hence I give it a try here!

----

EDIT2:

I try to be somewhat more specific with an example. Assume we have a standard 6 sided dice (no manipulations done to the dice) in the following game:
We have two players and intially player 1 throws the dice once. From there on the other player has to make a guess about the next number player 1 rolls. This process repeats up to N (e.g. N = 12) and the game is played M times (e.g. M = 10). So the maximum score for player 2 is ""12  9 = 108"" and the score for player 1 is ""108 - #player2's-false-guess"". How would the model for player 2 (to predict the next dice roll) look like?

Furthermore, does this example help to calrify what I am trying to understand?

My ideas so far would be to use a Markov Chain or even a Hidden Markov Model (considering that the latent variable might be the dice throwing manner, etc.).",11,5
211,2015-9-10,2015,9,10,19,3kdnn2,Richest Prime Ministers of the World,https://www.reddit.com/r/MachineLearning/comments/3kdnn2/richest_prime_ministers_of_the_world/,askebuddy11,1441880182,[removed],0,1
212,2015-9-10,2015,9,10,20,3kdxsn,Curated list of neural network papers,https://www.reddit.com/r/MachineLearning/comments/3kdxsn/curated_list_of_neural_network_papers/,TristanDL,1441885749,,4,144
213,2015-9-10,2015,9,10,22,3ke7as,Best way of regularizing recurrent neural network architectures?,https://www.reddit.com/r/MachineLearning/comments/3ke7as/best_way_of_regularizing_recurrent_neural_network/,negazirana,1441890052,"Dear r/MachineLearning,

I recently started playing with recurrent architectures such as vanilla RNNs, LSTMs and GRUs on various NLP tasks, similarly to the work by Collobert et al. in [1]. In particular, I've seen GRUs work amazingly well on several, non-trivial NLP tasks.

My main question is the following: after some time, during training, the validation error starts increasing (and early stopping turns out to be highly beneficial).

What's the best way to prevent this to happen? I've tried plugging Dropout in various places, with highly varying results: my impression is that it's harmful to use it on the recurrent layer, and can yield some (small) benefit on non-recurrent layers.

What is the best guideline on regularizing recurrent neural architectures?

[1] http://ronan.collobert.com/pub/matos/2011_nlp_jmlr.pdf


Thank you in advance guys, you're amazing.",5,7
214,2015-9-10,2015,9,10,23,3kemc9,From search to distributed computing to large-scale information extraction,https://www.reddit.com/r/MachineLearning/comments/3kemc9/from_search_to_distributed_computing_to/,gradientflow,1441896076,,0,1
215,2015-9-11,2015,9,11,0,3kes2e,"Fields Review paper on Statistical Inference, Learning and Models in Big Data /xpost bigdata",https://www.reddit.com/r/MachineLearning/comments/3kes2e/fields_review_paper_on_statistical_inference/,FieldsInstitute,1441898119,"This year we held a [Thematic Program on Statistical Inference, Learning and Models in Big Data](http://www.fields.utoronto.ca/programs/scientific/14-15/bigdata/index.html) and the organizers have released a paper with some of their findings [here](http://arxiv.org/ftp/arxiv/papers/1509/1509.02900.pdf)
If you want to see another of their reports on the program you can read about it in our free newsletter [here](http://www.fields.utoronto.ca/aboutus/annual_reports/FieldsNotesSummer2015.pdf)
",0,2
216,2015-9-11,2015,9,11,0,3keymu,26 Things I Learned in the Deep Learning Summer School,https://www.reddit.com/r/MachineLearning/comments/3keymu/26_things_i_learned_in_the_deep_learning_summer/,alexeyr,1441900388,,12,103
217,2015-9-11,2015,9,11,1,3kf81t,RNN generating random output on Haiku Data. Suggestions ?,https://www.reddit.com/r/MachineLearning/comments/3kf81t/rnn_generating_random_output_on_haiku_data/,napsternxg,1441903751,"I am trying to train a char-rnn model on a Haiku corpus (176k chars,70 epochs,10000 batch size). However my model generates random chars even after training for 70 epochs (I don't have a GPU so training for 70 epochs using my settings took more than 2 days)

I reach a keras reported loss close to 0.002 after 70 epochs.

What should I do to improve my model performance. I have a few things in mind but want to consult before running my experiments again. I am planning the following:

1. Reducing number of hidden units to 100 (512 currently) in both the LSTM layers. Because the data is small this may help in fitting the data well.
2. Increasing the memory to last 100 characters (as suggested in Andrej Karpathy's blog). This will reduce the number of sequences as I consider sequences only inside a single Haiku. 
3. Train with only 1 layer of LSTM. I don't know how effective this will be but will surely reduce computation time.

Can someone suggest what is the best strategy going forward? Also, what measures people take when training RNN's on small data sets like mine ?

My current code uses Keras based 2 Layer LSTM with 512 hidden units each and has a memory of 20 characters.

The output of my model can be found at: https://gist.github.com/napsternxg/824c25188627d7266f32#file-output-txt

Code for training my model can be found at:
https://github.com/napsternxg/haiku_rnn/blob/master/haiku_gen.py",5,7
218,2015-9-11,2015,9,11,2,3kfea3,Computing with Artificial Spiking Neurons,https://www.reddit.com/r/MachineLearning/comments/3kfea3/computing_with_artificial_spiking_neurons/,reidhoch,1441905881,,3,25
219,2015-9-11,2015,9,11,2,3kfkg5,Journal article clustering - references? experiences?,https://www.reddit.com/r/MachineLearning/comments/3kfkg5/journal_article_clustering_references_experiences/,[deleted],1441907956,[deleted],0,0
220,2015-9-11,2015,9,11,3,3kft8b,"Diabetic Retinopathy Winner's Interview: 1st place, Ben Graham",https://www.reddit.com/r/MachineLearning/comments/3kft8b/diabetic_retinopathy_winners_interview_1st_place/,Shishaddict,1441910851,,5,7
221,2015-9-11,2015,9,11,4,3kg1zy,What is the current method of prediction for Multivariate and Heterogeneous Time Series?,https://www.reddit.com/r/MachineLearning/comments/3kg1zy/what_is_the_current_method_of_prediction_for/,augustus2010,1441913827,,4,0
222,2015-9-11,2015,9,11,5,3kgfaj,How to optimize hyper-parameters for deep neural networks?,https://www.reddit.com/r/MachineLearning/comments/3kgfaj/how_to_optimize_hyperparameters_for_deep_neural/,regularized,1441918442,Can you point out some recent papers on hyper-parameter optimization for neural networks? I started from this paper (Practical Bayesian Optimization of Machine Learning Algorithms) http://arxiv.org/abs/1206.2944 but I am not aware of recent developments. What is the current state of this research?,12,11
223,2015-9-11,2015,9,11,6,3kgj6i,State of one-shot learning?,https://www.reddit.com/r/MachineLearning/comments/3kgj6i/state_of_oneshot_learning/,asymptotics,1441919810,I've read that one-shot learning is still a work in progress and most successful CV algorithms still rely on massive image databases. Have there been any recent improvements?,6,4
224,2015-9-11,2015,9,11,6,3kgmdn,Building Featureful Machine Learning Models,https://www.reddit.com/r/MachineLearning/comments/3kgmdn/building_featureful_machine_learning_models/,dabshitty,1441921071,,0,2
225,2015-9-11,2015,9,11,6,3kgnqc,Multivariate statistics or Stochastic simulation for ML ?,https://www.reddit.com/r/MachineLearning/comments/3kgnqc/multivariate_statistics_or_stochastic_simulation/,rishok,1441921582,"Hi guys

I have to pick between two statistics courses, and would like to know which is more / most relevant for Machine learning:

1) Multivariate Statistics (13 weeks course)

2) Stochastic Simulation (3 weeks course)",1,3
226,2015-9-11,2015,9,11,7,3kgpxp,MIT paper on the NEAT algorithm,https://www.reddit.com/r/MachineLearning/comments/3kgpxp/mit_paper_on_the_neat_algorithm/,YoboStudios,1441922494,,0,7
227,2015-9-11,2015,9,11,8,3kh2tk,Google Prediction API provides a RESTful interface to build Machine Learning models,https://www.reddit.com/r/MachineLearning/comments/3kh2tk/google_prediction_api_provides_a_restful/,honoff,1441928206,,0,6
228,2015-9-11,2015,9,11,11,3khnjd,IMPRESSIVE! Continuous control with deep reinforcement learning,https://www.reddit.com/r/MachineLearning/comments/3khnjd/impressive_continuous_control_with_deep/,transhumanist_,1441937787,,13,33
229,2015-9-11,2015,9,11,15,3kidbd,Factorization Machines for Large Scale Learning,https://www.reddit.com/r/MachineLearning/comments/3kidbd/factorization_machines_for_large_scale_learning/,mattbwilson,1441951864,,2,4
230,2015-9-11,2015,9,11,16,3kimkj,Neural Machine Translation of Rare Words with Subword Units,https://www.reddit.com/r/MachineLearning/comments/3kimkj/neural_machine_translation_of_rare_words_with/,alexeyr,1441958337,,1,10
231,2015-9-11,2015,9,11,18,3kito2,Character recognition features?,https://www.reddit.com/r/MachineLearning/comments/3kito2/character_recognition_features/,tonytony999,1441963881,"For anyone who's built programs for digit or character recognition, what features did you find worked well?",3,6
232,2015-9-11,2015,9,11,19,3kiygh,Generating Captions - Describing Videos with Neural Networks,https://www.reddit.com/r/MachineLearning/comments/3kiygh/generating_captions_describing_videos_with_neural/,samim23,1441967327,,1,15
233,2015-9-11,2015,9,11,21,3kj6u7,DazzleBug: Help evolve camouflage for Cambridge University (online game with genetic programming),https://www.reddit.com/r/MachineLearning/comments/3kj6u7/dazzlebug_help_evolve_camouflage_for_cambridge/,nebogeo,1441973118,,14,32
234,2015-9-11,2015,9,11,22,3kjh6p,Does it make sense to jitter to test for overfitting?,https://www.reddit.com/r/MachineLearning/comments/3kjh6p/does_it_make_sense_to_jitter_to_test_for/,i_bobr,1441978643,,1,12
235,2015-9-11,2015,9,11,23,3kjl6f,Generate sentences word by word,https://www.reddit.com/r/MachineLearning/comments/3kjl6f/generate_sentences_word_by_word/,rs9000i,1441980516,"Hi guys, 
I tried to use Nervana neon framework to generate sentences.
I'm interested in this example https://github.com/NervanaSystems/neon/blob/master/examples/word_lstm.py 
After running the train, how  I can save the network and generate a random sentence to test it?",2,9
236,2015-9-11,2015,9,11,23,3kjla3,The Bayes Classifier: building a tweet sentiment analysis tool,https://www.reddit.com/r/MachineLearning/comments/3kjla3/the_bayes_classifier_building_a_tweet_sentiment/,[deleted],1441980566,[deleted],0,1
237,2015-9-11,2015,9,11,23,3kjlio,Question on A Problem from Michael Nielson's Chapter 6 of Neural Networks and Deep Learning?,https://www.reddit.com/r/MachineLearning/comments/3kjlio/question_on_a_problem_from_michael_nielsons/,LegacyAngel,1441980677,"Hey guys, I was going through this book to learn a bit more on neural network implementation. 

In case you aren't familiar with what I am referencing, here is the link: http://neuralnetworksanddeeplearning.com/chap6.html

The problem at hand was: ""The idea of convolutional layers is to behave in an invariant way across images. It may seem surprising, then, that our network can learn more when all we've done is translate the input data. Can you explain why this is actually quite reasonable?""

So I understand that each feature map abstracts away some the translation variant information by the shared weight system. Translating an image would then seem like the network was essentially trained on the same data. I don't really know how to start answering this question. 

My thinking is then that the convolution layer might not learn all of the relevant translation variant information. Translating the input images also mixes up the receptive fields, so the diversity of the receptive fields gets larger. Is this along the right track?",3,9
238,2015-9-11,2015,9,11,23,3kjnzz,The Bayes Classifier: building a tweet sentiment analysis tool,https://www.reddit.com/r/MachineLearning/comments/3kjnzz/the_bayes_classifier_building_a_tweet_sentiment/,asdrubaldon,1441981769,,0,1
239,2015-9-12,2015,9,12,0,3kju6l,Neural Networks to Tackle the Impenetrable Resting Bitch Face Conundrum,https://www.reddit.com/r/MachineLearning/comments/3kju6l/neural_networks_to_tackle_the_impenetrable/,KeponeFactory,1441984396,,9,35
240,2015-9-12,2015,9,12,0,3kjv1i,What Construction Brand is your favorite?,https://www.reddit.com/r/MachineLearning/comments/3kjv1i/what_construction_brand_is_your_favorite/,madisonrmcmahon,1441984750,,0,0
241,2015-9-12,2015,9,12,1,3kk29n,T-SNE question,https://www.reddit.com/r/MachineLearning/comments/3kk29n/tsne_question/,swampsofjersey,1441987637,"does anyone have an idea why each time I run this code I am given a different result for the 2 dimensional vector? I am trying to plot some words with word2vec and running into a bit of a wall. is this the correct sub for such questions?

http://pastebin.com/bvTuzZjH
",5,2
242,2015-9-12,2015,9,12,1,3kk5et,Are there Python examples of using Caffe with simple data arrays?,https://www.reddit.com/r/MachineLearning/comments/3kk5et/are_there_python_examples_of_using_caffe_with/,d3pd,1441988892,Let's say I have some simple NumPy data arrays. Are there examples in Python of training on such arrays and then classifying them?,3,1
243,2015-9-12,2015,9,12,2,3kkdiw,[Question] Building Question&amp;Answer system with help of NTLK ?,https://www.reddit.com/r/MachineLearning/comments/3kkdiw/question_building_questionanswer_system_with_help/,__Julia,1441992131,"I am trying to build  Question &amp; Answer system using NTLK, is there any paper, book or project that you recommend checking ?

I have been trying to understand the fundamentals in order to build this project, but I am still lost  ;) Anyone to help ?  ",3,3
244,2015-9-12,2015,9,12,3,3kkrym,"If we build one of Yuli's Sensory Orbs, maybe we could create a legitimate AGI",https://www.reddit.com/r/MachineLearning/comments/3kkrym/if_we_build_one_of_yulis_sensory_orbs_maybe_we/,[deleted],1441997953,[deleted],3,0
245,2015-9-12,2015,9,12,5,3kl1d4,Parts of speech tagging in R language,https://www.reddit.com/r/MachineLearning/comments/3kl1d4/parts_of_speech_tagging_in_r_language/,ChotiDon,1442001959,I am working with roman urdu. I have a .txt file that contains all urdu words with their POS tags(say file A). I have another text file (say file B) that contains text in roman urdu. I want to assign POS tags to words of file B using using file A. Please help.,4,4
246,2015-9-12,2015,9,12,5,3kl2nu,"I fit a word2vec model on some ESPN.com data, and was really excited about my results...",https://www.reddit.com/r/MachineLearning/comments/3kl2nu/i_fit_a_word2vec_model_on_some_espncom_data_and/,[deleted],1442002470,[deleted],0,0
247,2015-9-12,2015,9,12,6,3klazi,"Best strategy for optimizing a content recommender: considering supervised vs unsupervised approaches, and hyper-parameter optimization.",https://www.reddit.com/r/MachineLearning/comments/3klazi/best_strategy_for_optimizing_a_content/,AlexTHawk,1442005871,"I am working on a recommendation engine that promotes things like recipes, and blog articles on a client's website.  Presently, the engine blends a variety of unsupervised ranking strategies to generate recommendations.  Specifically, it blends a document similarity score (based on TF-IDF / Jaccard similarity) with relevance scores from user based and item based collaborative filtering models.  AB tests on client's sites show the engine gives a definitive lift, so I know something is working. But it's unclear what the next best step is to improve the recommender.

One simple option is to optimize the hyper-parameters (of which there 17) for the current recommender.  Grid search is clearly not an option, but iteratively optimizing 1 hyper-parameter at a time until convergence is accessible.  The metric I would optimize would be the out-of-time NDCG.  

A more fruitful option however might be to make use of my supervised data --i.e. the examples where the engine made recommendations, and the user clicked on one of the options.  For example, I could feed the indicators from the item based collaborative filtering model into AdaRank (a supervised ranking algorithm) as features.  Similarly, I could feed in the user cluster derived from the user based collaborative filtering model as a feature, and the topics each document is textually associated with (according to LDA or something of the sort) as well.  The problem with using a supervised algorithm is that click data is sparse compared to the unsupervised data (i.e. the data on how visitors to the client site naturally traverse the site). 

What option (above, or not above), seems the most fruitful?  And what tips might you have for success in pursuing them?  E.g. how should i deal with the sparsity of click data? Should I blend the relevance scores of the supervised model with the scores of the unsupervised model in proportion to the amount of click data that i have? Is AdaRank one of the best performing / most accessible supervised algorithms for content recommendation?  Perhaps I should focus on using a more sophisticated document similarity scoring method...based on DocToVec or something like that? I'd greatly appreciate any suggestions, thoughts, or criticisms!  ",3,2
248,2015-9-12,2015,9,12,8,3klqdh,[Q] What's the difference between cross-entropy and mean-squared-error loss functions?,https://www.reddit.com/r/MachineLearning/comments/3klqdh/q_whats_the_difference_between_crossentropy_and/,__AndrewB__,1442012527,"When training neural networks one can often hear that cross entropy is a better cost function than mean squared error. 

Why is it the case if both have the same derivatives and therefore lead to the same updates in weights?",7,17
249,2015-9-12,2015,9,12,8,3klsdg,"End of the road for journalists? Tencent's Robot reporter 'Dreamwriter' churns out perfect 1,000-word news story - in 60 seconds",https://www.reddit.com/r/MachineLearning/comments/3klsdg/end_of_the_road_for_journalists_tencents_robot/,DamonVryce,1442013450,,27,23
250,2015-9-12,2015,9,12,9,3km3ow,I made a Self-Organizing Map of Porn. So far no luck explaining what that is on /r/dataisbeautiful. ): [OC],https://www.reddit.com/r/MachineLearning/comments/3km3ow/i_made_a_selforganizing_map_of_porn_so_far_no/,[deleted],1442018986,[deleted],3,0
251,2015-9-12,2015,9,12,16,3kn2ny,A Systems View of Machine Learning (Josh Bloom: Keynote - PyData 2015),https://www.reddit.com/r/MachineLearning/comments/3kn2ny/a_systems_view_of_machine_learning_josh_bloom/,[deleted],1442041901,[deleted],0,1
252,2015-9-12,2015,9,12,17,3kn966,[Question #ML #DataScience]: How can I build Question &amp; Answer system ?,https://www.reddit.com/r/MachineLearning/comments/3kn966/question_ml_datascience_how_can_i_build_question/,__Julia,1442047746,"I am trying to build Question &amp; Answer system, is there any paper, book or project that you recommend checking ?
I have been trying to understand the fundamentals in order to build this project, but I am still lost ;) Anyone to help ?",3,0
253,2015-9-12,2015,9,12,18,3kndji,"If performing a many-dimensioned regression (i.e. vector regression) with a recurrent neural network, but are the recommended activation for the output layer and loss function?",https://www.reddit.com/r/MachineLearning/comments/3kndji/if_performing_a_manydimensioned_regression_ie/,[deleted],1442051851,[deleted],0,1
254,2015-9-12,2015,9,12,19,3knexn,Question: when performing a regression to predict output vectors with a recurrent neural network - what are the best activation for the output layer and loss function to train towards,https://www.reddit.com/r/MachineLearning/comments/3knexn/question_when_performing_a_regression_to_predict/,[deleted],1442053096,"I'm currently using mean-square error and softmax, but I wondered if someone knew an orthodox solution to what I'm doing (I have not come across any seq-to-seq research which quite the same type of outputs). I'm training against a set of output vectors of which the vast majority of values are zero. To explain further: for vectors I'm not interested in learning anything about for the output (but which contribute in the input sequence), the values are all zero. For vectors I do want to predict an output for, the values sum to one, e.g.:

    0.0 0.0 0.0 0.0 0.2 0.2 0.2 0.2 0.2 0.0 0.0 0.0 0.0 0.0

These vectors often have a value of 1 at one dimension, and zero everywhere else, e.g.:

    0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0

What I'm doing just now works, but I wondered if anyone had tried something else successfully, or if there was a more orthodox solution to this - using a softmax for a regression does not seem typical (although it has produced predictive results for me), in that a softmax would usually be used for classification probabilities with a cross entropy loss, rather than mean-square error.",1,0
255,2015-9-12,2015,9,12,19,3knezi,"Predictive Analytics, NLP Flag Psychosis with 100% Accuracy",https://www.reddit.com/r/MachineLearning/comments/3knezi/predictive_analytics_nlp_flag_psychosis_with_100/,Cynthiaay_Moises5,1442053137,,0,1
256,2015-9-12,2015,9,12,20,3kni5i,"WebSight, Computer Vision on the cloud!",https://www.reddit.com/r/MachineLearning/comments/3kni5i/websight_computer_vision_on_the_cloud/,WebSightVision,1442055942,,1,0
257,2015-9-12,2015,9,12,20,3knj6x,Latent Dirichlet Allocation (LDA) result metric,https://www.reddit.com/r/MachineLearning/comments/3knj6x/latent_dirichlet_allocation_lda_result_metric/,[deleted],1442056815,[removed],0,1
258,2015-9-12,2015,9,12,21,3knpb6,[Q] What metric can be used to compare Latent Dirichlet Allocation (LDA) results?,https://www.reddit.com/r/MachineLearning/comments/3knpb6/q_what_metric_can_be_used_to_compare_latent/,alexperrier,1442061772,"Blog posts about LDA usually end with the author (myself included) looking at the main resulting topics of LDA and claiming that they are relevant.

I have also been playing around with LDA for topic Modeling of my twitter followers (http://t.co/hqOq75CFJx) and I'm not really satisfied with the results. I feel there has to be a more scientific way of assessing the quality of LDA results than simply looking at the main topics.

What kind of metric could I use to compare two LDA results trained on the same corpus and assess if one is better than the other?
",0,2
259,2015-9-12,2015,9,12,21,3knqff,word2vec &amp; doc2vec: Modern NLP Sentiment Analysis in Python,https://www.reddit.com/r/MachineLearning/comments/3knqff/word2vec_doc2vec_modern_nlp_sentiment_analysis_in/,cast42,1442062633,,9,79
260,2015-9-12,2015,9,12,22,3knrki,Character recognition Data Pre Processeing,https://www.reddit.com/r/MachineLearning/comments/3knrki/character_recognition_data_pre_processeing/,CecilStan,1442063290,"Guys, what are the top things i can do to 'clean up' my image data (hand written letters) before using some learning algorithm for character recognition? Im doing a  very basic assignment

it seems each image would have to be the same size. but what about the actual size of the letters in the image...do i have to perform some type of scaling (crop off blank areas around small letters and enlarge the image so tht its similar size to the larger letters? or is it a GOOD thing to have some letter be small and others big

what are the the best preprocessing methods that will lead to improved results? my data is completely raw (black and white images) Thanks!",2,1
261,2015-9-12,2015,9,12,22,3knsia,Speech Recognition data pre-processing,https://www.reddit.com/r/MachineLearning/comments/3knsia/speech_recognition_data_preprocessing/,CecilStan,1442063889,"I made a thread earlier but also forgot to ask about this data set.

I have sound signals of alphabets and single digit numbers being recited by different speakers. what are some basic things i need to do to preprocess the data before using a learning algorithm for speech recognition. Should i make all the signals 0 mean and normalize so that they're in the -1 to 1 range? if the speakers speak clearly and there is not really a noise issue for both training and testing sets should i just leave the data alone(in terms of filtering)?

how about the 'time' axis of the signals..what do i do to normalize all the signals. This is what im having the most difficulty figuring out

",2,6
262,2015-9-13,2015,9,13,8,3kpy3k,Startup Idea: Online hosted neural network editer.,https://www.reddit.com/r/MachineLearning/comments/3kpy3k/startup_idea_online_hosted_neural_network_editer/,mettama,1442099571,"We are currently participating in a Startup Weekend and are developing an online based neural network editor. It's basically version control for machine learning, including history of commits, a GUI, and easy parameter tweaking/visualization. Users would also be able to share/collaborate with others. 

We're implementing one for neural networks first, but eventually we will scale to other algorithms.  

Would this be of use to you? We are launching an Indiegogo as a proof of concept. Give us some feedback!",8,0
263,2015-9-13,2015,9,13,9,3kq9sd,The Fallacy of Placing Confidence in Confidence Intervals,https://www.reddit.com/r/MachineLearning/comments/3kq9sd/the_fallacy_of_placing_confidence_in_confidence/,[deleted],1442105325,[deleted],0,0
264,2015-9-13,2015,9,13,11,3kqms3,ML Algorithms for making predictions when you have a Real Number output variable?,https://www.reddit.com/r/MachineLearning/comments/3kqms3/ml_algorithms_for_making_predictions_when_you/,MusicIsLife1995,1442112116,"I'm working with a lot of features (about 30ish) or so. My output is a real number. I've so far tried Linear Regression and have achieved about 66% accuracy but I'm sure 70% or higher is very possible (from research). 

What can I use/apply instead of linear regression?

edit: I'm measuring accuracy by checking to see if the result has the same sign as the actual value. I tried using an SVM/logistic regression but it didn't work as well since my data isn't exactly linearly separable. ",16,0
265,2015-9-13,2015,9,13,15,3kr7dc,Interesting take-aways from Data Science For Business,https://www.reddit.com/r/MachineLearning/comments/3kr7dc/interesting_takeaways_from_data_science_for/,sachinrjoglekar,1442124906,,6,22
266,2015-9-13,2015,9,13,15,3kr7uv,Going Deeper with Convolutions (pre-trained network for the Inception model by google),https://www.reddit.com/r/MachineLearning/comments/3kr7uv/going_deeper_with_convolutions_pretrained_network/,samim23,1442125261,,4,1
267,2015-9-13,2015,9,13,22,3ks22a,Possible approaches for writing a music generating actor,https://www.reddit.com/r/MachineLearning/comments/3ks22a/possible_approaches_for_writing_a_music/,sciss_,1442149387,"Hi,

I have a signal-graph based (modular) improvisation system for electronic music. I.e., there are generating, filtering etc. modules that can be patched together and parametrised.

I would like to experiment with simple actors that create and connect these kind of graphs by themselves. Possibly through learning by example, possibly ""blindly"" by something like reinforced learning. In the latter case, I think I would need to assign rewards manually in the training phase, as it is almost impossible to formalise the kind of sounds that are acceptable or interesting and those that I would like to avoid.

I am going though the book of Sutton and Barto, but my resources at the moment are very limited, so I would prefer to get started with any sort of basic implementation no matter how good or bad.

Therefore, I am grateful for a) any pointers to simple algorithms or libraries (preferably running on the JVM) that I could exploit. b) ideas about how to define the state and value structure in this case if I stick to RL. The state seems particular difficult to me, as the world is a very generic directed acyclic graph with many different modules and quasi-continuous parameters. I guess I'll have to break it down to something much more simple.

Thanks!

__Edit:__ Title should read: ""music generating __agent__""",2,8
268,2015-9-13,2015,9,13,22,3ks370,"Anyone on Reddit have experience with Apache UIMA, and the best way to get started using it?, I want to build a WATSON like system. https://www.ibm.com/developerworks/mydeveloperworks/blogs/InsideSystemStorage/entry/ibm_watson_how_to_build_your_own_watson_jr_in_your_basement7?lang=en&amp;CE=ISM0008",https://www.reddit.com/r/MachineLearning/comments/3ks370/anyone_on_reddit_have_experience_with_apache_uima/,Rainfox,1442150112,,5,4
269,2015-9-13,2015,9,13,22,3ks77x,Gradient Boosted Regression Trees with sklearn notebook tutorial,https://www.reddit.com/r/MachineLearning/comments/3ks77x/gradient_boosted_regression_trees_with_sklearn/,cast42,1442152522,,1,24
270,2015-9-14,2015,9,14,0,3kskks,Did someone tried to find important patents by training?,https://www.reddit.com/r/MachineLearning/comments/3kskks/did_someone_tried_to_find_important_patents_by/,fimari,1442159104,"Like training on actual licensed patents and search for similar patents? 

Alternatively trolling patent offices with Char-RNN generated fake patents also counts ;-)",7,0
271,2015-9-14,2015,9,14,1,3ksnfx,Previous work in using clustering to manage user-generated labels?,https://www.reddit.com/r/MachineLearning/comments/3ksnfx/previous_work_in_using_clustering_to_manage/,ddttox,1442160376,I'm looking at a storage system that allows users to label / tag entities contained in it (for the sake of this post let's say images) with any text string their little heart desires.  I would like to have the computer take a cut at creating semantic groups of these labels / tags so that a human can start to build an ontology from them.  I can't imagine that I'm the first to think of this but have been unable to find any good references.  Could someone point me in the right direction? ,1,5
272,2015-9-14,2015,9,14,1,3ksu6b,Thoughts on a Masters in Big Data from GalvanizeU?,https://www.reddit.com/r/MachineLearning/comments/3ksu6b/thoughts_on_a_masters_in_big_data_from_galvanizeu/,lp0_on_fire_,1442163242,"I'm a recent graduate with a B.S. in Computer Engineering. I did most of my work on embedded systems throughout my bachelors but discovered that I really, really enjoyed A.I. and, specifically, Computer/Machine Vision.  I'm currently working for the government doing uninteresting work and through MeetUpss and networking in the Bay Area, I discovered [Galvanize U] (http://www.galvanizeu.com). The TL;DR of the program is that it's a one-year masters degree that they've just launched through the University of New Haven (previously Galvanize has only done immersive boot camps) to the tune of about $65k. I've been the campus and met the professors and I'm very impressed and hopeful at the moment, but as the program is brand new, they have no alumni yet, and so, no data on placement rate, salaries, etc. 

The problem is that I have no way to evaluate how good this program is w.r.t. a traditional approach. I'm the first person in my family to have ever even gotten a B.S., nonetheless shoot for a Masters. The very few friends I do have with master's degrees have them in completely unrelated fields, so it's hard been impossible to compare their experiences with what Galvanize is offering. I'm looking at either starting with Galvanize this coming January, or holding out and *trying* to *maybe* get into a campus like Stanford or University of San Francisco. Galvanize boasts impressive connections and alumni from it's boot camp in lots of great companies, but I'm terrified that I could walk away with this and still not be taken seriously as a data scientist. 

What's reddit's opinion on these types of programs? Am I in good hands, or should I save my time and take a super long shot at Stanford?",6,2
273,2015-9-14,2015,9,14,4,3ktk4c,Feature reduction and Boosting: what's the right order of things?,https://www.reddit.com/r/MachineLearning/comments/3ktk4c/feature_reduction_and_boosting_whats_the_right/,surangak,1442174165,"Hi folks,

I have a dataset that badly needs (a) dimensionality reduction to get rid of the excess attributes and (b) boosting / SMOTE to address dataset imbalance.

My question is, whats the right order to do this? based on my own understanding and what i've read/tried out, I feel as if I need to boost my data to fix the imbalance first, and then apply a feature selection approach (maybe infogain) to pick the best tokens. I feel that if I don't boost first, i'm going to loose some tokens that are essential for representing the imbalanced outcome. 

Any suggestions? 
",6,1
274,2015-9-14,2015,9,14,6,3kty2r,old news: kaggle is incompetent,https://www.reddit.com/r/MachineLearning/comments/3kty2r/old_news_kaggle_is_incompetent/,[deleted],1442180125,[deleted],2,1
275,2015-9-14,2015,9,14,7,3ku3ja,Big Data Specialization at Coursera (Starting in 2 days),https://www.reddit.com/r/MachineLearning/comments/3ku3ja/big_data_specialization_at_coursera_starting_in_2/,[deleted],1442182578,[deleted],4,27
276,2015-9-14,2015,9,14,9,3kul05,Test Set Performance Is Negatively Correlated With Validation Set ?!,https://www.reddit.com/r/MachineLearning/comments/3kul05/test_set_performance_is_negatively_correlated/,ML_CBG,1442190878,"I am working on a dataset with a particularly high noise / signal ratio, and after doing some analysis I have found that test set performance is negatively correlated with validation set performance. Has anyone experienced this before or can offer any interpretation?

My current idea is something involving overfitting to the validation set.",4,1
277,2015-9-14,2015,9,14,10,3kup66,Visualization of a neural network evolving a B2D converter.,https://www.reddit.com/r/MachineLearning/comments/3kup66/visualization_of_a_neural_network_evolving_a_b2d/,GregTJ,1442192973,,12,9
278,2015-9-14,2015,9,14,11,3kuwwc,Most flexible library to build LSTM networks with Torch,https://www.reddit.com/r/MachineLearning/comments/3kuwwc/most_flexible_library_to_build_lstm_networks_with/,kmnns,1442196860,"A lot has happened in the recent months concerning implementations of LSTMs for various frameworks, especially Torch. But what is the best LSTM library / base implementation to get off the ground? Two important requirements:

1. It should play nicely with a larger more common framework, such as `nn` and `dp`, i.e. a subclass of `nn.module`. I want to put an LSTM layer after or before a sigmoid layer.

2. It should be easily modifiable. Some implementations are rigidly tuned to the vanilla LSTM topology: one gate per cell, no block-internal synergies between the cells (i.e. weight vectors for peepholes, not matrices), no recurrent feedback connections to lower layers in a deep network.

I believe the most promising library is this:

https://github.com/Element-Research/rnn

What do you think? Do you have a hot tip for me here?

Other notable implementations (somehow based off each other)

- https://github.com/wojzaremba/lstm
- https://github.com/oxford-cs-ml-2015/practical6/blob/master/LSTM.lua
- https://github.com/karpathy/char-rnn/blob/master/model/LSTM.lua

UPDATE December 2015: my journey: for anyone looking for the best Torch RNN framework:

1. Tried Nicholas' module (https://github.com/Element-Research/rnn) based on the example code, but all examples are tightly embedded with the DP package, which doesn't support all your crazy experimental setups. I was pressed to deliver fast results, so I went for another quick-off-the-ground code base.

2. Karpathy's code &amp; example (https://github.com/karpathy/char-rnn) provided all the preprocessing code, allowed quick deployment and easy low-level changes. But surprise surprise, it suffers from major memory problems for longer sequences due to its cloning hack.

3. Back to Nicholas' module (https://github.com/Element-Research/rnn). Reimplemented my net, without the DP package, subclassing the LSTM as proposed by The Nicholas Himself. Super-long sequences, very fast, small memory, much wow.

Tl;dr:

- Karpathy's preprocessing and Nicholas' network is the killer combi.
- Nicholas' DP package is great, but your special experimental setup might not be supported.",2,7
279,2015-9-14,2015,9,14,15,3kvl58,Machine Learning Trick of the Day: Hutchinsons Trick,https://www.reddit.com/r/MachineLearning/comments/3kvl58/machine_learning_trick_of_the_day_hutchinsons/,iori42,1442210642,,20,68
280,2015-9-14,2015,9,14,15,3kvlgg,[1509.03475] Hessian-Free Optimization For Learning Deep Multidimensional Recurrent Neural Networks,https://www.reddit.com/r/MachineLearning/comments/3kvlgg/150903475_hessianfree_optimization_for_learning/,iori42,1442210826,,1,17
281,2015-9-14,2015,9,14,17,3kvvk8,What is your specialty as a C-arm manufacturer India?,https://www.reddit.com/r/MachineLearning/comments/3kvvk8/what_is_your_specialty_as_a_carm_manufacturer/,anniejohar,1442218254,[removed],0,0
282,2015-9-14,2015,9,14,19,3kw7f5,Splitting of continuous variables for Regression Tree,https://www.reddit.com/r/MachineLearning/comments/3kw7f5/splitting_of_continuous_variables_for_regression/,[deleted],1442227219,[deleted],0,2
283,2015-9-14,2015,9,14,23,3kwv1v,Giraffe: Using Deep Reinforcement Learning to Play Chess,https://www.reddit.com/r/MachineLearning/comments/3kwv1v/giraffe_using_deep_reinforcement_learning_to_play/,merlin0501,1442240349,,16,64
284,2015-9-14,2015,9,14,23,3kwwl7,"Solutions for ""Machine Learning: A Probabilistic View""'s Exercises?",https://www.reddit.com/r/MachineLearning/comments/3kwwl7/solutions_for_machine_learning_a_probabilistic/,Arssal,1442241054,Are there anywhere solution for Kevin Murphys book's exercises out there? I am trying to do all the exercises on my own but sometimes I am not sure if my solutions are correct or I do not come to a solution at all.,1,8
285,2015-9-14,2015,9,14,23,3kwzsj,Machine Learning workshops using Python and Spark in New York,https://www.reddit.com/r/MachineLearning/comments/3kwzsj/machine_learning_workshops_using_python_and_spark/,dabshitty,1442242518,,0,5
286,2015-9-15,2015,9,15,0,3kx0tn,Scalable Distributed DNN Training Using Commodity GPU Cloud Computing,https://www.reddit.com/r/MachineLearning/comments/3kx0tn/scalable_distributed_dnn_training_using_commodity/,iori42,1442242962,,4,18
287,2015-9-15,2015,9,15,1,3kxbux,A Statistical View of Deep Learning: Retrospective,https://www.reddit.com/r/MachineLearning/comments/3kxbux/a_statistical_view_of_deep_learning_retrospective/,j_juggernaut,1442247643,,5,29
288,2015-9-15,2015,9,15,2,3kxjvz,Tool for annotating dataset?,https://www.reddit.com/r/MachineLearning/comments/3kxjvz/tool_for_annotating_dataset/,CashierHound,1442250853,"Does anyone know of a simple tool for annotating a dataset of images? I basically need to annotate ~500 images with a list of predefined labels. It's not a large enough dataset to outsource it to mechanical turk and it requires too much domain knowledge.

I just want to have the image display, click the appropriate labels, click next image and have it generate a CSV or something similar.

I'll probably roll my own in Python/Tk and post it here unless anyone knows of existing tools.",3,1
289,2015-9-15,2015,9,15,3,3kxs5k,A Library for Bayesian Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/3kxs5k/a_library_for_bayesian_reinforcement_learning/,pierrelux,1442254022,,0,5
290,2015-9-15,2015,9,15,6,3kymzx,"Multilayer RNNs and LSTMs with pycaffe, because I didn't want to learn Lua..",https://www.reddit.com/r/MachineLearning/comments/3kymzx/multilayer_rnns_and_lstms_with_pycaffe_because_i/,surelyouarejoking,1442266110,,7,46
291,2015-9-15,2015,9,15,7,3kysmv,Dataset of clustered and/or occluded objects?,https://www.reddit.com/r/MachineLearning/comments/3kysmv/dataset_of_clustered_andor_occluded_objects/,neurograce,1442268422,"Hi, I'm looking for where I can find a labeled dataset (doesn't have to be huge) of images each containing several objects, preferably in a naturalistic setting. If the objects overlap with ImageNet categories that would be great as well. Or if the dataset includes images of the objects alone. I'm thinking bounding box datasets would be good, but would like some input. Much thanks.",2,1
292,2015-9-15,2015,9,15,7,3kyvoz,Deeplearning4j on Spark,https://www.reddit.com/r/MachineLearning/comments/3kyvoz/deeplearning4j_on_spark/,vonnik,1442269767,,0,2
293,2015-9-15,2015,9,15,10,3kziz1,Machine learning ideas for my senior projects in software engineering,https://www.reddit.com/r/MachineLearning/comments/3kziz1/machine_learning_ideas_for_my_senior_projects_in/,shweel,1442280415,I would like to do something machine learning related for my senior project. I have 2 competent partners and intermediate experience in machine learning (I've done some Kaggle). Senior projects at my school consist of a groups of 3/4 people and  last around 9 months. Any ideas so I don't end up with an uninspired project?,4,6
294,2015-9-15,2015,9,15,11,3kzo0c,DAG-Recurrent Neural Networks For Scene Labeling,https://www.reddit.com/r/MachineLearning/comments/3kzo0c/dagrecurrent_neural_networks_for_scene_labeling/,SuperFX,1442282829,,0,3
295,2015-9-15,2015,9,15,13,3l0429,A survey of current datasets for vision and language research,https://www.reddit.com/r/MachineLearning/comments/3l0429/a_survey_of_current_datasets_for_vision_and/,Articulated-rage,1442290917,,1,11
296,2015-9-15,2015,9,15,16,3l0n0u,Model Accuracy and Runtime Tradeoff in Distributed Deep Learning,https://www.reddit.com/r/MachineLearning/comments/3l0n0u/model_accuracy_and_runtime_tradeoff_in/,[deleted],1442303845,[deleted],0,0
297,2015-9-15,2015,9,15,17,3l0naq,C-Arm Image Intensifier,https://www.reddit.com/r/MachineLearning/comments/3l0naq/carm_image_intensifier/,anniejohar,1442304100,[removed],0,1
298,2015-9-15,2015,9,15,19,3l0xsq,NIPS 2015 Accepted Papers: Some Preprints (x-post r/CompressiveSensing ),https://www.reddit.com/r/MachineLearning/comments/3l0xsq/nips_2015_accepted_papers_some_preprints_xpost/,compsens,1442312308,,0,6
299,2015-9-15,2015,9,15,19,3l10bp,Amazon Machine Learning: use cases and a real example in Python,https://www.reddit.com/r/MachineLearning/comments/3l10bp/amazon_machine_learning_use_cases_and_a_real/,fidelyo,1442314208,,0,3
300,2015-9-15,2015,9,15,21,3l1c6x,ConvNet or CNN? Which name to use?,https://www.reddit.com/r/MachineLearning/comments/3l1c6x/convnet_or_cnn_which_name_to_use/,yaolubrain,1442321514,"Which is the more appropriate abbreviation for Convolutional Neural Network? ConvNet or CNN?

ConvNet looks more informative. And some people dislike the 'Neural' term since Convolutional Neural Network doesn't work like the neuronal network in the brain. They just say Convolutional Network and therefore ConvNet.

But RNN is for Recurrent Neural Network. So for consistency, maybe we should also use CNN for Convolutional Neural Network.",4,0
301,2015-9-15,2015,9,15,23,3l1mmp,3 minute summary of The Empire Strikes Back (applying automatic text summarization to film),https://www.reddit.com/r/MachineLearning/comments/3l1mmp/3_minute_summary_of_the_empire_strikes_back/,icandoitbetter,1442326725,,28,72
302,2015-9-15,2015,9,15,23,3l1pxg,"Realtime Bidding: Predicting the future, 10,000 times per second (scroll down for embedded video)",https://www.reddit.com/r/MachineLearning/comments/3l1pxg/realtime_bidding_predicting_the_future_10000/,[deleted],1442328278,[deleted],0,1
303,2015-9-16,2015,9,16,0,3l1vp3,Semantic Tuples for Evaluation of Image to Sentence Generation,https://www.reddit.com/r/MachineLearning/comments/3l1vp3/semantic_tuples_for_evaluation_of_image_to/,bcnmovein,1442330778,,0,1
304,2015-9-16,2015,9,16,0,3l1vrn,A Gentle Guide to Machine Learning,https://www.reddit.com/r/MachineLearning/comments/3l1vrn/a_gentle_guide_to_machine_learning/,wildcodegowrong,1442330808,,4,32
305,2015-9-16,2015,9,16,0,3l1xcw,Quake vs. Picasso Mashup: Deep Style Networks Make Game Concept Art,https://www.reddit.com/r/MachineLearning/comments/3l1xcw/quake_vs_picasso_mashup_deep_style_networks_make/,alexjc,1442331461,,0,11
306,2015-9-16,2015,9,16,1,3l21oi,"How to implement the ""Downpour SGD""?",https://www.reddit.com/r/MachineLearning/comments/3l21oi/how_to_implement_the_downpour_sgd/,zlli0520,1442333206,"The algorithm is in the appendix of this paper, [Large Scale Distributed Deep Networks] (http://static.googleusercontent.com/media/research.google.com/en//archive/large_deep_networks_nips2012.pdf). 
There are two functions in the pseudocode called STARTASYNCHRONOUSLYFETCHINGPARAMETERS  and STARTASYNCHRONOUSLYPUSINGGRADIENTS. I don't know how to implement these two methods. When computing the gradients, the model is being used, how to fetch the parameters at the same time? Thanks.",3,3
307,2015-9-16,2015,9,16,1,3l25gd,[Video lecture] How we applied machine learning and game theory to build an ad targeting engine that could outperform Google AdWords,https://www.reddit.com/r/MachineLearning/comments/3l25gd/video_lecture_how_we_applied_machine_learning_and/,[deleted],1442334746,[deleted],0,1
308,2015-9-16,2015,9,16,2,3l2cpi,In-depth intro to Amazon Machine Learning,https://www.reddit.com/r/MachineLearning/comments/3l2cpi/indepth_intro_to_amazon_machine_learning/,asdrubaldone,1442337809,,1,7
309,2015-9-16,2015,9,16,3,3l2lnr,Questions about karpathy's torch char rnn implementation?,https://www.reddit.com/r/MachineLearning/comments/3l2lnr/questions_about_karpathys_torch_char_rnn/,[deleted],1442341454,[deleted],4,7
310,2015-9-16,2015,9,16,3,3l2mss,int to binary digits,https://www.reddit.com/r/MachineLearning/comments/3l2mss/int_to_binary_digits/,CecilStan,1442341893,"i want to do this:

A=00001
B=00010
C=00011
D=00100
E=00101
F=00110
G=00111
...
Z=...
etc. in matlab...is there a built in function for this?",2,0
311,2015-9-16,2015,9,16,3,3l2njo,A Biologically Plausible Computational Theory for Value Integration and Action Selection in Decisions with Competing Alternatives,https://www.reddit.com/r/MachineLearning/comments/3l2njo/a_biologically_plausible_computational_theory_for/,[deleted],1442342221,[deleted],0,0
312,2015-9-16,2015,9,16,3,3l2o1q,"The Ashley Madison Hack, Data Ethics &amp; Machine Learning",https://www.reddit.com/r/MachineLearning/comments/3l2o1q/the_ashley_madison_hack_data_ethics_machine/,marcoborracho,1442342434,,0,0
313,2015-9-16,2015,9,16,3,3l2pes,"Another rant about Rumelhart/Hinton/LeCun not 'inventing' BP in context of MLPs, just rewrote it",https://www.reddit.com/r/MachineLearning/comments/3l2pes/another_rant_about_rumelharthintonlecun_not/,nonap_,1442343007,,6,4
314,2015-9-16,2015,9,16,4,3l2s04,IRNN Results not matching with Paper,https://www.reddit.com/r/MachineLearning/comments/3l2s04/irnn_results_not_matching_with_paper/,pranv,1442344120,"Has anyone used IRNNs[1] as a replacement of LSTMs and achieved the claimed results? I am attempting sequence to sequence toy tasks (like simple encoding and decoding), but I have not been able to get IRNNs to work as well as LSTMs. Stacking many IRNNs on top of each other isn't helping either. I have used hyper parameters similar to the ones suggested in the paper (I don't have the luxury to perform grid searches.) Maybe my intuition around this thing is wrong. If you can share your experience or results or models or code, it would be great.

[1] http://arxiv.org/pdf/1504.00941.pdf",11,20
315,2015-9-16,2015,9,16,4,3l2vcr,Bayesian approach to compare hypotheses about human trails on the Web: Hyptrails,https://www.reddit.com/r/MachineLearning/comments/3l2vcr/bayesian_approach_to_compare_hypotheses_about/,cast42,1442345500,,1,14
316,2015-9-16,2015,9,16,4,3l2wds,[xpost from r/programming] How we used machine learning and game theory to build an ad targeting engine that could outperform Google Adwords,https://www.reddit.com/r/MachineLearning/comments/3l2wds/xpost_from_rprogramming_how_we_used_machine/,sanity,1442345921,,0,3
317,2015-9-16,2015,9,16,7,3l3gtv,Need some help understanding how to use the output of a neural network.,https://www.reddit.com/r/MachineLearning/comments/3l3gtv/need_some_help_understanding_how_to_use_the/,fourhoarsemen,1442354449,"Hi. I recently read this [article](http://iamtrask.github.io/2015/07/12/basic-python-network/) describing the forward/back-propagation steps in a toy problem. 

The author himself describes the problem like this: 

&gt; Consider trying to predict the output column given the three input columns.

The training data is: 

    [0, 0, 1, 0]
    [0, 1, 1, 0]
    [1, 0, 1, 1]
    [1, 1, 1, 1]
    
... where the fourth column is the output values, and each row is a training example. 

After training w/ 10000 or so iterations, the output is an array: 

    [ 0.00966449]
    [ 0.00786506]
    [ 0.99358898]
    [ 0.99211957]

The output, I presume, is the result of the algorithm tending towards the first column, which the author notes how the first column correlates with the last (output) column. 

My question is, given the last array, how would I use it to ""predict the output column given three input columns"" (as the author states we should be able to do)? ",7,0
318,2015-9-16,2015,9,16,7,3l3j9d,Understanding Machine Learning  An Interactive Coding Tutorial,https://www.reddit.com/r/MachineLearning/comments/3l3j9d/understanding_machine_learning_an_interactive/,gauss007,1442355540,,0,7
319,2015-9-16,2015,9,16,7,3l3m3b,"Deep Learning Machine Teaches Itself Chess in 72 Hours , Plays at International Master Level",https://www.reddit.com/r/MachineLearning/comments/3l3m3b/deep_learning_machine_teaches_itself_chess_in_72/,alexeyr,1442356826,,1,4
320,2015-9-16,2015,9,16,7,3l3ojo,"""Wrestling with attention based models + some other ""hacks"" makes me realize I've created a neural net which learns how to train a neural net"" (c) Roelof Pieters",https://www.reddit.com/r/MachineLearning/comments/3l3ojo/wrestling_with_attention_based_models_some_other/,derRoller,1442357981,,1,0
321,2015-9-16,2015,9,16,9,3l41zd,Question Regard multilayer perceptron and how to insert weights in the equation,https://www.reddit.com/r/MachineLearning/comments/3l41zd/question_regard_multilayer_perceptron_and_how_to/,Frescovivir,1442364230,"Hello everyone, 

I'm not that strong in math so please bear with me.  I made some search on reddit regard multilayer perceptron (MLP) and I think I end up in the right place ""to be honest I didn't even knew that MLP is related to machine learning"" 

anyway I'm trying to implement some broadcasting protocol in Ns3 that use MLP function in order to decide either to broadcast or not. 

this is the function: 

http://i.imgur.com/BmeQGFO.png
n =2 , m =3 

and this table show the weights: 

http://i.imgur.com/4ik4mW2.png

I asked an expert ""really busy expert he doesn't have time to explain"" and he told me and I quote ""use this function  with x = (N, Q, K).  Weight vectors are given in table II to calculate your desired Threshold""

I know based on some other calculation the value of N , Q , K 

N= 15
Q= .98 
K= 10

so now I'm not sure how to plug these weights in this MLP function also what does the expert mean by 
X=(N,Q,K) do I have to calculate the function three times based on N ,Q,K then add the results? 


I'm sorry if my questions sound silly to you guys but I'm really lost here any help or pointers will be appreciated. ",2,2
322,2015-9-16,2015,9,16,9,3l43ju,Target Prop and Difference Target Prop questions,https://www.reddit.com/r/MachineLearning/comments/3l43ju/target_prop_and_difference_target_prop_questions/,jyegerlehner,1442364924,"Bengio's target prop paper describes a scheme that bypasses vanishing or exploding gradients in deep networks by  finding targets for autoencoder encoder/decoder layer pairs to aim at:
http://arxiv.org/abs/1407.7906


The idea is very appealing to my intuition. But the scheme does appear to depend on every encoder layer f-sub-i and decoder layer g-sub-i already being approximate inverses of each other, so it doesn't seem like you could just start training via target prop from any kind of random initialization and expect it to work. 


In a subsequent paper, collaborators describe a variation called Difference Target Prop:
http://arxiv.org/abs/1412.7525


which differs from vanilla Target Prop in order to get it to ""actually work"". Algorithm 1 in the difference prop paper shows that to generate the initial target, one has to back-propagate to get gradient of global loss L with respect to the last encoder layer h-sub-(M-1). So we have to backprop half the way through a potentially deep autoencoder at the beginning of the Difference Target Prop process, which seems to bring us right back to the problem Target Prop was, at least partly, intended to sidestep: vanishing/exploding gradient in a very deep architecture. 


1) Am I missing something here? 
2) An alternative approach occurs to me: one could start with conventional greedy layerwise autoencoder training. Once one has fairly good performance, then f-sub-i and g-sub-i are approximately inverses, then one can use vanilla Target Prop, with the hope that Target Prop will provide much better targets for  training. 

",4,2
323,2015-9-16,2015,9,16,11,3l4fth,Is there a limit to the number of images for prediction that I can pass to deep learning libraries like Caffe at a time?,https://www.reddit.com/r/MachineLearning/comments/3l4fth/is_there_a_limit_to_the_number_of_images_for/,jackbrucesimpson,1442370676,"The Caffe MNIST example of instance allows for up to 64 images to be classified in chunks by the network, is there a reason why I couldn't increase this number to 200 or is there an optimal number in the range of a few dozen?",1,2
324,2015-9-16,2015,9,16,11,3l4ihg,I wrote a neural-network module (in Python) where the user can define how many layers and how many neurons per layer will be used,https://www.reddit.com/r/MachineLearning/comments/3l4ihg/i_wrote_a_neuralnetwork_module_in_python_where/,[deleted],1442371998,[deleted],18,22
325,2015-9-16,2015,9,16,15,3l50ry,Opening up the black box: Random forest interpretation with scikit-learn,https://www.reddit.com/r/MachineLearning/comments/3l50ry/opening_up_the_black_box_random_forest/,faming13,1442383516,,2,5
326,2015-9-16,2015,9,16,19,3l5lup,"yolong Industrial is beer equipment,turn-key brewery, water treatment and beverage production line solution supplier.",https://www.reddit.com/r/MachineLearning/comments/3l5lup/yolong_industrial_is_beer_equipmentturnkey/,Rachel-Han,1442399288,,0,1
327,2015-9-16,2015,9,16,19,3l5mof,Looking for a One month project on implementing a spam filter,https://www.reddit.com/r/MachineLearning/comments/3l5mof/looking_for_a_one_month_project_on_implementing_a/,[deleted],1442399910,"Me and a couple of my friends are looking for a one month project on implementing a spam filter.

We want to proceed with Naive Bayes. However, we want to learn more of machine learning...so feel free to tweak and suggest a better way to implement a spam filter using some other algorithm or maybe optimise Naive Bayes to make it better.


Including me there are 3 people in this project and after work, we can dedicate around 2 hours everyday. We are all Machine Learning noobs",1,2
328,2015-9-16,2015,9,16,20,3l5pb2,Generating Comics - A Creative Human Machine Collaboration.,https://www.reddit.com/r/MachineLearning/comments/3l5pb2/generating_comics_a_creative_human_machine/,samim23,1442401807,,0,6
329,2015-9-16,2015,9,16,20,3l5qu7,Rules of thumb for CNN architectures,https://www.reddit.com/r/MachineLearning/comments/3l5qu7/rules_of_thumb_for_cnn_architectures/,stua8992,1442402822,"I'm currently trying to get a CNN up and running but so far haven't been able to get results quite as promising as I was expecting. This may just be down to a difficult dataset, but I'm fairly inexperienced so I was wondering if there were any rules of thumb to get reasonable initial parameters/hyperparameters.

Specifically I'm not sure how many filters I should be trying to generate for a given input, how much dropout is too much, reasonable sizes for convolution windows, etc. I suspect that most of these will depend heavily on the size of the input volume, and currently my first layer takes in 48x48 grayscale images.",13,9
330,2015-9-16,2015,9,16,20,3l5s6t,A gentle introduction to understand the Gaussian distribution,https://www.reddit.com/r/MachineLearning/comments/3l5s6t/a_gentle_introduction_to_understand_the_gaussian/,AlanZucconi,1442403705,,7,49
331,2015-9-16,2015,9,16,22,3l66v7,BigML: Machine Learning made easy,https://www.reddit.com/r/MachineLearning/comments/3l66v7/bigml_machine_learning_made_easy/,asdrubaldone,1442411638,,0,3
332,2015-9-16,2015,9,16,23,3l6b5m,The Unreasonable Effectiveness of Random Forests,https://www.reddit.com/r/MachineLearning/comments/3l6b5m/the_unreasonable_effectiveness_of_random_forests/,john_philip,1442413489,,7,1
333,2015-9-17,2015,9,17,0,3l6ia0,DeepHear - Composing and harmonizing music with neural networks,https://www.reddit.com/r/MachineLearning/comments/3l6ia0/deephear_composing_and_harmonizing_music_with/,sunnyboy780,1442416539,,5,49
334,2015-9-17,2015,9,17,3,3l76xu,Negative sampling vs. method by Jean et al.?,https://www.reddit.com/r/MachineLearning/comments/3l76xu/negative_sampling_vs_method_by_jean_et_al/,__AndrewB__,1442426597,"I was reading the paper by Sebastien Jean et al. [On Using Very Large Target Vocabulary for Neural Machine Translation](http://arxiv.org/pdf/1412.2007v2.pdf)

and I fail to understand how is this method different from negative sampling used, for example, in word2vec?

At each iteration we choose some number of negative classes -&gt; compute softmax as usual -&gt; update parameters for sampled classes only. ",1,3
335,2015-9-17,2015,9,17,3,3l79k7,Data Science Data Logic,https://www.reddit.com/r/MachineLearning/comments/3l79k7/data_science_data_logic/,__mtb__,1442427650,,0,3
336,2015-9-17,2015,9,17,3,3l7bc5,Deploying Character Classification System made using CNN(caffe) on cloud.,https://www.reddit.com/r/MachineLearning/comments/3l7bc5/deploying_character_classification_system_made/,harshitladdha,1442428373,I want to deploy character classification system made using Convolutional Neural Network on caffe. Web interface is implemented using Flask. I wanted to know on which cloud services this can be easily deployed ? Heroku or AWS or Google App Engine or any other service?,3,3
337,2015-9-17,2015,9,17,4,3l7ker,"Question - Text classification with ""code"" input",https://www.reddit.com/r/MachineLearning/comments/3l7ker/question_text_classification_with_code_input/,OffsideLikeWorf,1442432067,"Let me know if there's a better place to ask this. 

I'm playing with scraping and analyzing chess games. It occurred to me that it is, of course, possible to predict which player (black or white) won a game using the source code from that game's webpage. This code is about 1000 lines. About 5 lines of the code contain some information about who won the game.

Say I wanted to train a binary classifier on this 1000 lines of code to predict the game's outcome. Is this basically a text classification problem? http://www.scholarpedia.org/article/Text_categorization If so, would a linear classifier like SVM be appropriate?

Thanks! Any insight would be really appreciated. I have experience with classifiers but never with this kind of training data.",7,4
338,2015-9-17,2015,9,17,4,3l7mzf,[Help] Variational autoencoder on categorical data,https://www.reddit.com/r/MachineLearning/comments/3l7mzf/help_variational_autoencoder_on_categorical_data/,AnvaMiba,1442433134,"Hello, first post here.

I'm having troubles training a variational autoencoder on categorical data.

Each sample of my dataset consists of 10 categorical variables over the same 8 symbol alphabet. I represent each categorical variable using one-hot encoding and concatenate them, resulting in a 80-dimensional vector per sample.

My dataset is syntetic and dynamically generated during training by sampling from a simple distribution: each sample is generated as a random string over a 7-symbol alphabet with geometrically-distributed length, truncated or padded with a distinguished symbol to length 10.

I tried to train a variational autoencoder with gaussian prior on the latent variables (zero mean, identity covariance), gaussian posterior on the latent variables (diagonal covariance) and deterministic decoder. A quite standard configuration, I think, which works on MNIST in my experiments.

However, on this task the reconstruction accuracy plateaus at around 63-65%. I tried varying the dimension of the latent variables, the size of the hidden layers in the encoder and decoder, the number of hidden layers, the activation functions (tanh or relu), the size of minibatches, the optimizer (Adam or Momentum SGD) and learning rate, but I can't get any significant improvement.

If I remove the prior cost term from the objective function or multiply it by a small constant, training quickly converges to perfect accuracy, but the prior cost explodes.

Am I doing something wrong or is training a variational autoencoder with gaussian latent variables on categorical data just a difficult problem?
",9,6
339,2015-9-17,2015,9,17,6,3l81pd,"""Evaluating Machine Learning Models"" - A Free Report/E-book by O'Reilly just came out",https://www.reddit.com/r/MachineLearning/comments/3l81pd/evaluating_machine_learning_models_a_free/,rasbt,1442438934,,2,67
340,2015-9-17,2015,9,17,7,3l88p1,Can I find a junior level job in ML with my background in the sciences?,https://www.reddit.com/r/MachineLearning/comments/3l88p1/can_i_find_a_junior_level_job_in_ml_with_my/,Miejuib,1442441859,"So I moved to the boston area for a PhD program in theoretical chemistry, but after a year, I realized that was very much not what I wanted to be doing with my time.  Rather, I found myself getting increasingly fascinated by the potential of deep learning.

So to give you an idea of my background: in my undergrad, I was originally pursuing a double-major in physics and chemistry, but I opted to only graduate with a degree in chemistry in order to save a lot of time and money.  That said, I have several years of experience programming simulations of quantum mechanical systems, and other assorted physics models. 


So things I've done include:

Eigenvalue calculator for particle in a finite well

Hermite Polynomial generator (to 700th order)

Monte Carlo integration

Harmonic Oscillator wavefunction generator

Linear Conductance Percolation Simulation

Fractal Dimensive Ising Model Simulation with Wolff Clustering Algorithm


My programs are all written in Python, utilizing the Scipy/numpy/matplotlib modules, and also in C for when I need better efficiency.

With regards to ML, I honestly don't have much experience in it yet, but I'm looking for a role where I can learn the ropes.  I'm no stranger to studying hard to improve my skills.  I'm planning on taking the Stanford course on coursera, but in the meantime, I also need to find work.


So, sorry in advance for the resume-in-a-post, but does anyone have any leads?  Or recommendations on steps to take in order to pursue my career goals?",9,3
341,2015-9-17,2015,9,17,13,3l9ie0,What is the biggest obstacle to progress in machine learning right now?,https://www.reddit.com/r/MachineLearning/comments/3l9ie0/what_is_the_biggest_obstacle_to_progress_in/,Whitey_Knightey,1442464163,,84,36
342,2015-9-17,2015,9,17,14,3l9nik,Is there a neural network interpretation of LDA or topic models ?,https://www.reddit.com/r/MachineLearning/comments/3l9nik/is_there_a_neural_network_interpretation_of_lda/,napsternxg,1442467291,"I have heard RBMs can be used as topic models as they learn the latent variables, but is there a direct interpretation of LDA topic models using neural networks, just like there are interpretation of SVM, logistic regression using neural networks. 

Would love to hear any thoughts from everyone. 

UPDATE: I don't mean to say that LDA and PGMs are bad. What I want to know, is that if anyone has done a mapping of LDA to ANN? And what can be  a possible way to do this kind of formulation?

I was intrigued by the way RBMs work and thought LDA or PGM's in general might be implemented using ANN method. 

One of the usefulness of using ANNs for LDA implementation would be an easy to use online method of training such models. However, that is not the only reason I am interested in this mapping. ",9,5
343,2015-9-17,2015,9,17,14,3l9p0n,A nice article on setting up your machine learning dataset.,https://www.reddit.com/r/MachineLearning/comments/3l9p0n/a_nice_article_on_setting_up_your_machine/,daniel_cointrop,1442468265,,0,3
344,2015-9-17,2015,9,17,16,3l9ye5,What are some machine learning algorithms for learning and predicting facial attractiveness?,https://www.reddit.com/r/MachineLearning/comments/3l9ye5/what_are_some_machine_learning_algorithms_for/,pobm2f,1442474976,"I was wondering what are some published academic papers or examples used by companies on machine learning algorithms for learning and predicting facial attractiveness.
Are there certain facial features, which are more useful than others?
Does regression type algorithm work well? Clustering? Classification?",6,6
345,2015-9-17,2015,9,17,18,3la6er,Hand Refractometer Supplier,https://www.reddit.com/r/MachineLearning/comments/3la6er/hand_refractometer_supplier/,devenderakumar,1442481582,,0,0
346,2015-9-17,2015,9,17,19,3lac6m,Spark &amp; machine learning workshops in New York and London! Limited offer - $99!,https://www.reddit.com/r/MachineLearning/comments/3lac6m/spark_machine_learning_workshops_in_new_york_and/,deepsenseio,1442486086,,0,1
347,2015-9-17,2015,9,17,20,3laetc,Different pooling layer implementations and references,https://www.reddit.com/r/MachineLearning/comments/3laetc/different_pooling_layer_implementations_and/,[deleted],1442488024,[deleted],0,1
348,2015-9-17,2015,9,17,20,3lah7w,Different,https://www.reddit.com/r/MachineLearning/comments/3lah7w/different/,[deleted],1442489670,[deleted],0,1
349,2015-9-17,2015,9,17,20,3lahq1,Recurrent Neural Networks Tutorial (Part 1)  Introduction to RNNs,https://www.reddit.com/r/MachineLearning/comments/3lahq1/recurrent_neural_networks_tutorial_part_1/,pogopuschel_,1442489998,,0,101
350,2015-9-17,2015,9,17,20,3lai2j,Predictions are not exactally the same after converting FC layers to CONV layers,https://www.reddit.com/r/MachineLearning/comments/3lai2j/predictions_are_not_exactally_the_same_after/,entron,1442490240,"I followed the Stanford convnet course (http://cs231n.github.io/convolutional-networks/) to convert a pertained AlexNet into a fully convolutional network. However when I use the fully convnet to predict some example images I found that the predicted categories are not exactly the same as the original AlexNet. For example a ""beagle dog"" and a ""west highland white terrier dog"" are classified as two similar kinds of dog by the fully convnet  ""basset dog"" and ""great pyrenees"" respectively. Does anybody know what could be my problem?",14,2
351,2015-9-17,2015,9,17,21,3lajnd,[Help] Difficulty in training Generative Adversarial Net,https://www.reddit.com/r/MachineLearning/comments/3lajnd/help_difficulty_in_training_generative/,[deleted],1442491266,"Hi all  
I am referring [this paper](http://arxiv.org/pdf/1406.2661v1) from NIPS'14. It is a very interesting model to generate samples from an unknown data distribution.  

I wrote code to test it out on MNIST using lasagne+theano. But unfortunately my generator isn't optimizing well enough. I followed the exact same model architecture as the [author's code in pylearn2] (https://github.com/goodfeli/adversarial). The paper doesnt mention how much training time is needed but even after a 1000 epochs the generator isn't learning (generator probability is ~ 10^-6) . I checked and found that the gradient norm of the generator was very small.  


Has anybody else got it working ? If so, could you please share some advice on how you trained it.

EDIT: [Link](http://pastebin.com/RsH2pqbH) to my code
",16,4
352,2015-9-17,2015,9,17,21,3lakv2,Different neural network pooling functions references,https://www.reddit.com/r/MachineLearning/comments/3lakv2/different_neural_network_pooling_functions/,ciolaamotore,1442491940,"By looking at this video by LeCun on ConvNets: https://www.youtube.com/watch?v=M7smwHwdOIA at min 16:55 circa, I came across a peculiar way of doing pooling, that is by taking the log probs instead of a common max or the usual mean average function.
However, I was not able to find some reference for it in the literature about it.
Do you have some pointers to studies on different pooling functions exploitations? I am expecially interested in that log version more than the classical average, max, norm etc.",6,5
353,2015-9-17,2015,9,17,21,3lande,,https://www.reddit.com/r/MachineLearning/comments/3lande//,giantcomputerpk,1442493368,,2,0
354,2015-9-17,2015,9,17,22,3lawwt,"When should I prefer PMI over the t-test, or the other way around?",https://www.reddit.com/r/MachineLearning/comments/3lawwt/when_should_i_prefer_pmi_over_the_ttest_or_the/,fuckinghelldad,1442498280,,0,4
355,2015-9-17,2015,9,17,23,3layuz,Evaluating which decision model/approach performs best,https://www.reddit.com/r/MachineLearning/comments/3layuz/evaluating_which_decision_modelapproach_performs/,surangak,1442499209,"Hi all,

I'm trying to build decision models using an imbalanced dataset that's n = 300..... I know - I don't like it either :-(

Thanks to advice from a few reditters, I tried out multiple options including minority oversampling, CostSensitiveClassifing, GBM, XGBoost etc.
Now here's my problem. All these approaches give me good precision, recall, f-measure. but no matter what thresholds / cutoff's I try, I always end up with very poor ROC, which tells me that the models are bad.

However, the sole exception seems to be WEKA's SMOTE approach, which is a synthetic minority oversampling technique.

This gives me high precision, recall, f-measure AND ROC curve, which tells me that it is the best approach.

My question is, does high precision, recall, f-measure AND ROC curve indicate that the model is ok? should I take this as a sign of good model fitting? ",1,0
356,2015-9-17,2015,9,17,23,3lb1bo,Metric Learning Package Available in Python,https://www.reddit.com/r/MachineLearning/comments/3lb1bo/metric_learning_package_available_in_python/,terrytangyuan,1442500341,,0,15
357,2015-9-18,2015,9,18,0,3lb72z,On the Expressive Power of Deep Learning: A Tensor Analysis,https://www.reddit.com/r/MachineLearning/comments/3lb72z/on_the_expressive_power_of_deep_learning_a_tensor/,SuperFX,1442502910,,2,24
358,2015-9-18,2015,9,18,0,3lb9dk,Need help in SdA implementation,https://www.reddit.com/r/MachineLearning/comments/3lb9dk/need_help_in_sda_implementation/,[deleted],1442503904,[deleted],2,0
359,2015-9-18,2015,9,18,0,3lbddq,How to Leverage Machine Learning for Your Business,https://www.reddit.com/r/MachineLearning/comments/3lbddq/how_to_leverage_machine_learning_for_your_business/,tarpus,1442505526,,0,1
360,2015-9-18,2015,9,18,2,3lbu6t,Reinforcement learning (practical) example?,https://www.reddit.com/r/MachineLearning/comments/3lbu6t/reinforcement_learning_practical_example/,noclaf,1442512427,"I'm planning on doing an RL related project for a hack-day. Are there any practical examples which show an experienced programmer how to build a solution in a short amount of time?

I've watched several videos on RL and even have Norvig's AI book. All of them (and I mean ALL OF THEM) use the same grid example to build up mdps, then q-whatever, then some other layer. I've found that I watch a few lectures, then get distracted in the middle of these professors' foundation building exercises.

Instead, I'd like to try a more programmer-centric approach. There must be pre-built libraries out there and those libraries much have examples or tutorials which show how to build a system for my domain. I know there are a number of popular libraries for typical ML paradigms, such as supervised learning, unsupervised learning, clustering, etc. However, I am completely unfamiliar with the RL ecosystem.

I'd love any suggestions on tutorials which will get a programmer building RL solutions quickly. Keeping in mind that this will be an exercise in learning through prototyping.",7,4
361,2015-9-18,2015,9,18,2,3lbur1,Training an algorithm to recognise a fuse,https://www.reddit.com/r/MachineLearning/comments/3lbur1/training_an_algorithm_to_recognise_a_fuse/,MachineVision,1442512656,"Is it possible to train a machine learning algorithm to recognise an image of a fuse (the fuse shapes are the same, only colors change). Up until now I have been using KMeans Clustering to get the dominant color in an image and using that to describe a fuse's color. This approach is somewhat sensitive to lighting changes.

SIFT/SURF don't work so well here because they do not use colour information and also aren't able to find a decent number of matches.

However, I have been wondering if it is possible to use machine learning such that my software can recognise the image of a particular fuse and go ""This fuse is brown!""

I have a bit of background in Computer Vision but very little when it comes to Machine Learning. This is somewhat of an exercise that I want to use to get into Machine Learning.

[Brown Fuse](http://i.stack.imgur.com/w3TB8.png)
[Fuse-colors](http://i.stack.imgur.com/rorYr.jpg)

This is somewhat of a cross-post because a couple of days back I posted on /r/computervision asking how to match fuse colours.",1,1
362,2015-9-18,2015,9,18,3,3lbvyy,Deep Style: Inferring the Unknown to Predict the Future of Fashion,https://www.reddit.com/r/MachineLearning/comments/3lbvyy/deep_style_inferring_the_unknown_to_predict_the/,Kombutini,1442513128,,2,10
363,2015-9-18,2015,9,18,3,3lc0ui,Clustering with Hierarchical &amp; Density-Based Algorithms (hands-on workshop),https://www.reddit.com/r/MachineLearning/comments/3lc0ui/clustering_with_hierarchical_densitybased/,acangiano,1442515142,,1,2
364,2015-9-18,2015,9,18,3,3lc3nn,Building a SEO tool with Machine Learning,https://www.reddit.com/r/MachineLearning/comments/3lc3nn/building_a_seo_tool_with_machine_learning/,wildcodegowrong,1442516308,,1,2
365,2015-9-18,2015,9,18,4,3lc4pn,"What is the best way you know to get a feature representation of documents, and why? [unsupervised learning]",https://www.reddit.com/r/MachineLearning/comments/3lc4pn/what_is_the_best_way_you_know_to_get_a_feature/,shrimpMasta,1442516691,"Which algorithm ? i.e.:

* Deep Belief Networks (DBN)?
* DBN then autoencoder?
* Autoencoder alone ?
* Latent Dirichlet Allocation (LDA)?
* Latent Semantic Indexing (LSI)?
* ???
 
Which embedding ? i.e.: 

* tf-idf ?
* ???

I have a large amount of documents, I want to get preferably large feature-representations of the semantic info contained in each document. Links to publications and cool libraries are very welcome.

My own experience:

I've used LDA a lot, I think it's great to extract the core topics inside documents. However the Dirichlet priors force the number of topics to be around a certain value. It's their job, but it yields that the algorithm fails to correctly represent documents with few or many topics. LDA really smooths the feature-representations. Also it uses gibbs sampling to infer representations on new documents. 

LSI is the best if you don't have much data. It is just a truncated SVD so it's the best compression you can get of your data using linear transformations. 

I've never used DBN but I would guess that its feature detector nature solves the dirichlet prior problem with LDA. Also the inference for new documents is fast. The most successful approaches seem to pre-train an autoencoder with a DBN and fine-tune the net training as an autoencoder with ReLU units ?

Training an autoencoder from scratch without DBN seems really tedious. Any experience on this?

I believe the future of unsupervised document representation lies in RNNs, but I'm yet to find a paper on this. 
",11,2
366,2015-9-18,2015,9,18,5,3lclyk,ELI5 HMM training,https://www.reddit.com/r/MachineLearning/comments/3lclyk/eli5_hmm_training/,augustus2010,1442523476,Anybody over here can give a ELI5 about HMM training like you talk to somebody with no background of machine learning? Is it way too hard to do ?,2,0
367,2015-9-18,2015,9,18,8,3ld9oo,Metric Learning Package available in R,https://www.reddit.com/r/MachineLearning/comments/3ld9oo/metric_learning_package_available_in_r/,[deleted],1442533943,[deleted],0,1
368,2015-9-18,2015,9,18,10,3ldips,Are there ways to take an evolved game-playing neural network and derive a set of statements or rules about the strategy it's implementing? (x-post from r/compsci),https://www.reddit.com/r/MachineLearning/comments/3ldips/are_there_ways_to_take_an_evolved_gameplaying/,ascetica,1442538374,"For example, you can evolve a neural network to play tic-tac-toe and it will get to a state where it will basically always block when the other player has two in a row. It won't start at that point but will evolve to it over time. Is there a way to analyze the AI's evolution, or even just look at its end state, and derive that the AI is heading towards the strategy ""always block the other play when they have two in a row"" just by knowing the rules of the game and the values of the neurons' connections and weightings?
Tic-tac-toe is a trivial example, but say you had the AI play a more complex game where the strategy depended on a lot more parameters. Is it possible to translate the black-box of the AI's decision making into something like a decision tree of the strategy?

Also, is there a name for this sort of technique or field of study that I could look more into?

Thanks",2,7
369,2015-9-18,2015,9,18,10,3ldjqy,"Videos of Deep Learning Summer School, Montreal 2015",https://www.reddit.com/r/MachineLearning/comments/3ldjqy/videos_of_deep_learning_summer_school_montreal/,matiskay,1442538922,,9,88
370,2015-9-18,2015,9,18,11,3ldpke,Data Mining Courses,https://www.reddit.com/r/MachineLearning/comments/3ldpke/data_mining_courses/,brotherrain,1442541804,,0,4
371,2015-9-18,2015,9,18,14,3lecrm,How the NYC Mayor's Office of Data Analytics Uses Machine Learning to Make Bad Buildings Better,https://www.reddit.com/r/MachineLearning/comments/3lecrm/how_the_nyc_mayors_office_of_data_analytics_uses/,dataforgood,1442554769,,0,2
372,2015-9-18,2015,9,18,17,3lep8f,What's Wrong With Deep Learning by Yann Lecunn,https://www.reddit.com/r/MachineLearning/comments/3lep8f/whats_wrong_with_deep_learning_by_yann_lecunn/,[deleted],1442564176,[deleted],0,0
373,2015-9-18,2015,9,18,19,3ley3a,Equipment To Help Make A Solid Foundation,https://www.reddit.com/r/MachineLearning/comments/3ley3a/equipment_to_help_make_a_solid_foundation/,Esichoice,1442571105,,0,0
374,2015-9-18,2015,9,18,21,3lfbwm,What do you call this technique?,https://www.reddit.com/r/MachineLearning/comments/3lfbwm/what_do_you_call_this_technique/,jkabrg,1442579808,"I'm a worker in the energy business, and I'm trying to see if we can predict whether a customer will complain given that they called in about x, y and z. My team and I are new to data science.

In the data there are about $200$ call reasons.

The way we tried to do this before was to construct a sparse matrix which tells you how many times customer $i$ calls in about $j$. The final column is whether $i$ complained in 2015. Sadly it appears that more than 30% of customers who've called in have complained. We tried random forests (70% accuracy), naive bayes (54% accuracy) and JRip (70% accuracy). All of these were cross-validated. Previously some people tried this task out and made a random forest model with 85% OOB accuracy. I don't know how they managed that.

Anyway I've been thinking about a different approach. The aim is to estimate the probability $\operatorname p(s)$ that a customer whose call reasons form the [bag](https://en.wikipedia.org/wiki/Multiset) $s$ will complain. Pseudocode:

    let h be a hash table where the indices are bags and the entries are 2-vectors
    for row r in data
      h[the bag of call reasons in r][r.didComplain] += 1

Then $\operatorname{p}(s)\approx {h[s][1] \over h[s][0] + h[s][1]}$

After discussion with someone we reasoned that if we found someone who didn't complain with call reasons $s$ we should increment $h[s'][0]$ for every *sub*bag $s'$ of $s$. If on the other hand they did complain we would increment $h[s'][1]$ where $s'$ is every *super*bag of $s$. I've decided to omit the pseudocode.

What do you call this? Is it a good idea?",3,6
375,2015-9-18,2015,9,18,22,3lfj8f,Which is the better method when training a neural network on data from a person's mobile phone? (complete beginner),https://www.reddit.com/r/MachineLearning/comments/3lfj8f/which_is_the_better_method_when_training_a_neural/,transformium,1442583507,"I want to extract some data from a mobile phone (through an app) and run it through some NN model/models to get some output. (say, extract object from images). Is it better to run these computations on the device itself or have the data uploaded to some cloud where the computations can be done? Why?
Thanks in advance.",4,0
376,2015-9-18,2015,9,18,23,3lfmxc,"Videos of talks from Conference on Learning Theory (COLT), Paris 2015",https://www.reddit.com/r/MachineLearning/comments/3lfmxc/videos_of_talks_from_conference_on_learning/,Hydreigon92,1442585187,,0,16
377,2015-9-19,2015,9,19,1,3lg5g0,Predict new input base on Normalized Matrix In Neural network,https://www.reddit.com/r/MachineLearning/comments/3lg5g0/predict_new_input_base_on_normalized_matrix_in/,tuananh_bk,1442592971,"Recently I have learning to build text classifier system by neural network. 



I have used sklearn to build CountVectorize for all of items in sample test, and use Normalization class to normalize it . After that I used that matrix to train the Neural Network.

But I'm stucking in how to predict new items base on that Neural Network?



How can I normalize new Item?? ",2,1
378,2015-9-19,2015,9,19,3,3lgmki,[Question] Why are weights of a neural net initialised according to a distribution?,https://www.reddit.com/r/MachineLearning/comments/3lgmki/question_why_are_weights_of_a_neural_net/,khalimben,1442599828,"From my naive understanding of neural nets, you randomly initialise the weights to some values and then train with BP to find the optimum values. How does initialising according to a gaussian with some variance and mean help? How does probability even come in here? Isn't it just optimisation, differential geometry?

In one line, what will happen if I don't initialise according to some distribution, but uniformly pick out random values?

Thanks for reading and hopefully you reply!! :)

PS: If you have some quick tutorials/materials that a explain how these things work out, it'd be great.
",7,5
379,2015-9-19,2015,9,19,3,3lgph4,LSTM vs Vanilla RNN: Theoretical Capability,https://www.reddit.com/r/MachineLearning/comments/3lgph4/lstm_vs_vanilla_rnn_theoretical_capability/,[deleted],1442600975,[deleted],1,0
380,2015-9-19,2015,9,19,4,3lgxr1,Machine Learning for Recommender Systems with Alexandros Karatzoglou [Video/Talk],https://www.reddit.com/r/MachineLearning/comments/3lgxr1/machine_learning_for_recommender_systems_with/,rasbt,1442604378,,0,9
381,2015-9-19,2015,9,19,4,3lgy0r,Python &amp; HDF5  A Vision: pandas / PyTables / h5py stack,https://www.reddit.com/r/MachineLearning/comments/3lgy0r/python_hdf5_a_vision_pandas_pytables_h5py_stack/,[deleted],1442604490,[deleted],0,1
382,2015-9-19,2015,9,19,4,3lgy8v,Python &amp; HDF5  A Vision: h5py / PyTables / pandas stack,https://www.reddit.com/r/MachineLearning/comments/3lgy8v/python_hdf5_a_vision_h5py_pytables_pandas_stack/,dribnet,1442604588,,2,3
383,2015-9-19,2015,9,19,4,3lh2ea,Predicting Cab Booking Cancellations: from data acquisition to submission to Kaggle (jupiter notebook),https://www.reddit.com/r/MachineLearning/comments/3lh2ea/predicting_cab_booking_cancellations_from_data/,cast42,1442606388,,6,21
384,2015-9-19,2015,9,19,5,3lh52p,Help fund another season of ML podcast!,https://www.reddit.com/r/MachineLearning/comments/3lh52p/help_fund_another_season_of_ml_podcast/,omnipoint,1442607476,,3,9
385,2015-9-19,2015,9,19,6,3lhb0r,"New to ML, not sure what technique would be ""best"" to achieve my goal",https://www.reddit.com/r/MachineLearning/comments/3lhb0r/new_to_ml_not_sure_what_technique_would_be_best/,bajuwa,1442610055,"So, as mentioned, I'm relatively new to machine learning as I am currently going through a graduate machine learning course (only a week or so in, though).  We eventually have to do a team project, so as a possibility I chose the following idea, but am not sure what type(s) of algorithms/concepts I should research in to in order to accomplish this.

Idea:  Given a book (either by title or by ebook/content), identify other books of a similar writing style (not genre/author/'users also liked'/etc).

Data Required: Obviously I'm going to need a lot of book content in order to accomplish this, but it seems like a relatively easy to download a bunch of ebooks, parse them for metadata, and then parse through the content and gather the data points that I will be using for the main learning algorithm.  The data points I was planning on gathering were along the lines of:    
* Sentence length (to hint at complexity)    
* Counts of punctuation types (period count, quote count, etc)        
* Average word count per chapter    
* Count of first-person pronouns (I, me, us)    
* Total word count (both as data or as a way to swap between hard count values and percentages/ratios if need be)    
* etc...

Machine Learning Technique:    
This is where I am having problems....  My data doesn't seem to lend itself well to supervised learning, so it makes me think of some sort of unsupervised clustering, but is this a correct assessment?  If so, what types of clustering would best suite my needs?  If not, what else should I be looking at?    
When I was first considering this problem, my thoughts were that the final product would look like a graph of book-nodes and 'similarity-weighted'-edges (so a large weight for similar books, and low/zero weight for non-similar books, or vise versa).  From here anyone could find the node of a book they liked and find a similar book based on those edges.  If a new book was added, the machine learning algorithm would be able to do its magic and calculate the weights of the edges for each other book.


So.... any thoughts?  If I'm too far off the mark, then I would even settle for a simple ""Read up on X, Y, and Z then try again""",4,1
386,2015-9-19,2015,9,19,7,3lhkrp,What would be state of the art applications of machine learning to natural language processing?,https://www.reddit.com/r/MachineLearning/comments/3lhkrp/what_would_be_state_of_the_art_applications_of/,ConfuciusBateman,1442614408,"Sorry if this question is a bit general, but I'm really curious about what the most advanced implementations of machine learning in the field of NLP are. If anyone is aware of any projects that really excite them in this area, I would love to hear about them. ",5,2
387,2015-9-19,2015,9,19,9,3li539,Uplift modeling in Weka?,https://www.reddit.com/r/MachineLearning/comments/3li539/uplift_modeling_in_weka/,mcscreamy,1442624279,"I'd like to do some uplift modeling on medical datasets. The essential idea is to classify individuals who might benefit from a given intervention in a randomized trial. 
I've been using the Weka platform with good results for other tasks, but can't seem to find an uplift algorithm for the interface. Obviously, doing it manually is possible, but I'd anyone aware of existing uplift models that are Weka friendly?

Thanks.",0,5
388,2015-9-19,2015,9,19,15,3lj4n6,QT8-15 Automatic color bricks making machine with pallet loading machine...,https://www.reddit.com/r/MachineLearning/comments/3lj4n6/qt815_automatic_color_bricks_making_machine_with/,blockmachinemaker,1442645787,,1,0
389,2015-9-19,2015,9,19,17,3ljbub,[1509.05329] Recurrent Spatial Transformer Networks,https://www.reddit.com/r/MachineLearning/comments/3ljbub/150905329_recurrent_spatial_transformer_networks/,iori42,1442651477,,1,16
390,2015-9-19,2015,9,19,19,3ljm6c,Hyperparameters tuning,https://www.reddit.com/r/MachineLearning/comments/3ljm6c/hyperparameters_tuning/,data_wiz,1442659974,"Hi there, tuning of hyperparameters in algorithms are often referred to as an art. There are generally several key strategies, such as grid search, random search, bayesian, GA, model-based, etc.

Do you have any strategies that you personally like, whether its general or algorithm-specific?

Thanks!",3,1
391,2015-9-19,2015,9,19,19,3ljmkh,[1509.01626] Character-level Convolutional Networks for Text Classification,https://www.reddit.com/r/MachineLearning/comments/3ljmkh/150901626_characterlevel_convolutional_networks/,pilooch,1442660287,,2,4
392,2015-9-19,2015,9,19,20,3ljno6,Made a twitter bot that will create ArtStyleCNN images for the next 20 hours combining my photo with my artstyle directories.,https://www.reddit.com/r/MachineLearning/comments/3ljno6/made_a_twitter_bot_that_will_create_artstylecnn/,w0nk0,1442661125,,4,4
393,2015-9-19,2015,9,19,22,3ljzb7,Does anyone know if there's a Caffe C++ example on GitHub that allows for multiple image inputs at the same time?,https://www.reddit.com/r/MachineLearning/comments/3ljzb7/does_anyone_know_if_theres_a_caffe_c_example_on/,jackbrucesimpson,1442668837,,4,2
394,2015-9-19,2015,9,19,23,3lk9fx,CheatSheet - Python &amp; R Codes for common Machine Learning Algorithms,https://www.reddit.com/r/MachineLearning/comments/3lk9fx/cheatsheet_python_r_codes_for_common_machine/,john_philip,1442674537,,9,153
395,2015-9-20,2015,9,20,0,3lkbna,Binary classification where classifier choose not to classify an observation. CLASSIFICATION with REJECT OPTION [PDF],https://www.reddit.com/r/MachineLearning/comments/3lkbna/binary_classification_where_classifier_choose_not/,kunjaan,1442675518,,0,7
396,2015-9-20,2015,9,20,0,3lkhgw,Final year CS student... I need advice,https://www.reddit.com/r/MachineLearning/comments/3lkhgw/final_year_cs_student_i_need_advice/,arguenot,1442678126,"I'm in my final year of CS. I'm taking it at the University of Iceland. I'm currently enrolled in a Machine Learning class. The idea of this class is exactly the type of stuff that fascinates me to no end and I'd like to spend my life doing, but there's a problem.

The CS curriculum (for general CS) is very light on math. Until recently I didn't really understand how much math I should know as a Computer Scientist or how important it is (on a personal level I feel somewhat cheated as I didn't really understand what it meant to be a Computer Scientist when I started out, and now I don't really feel I'll be a legitimate one after I graduate). 

I don't know any Linear Algebra except how to do basic operations on vectors and matrices and just barely that much. I've taken an intro to stats and probability course and did well, and had a lot of interest in the subject (this is the same course taken by engineers and the first course taken in the subject by math majors). I took a dumbed down version of Calculus, where we covered only the basics (for comparison, engineers (including software ones) take three courses in Calculus, a course on Linear Algebra, Numerical Analysis and two courses in physics). Finally I've taken some discrete math courses. 

The ""grace period"" of the ML class I'm taking has now finished. The introduction and conceptual phase is over and it's implementation/math time. I'm screwed right?

I've managed to finish the first assignment of this type through googling and mimicking what the teacher did in class but it doesn't feel real to me. It's like when you're programming and you use a framework or a library that abstracts away all the details into a neat interface, which is fine if you need to get something done but you don't understand anything. And I want to understand it. I'd like to contribute to it if at all possible. I don't really stand a chance in that regard right? I don't think I can educate myself in math to the level I need/want to be at for this.



",12,2
397,2015-9-20,2015,9,20,1,3lkjzv,How would one go about training a model for this type of data?,https://www.reddit.com/r/MachineLearning/comments/3lkjzv/how_would_one_go_about_training_a_model_for_this/,rumborak,1442679219,"Out of personal interest, specifically to see what different ML techniques are capable of, I am trying to train different models on the following data set, which I call ""Add One"":

(example data):

Input: 0 0 0 1 0 1

Output: 1 0 0 1 0 1

The rule here is simple: ""Put one more 1 into the output"".

Now, the problem I'm facing with training something like this is that there's no one ground truth output for a given input, which seems to be an implicit assumption in many available tools.
Meaning, the output ""0 0 0 1 1 1"" would just as well have been correct for the above input, and thus during training I wouldn't want to penalize against it if the model hypothesizes it.

So, my question would be:

a) What models could possibly do a good job on this set?

b) How would one go about training it (with available tools, like R's caret, or Weka)

Thanks in advance.",21,1
398,2015-9-20,2015,9,20,2,3lku0j,To what extent are current deep reinforcement learners capable of deduction?,https://www.reddit.com/r/MachineLearning/comments/3lku0j/to_what_extent_are_current_deep_reinforcement/,onlyml,1442683372,"Just to give a bit of background this question came from thinking about building a reinforcement learner to play the game [http://agar.io](http://agar.io). It's basically a multiplayer version of fishy that adds some interesting mechanics. One of which is viruses which sit around on the map and if eaten cause a player to explode into many small pieces rendering them vulnerable. You can also feed viruses by ejecting mass into them, and if you feed them enough they will spawn a new virus which flies some distance in the direction of the last bit of ejected mass. You can use this to your advantage by launching a virus into another player and then eating the pieces when they explode.

So say you had a learning algorithm that had seen many examples of viruses multiplying from feeding, and many examples of players exploding from eating viruses, but no examples of a player exploding due to a launched virus. Could any current reinforcement algorithms deduce the tactic of launching a virus into another player without explicitly seeing it?

If yes by what rough mechanism would this be possible, if no, any idea what it would take to implement a system that could do this?

Note I'm not actually planning to tackle this project anytime soon, this is just a theoretical question.",5,9
399,2015-9-20,2015,9,20,5,3lli1l,Looking for an implementation of QuickCode to link to .NET application,https://www.reddit.com/r/MachineLearning/comments/3lli1l/looking_for_an_implementation_of_quickcode_to/,ReturningTarzan,1442693539,"So a while ago I came across [this paper](http://research.microsoft.com/en-us/um/people/sumitg/pubs/popl10-synthesis.pdf) which describes an algorithm for synthesising string manipulation programs based on input-output examples. Apparently it works surprisingly well, and it became the basis for the Flash Fill feature in Excel 2013. There's also [this interactive demo](http://rise4fun.com/QuickCode) you can play with.

I could use functionality like that in several projects I'm working on but the paper is a little hard for me to follow. I did find [this implementation](https://github.com/MikaelMayer/StringSolver), but it's written in Scala and really hard to link to Win32 or .NET applications. I considered translating it to F# or some such, but I'm not very familiar with Scala and it's a bit of an undertaking.

So before I spend weeks trying that, does anyone know of any other implementations of the algorithm (or similar algorithms, or alternatively a way to cleanly integrate the Scala library into .NET or Win32 applications)?",0,1
400,2015-9-20,2015,9,20,11,3lmra2,Why hasn't machine learning produced a human level general intelligence yet?,https://www.reddit.com/r/MachineLearning/comments/3lmra2/why_hasnt_machine_learning_produced_a_human_level/,Whitey_Knightey,1442715324,"Neural networks are supposed to be able to learn anything, and computer scientists have immense computational resources at their disposable and lots of data. Why haven't deep neural networks using machine learning techniques produced a human level general intelligence yet? What is fundamentally preventing the machine learning community from doing this? ",11,0
401,2015-9-20,2015,9,20,14,3lncii,ML Salaries in Europe,https://www.reddit.com/r/MachineLearning/comments/3lncii/ml_salaries_in_europe/,nd_irish,1442727949,"Given the field of ML/Data Mining is, at its core, all about data, I thought it was rather strange that it's so difficult to find statistics about ML job salaries in Europe. Are there any practicing European ML people who can give a rough estimate of salaries?

To show what I mean, I have seen jobs in Seattle at the big firms (Amazon, Google, Facebook, Microsoft) that range from 120k-150k, and SF is a bit higher than that. If anyone wants to give more information about SF/NYC, that would also be cool.

Thanks for your help.",25,27
402,2015-9-20,2015,9,20,15,3lnh86,"A baseline C++ neural network library, with genetic algorithm and backpropagation training",https://www.reddit.com/r/MachineLearning/comments/3lnh86/a_baseline_c_neural_network_library_with_genetic/,Weihua99,1442731725,,14,7
403,2015-9-20,2015,9,20,16,3lnkub,Simulated NN problem,https://www.reddit.com/r/MachineLearning/comments/3lnkub/simulated_nn_problem/,DinosaurChemist,1442734965,"I was asked the other day about a binary classification problem in two dimensions where there was grouping of data in each of the four quadrants of a grid.  The asker mentioned this was a problem which neural nets solved really well. Any reference for this that people are aware of?

Data looks like this: 

A | B
-  -  - 
B | A

",2,5
404,2015-9-20,2015,9,20,20,3lnzym,Beaker Notebooks is the next generation of Jupyter Notebooks,https://www.reddit.com/r/MachineLearning/comments/3lnzym/beaker_notebooks_is_the_next_generation_of/,alexperrier,1442748839,,21,22
405,2015-9-20,2015,9,20,22,3lo726,Has anybody done a study of the bias term in neural nets on benchmark datasets?,https://www.reddit.com/r/MachineLearning/comments/3lo726/has_anybody_done_a_study_of_the_bias_term_in/,Ghostlike4331,1442754049,"I know that neural nets need the bias units for the universal approximation property to hold, but on the other hand having them in bloats the code. I've looked for papers that quantify whether excluding them has an adverse affects on real life tasks, but I could not find any. In my own insufficient testing, adding them in did not make a marked difference. Any pointers would be appreciated.",5,4
406,2015-9-20,2015,9,20,22,3lo7xs,Ruby Gem for Sentimental Analysis,https://www.reddit.com/r/MachineLearning/comments/3lo7xs/ruby_gem_for_sentimental_analysis/,PMYOUMYTITS,1442754643,What ruby gem should i be using? I'll mainly be doing analysis of multiple tweets. I would like to not use an API to do this. ,4,5
407,2015-9-20,2015,9,20,22,3lo8ob,"MSc in Computational Statistics &amp; Machine Learning, University College London",https://www.reddit.com/r/MachineLearning/comments/3lo8ob/msc_in_computational_statistics_machine_learning/,EulersSecretIdentity,1442755180,"Hi everybody, I'm a student in the final year of my undergraduate degree (Mathematics &amp; Statistics joint major). I am considering pursuing a masters in machine learning, in particular the one mentioned in the title. 

* Has anyone here been enrolled in this program or a similar one and if so, what was your experience like?
* Is there much prior programming knowledge required?
* How tough is it?
* What are you doing now?

Any general advice about a career in machine learning would also be greatly appreciated. Apologies if this is the wrong place to post this.
Many thanks!
",13,8
408,2015-9-20,2015,9,20,22,3lo8qa,Ignoring background features with CNNs,https://www.reddit.com/r/MachineLearning/comments/3lo8qa/ignoring_background_features_with_cnns/,WhyDoYouNeedThat,1442755216,"Given an image of an object with a known, solid background color, how can I influence a CNN to ignore/discount the features of the background, thereby emphasing the object?

FYI - my scenario is feature extraction (e.g. neural codes) for the purposes of CBIR. I am using Caffe.

EDIT: In my scenario I am using the CNN for feature extraction for the purpose of image search. That is, I'm building up a large database of features of images in the data set, and given a new query, will extract it's features, and then do matching. 

The problem is the images in the database have background colors which are hurting the matching since the features assume the background is relevant. 

So to clarify my question: do you have any advice on how to de-emphasize the background in these extracted features when matching?",8,5
409,2015-9-20,2015,9,20,23,3lof7b,"Fujitsu Achieves 96.7% Recognition Rate for Handwritten Chinese Characters Using AI That Mimics the Human Brain - First time ever to be more accurate than human recognition, according to conference",https://www.reddit.com/r/MachineLearning/comments/3lof7b/fujitsu_achieves_967_recognition_rate_for/,Yuli-Ban,1442759540,,44,157
410,2015-9-20,2015,9,20,23,3lohws,Feature Selection using Fisher Score,https://www.reddit.com/r/MachineLearning/comments/3lohws/feature_selection_using_fisher_score/,[deleted],1442760971,[deleted],0,1
411,2015-9-21,2015,9,21,1,3loqlo,Which is the best convolutional generative model for images?,https://www.reddit.com/r/MachineLearning/comments/3loqlo/which_is_the_best_convolutional_generative_model/,wildtales,1442765225,"By convolutional generative models, I mean models like Convolutional RBM, deconvolutional networks etc.,",8,3
412,2015-9-21,2015,9,21,1,3lospw,MLDB  Machine Learning Database,https://www.reddit.com/r/MachineLearning/comments/3lospw/mldb_machine_learning_database/,cryptoz,1442766128,,1,9
413,2015-9-21,2015,9,21,2,3lp62v,Looking for ideas for a Bitcoin related Machine Learning Project,https://www.reddit.com/r/MachineLearning/comments/3lp62v/looking_for_ideas_for_a_bitcoin_related_machine/,OmegawOw,1442771645,"Me and some batchmates of mine in college are looking into Bitcoin. Particularly how we can apply Machine Learning to the data available and see what we can find.

I thought I would enquire here as to if you guys had any ideas or inputs. Alternatively let me know if you think there isn't much scope to apply Machine Learning to Bitcoin, or if all the significant work in the field has already been done.

Thanks in advance !",3,2
414,2015-9-21,2015,9,21,4,3lpjrf,Using K-Means to identify (and validate) the main colours in an images,https://www.reddit.com/r/MachineLearning/comments/3lpjrf/using_kmeans_to_identify_and_validate_the_main/,AlanZucconi,1442777110,,8,4
415,2015-9-21,2015,9,21,6,3lq1p7,Anyone have the full pdf of the reinforcement learning paper from Deepmind?,https://www.reddit.com/r/MachineLearning/comments/3lq1p7/anyone_have_the_full_pdf_of_the_reinforcement/,texalva,1442784379,,3,1
416,2015-9-21,2015,9,21,6,3lq30r,Getting Started with ML - groups for peer learning and sharing ideas,https://www.reddit.com/r/MachineLearning/comments/3lq30r/getting_started_with_ml_groups_for_peer_learning/,Blanket234533,1442784903,"I am starting to learn ML on my own using a variety of online resources. Mostly going to rely on machinelearningmastery.com and take it from there. 
I do have a BA in Maths and have studied ML in the past but it's been a few years so I do need to cover some basics quickly again. 

While learning alone and using online communities is great, I believe there is a lot of value in occasionally learning together with peers and discussing problems. 

Are there groups for ML beginners in London (UK) that anyone can recommend?
Are there beginners in London who would be interested in occasionally sharing progress and discussing ideas?

Thanks a lot.

",0,2
417,2015-9-21,2015,9,21,8,3lqkaj,"When writing a function to calculate the ACF of a time series, how do you handle the uneven lists that are created because of lags?",https://www.reddit.com/r/MachineLearning/comments/3lqkaj/when_writing_a_function_to_calculate_the_acf_of_a/,o_safadinho,1442792509,"For example if I was trying to write a function in java that calculated the ACF of a series.

Take this series for example:

&gt; int[] series = new int[]{3,7,1,0,8,2,4,1}

If I wanted to calculate the ACF with a lag of two I'd have to use the following series:

&gt; {1,0,8,2,4,1}

What I want to know is, when I'm calculating the autocorrelation how do you deal with the fact that one series is shorter than the other? The first series has a length of 8 and the second a length of 6? I can't just calculate the correlation the way the series are currently.

Are these the series that you use for your correlation?

&gt; series1(3,7,1,0,8,2)the last two elements are removed

and

&gt; series2(1,0,8,2,4,1) the series with a lag of two
",2,1
418,2015-9-21,2015,9,21,10,3lqxm8,Connectionist temporal classification outputs,https://www.reddit.com/r/MachineLearning/comments/3lqxm8/connectionist_temporal_classification_outputs/,speechMachine,1442798602,"I was reading through Alex Graves' work on the CTC for TIMIT phoneme recognition. My question is simple, when he outputs the graphs showing what the CTC outputs for each particular phoneme, he shows a set of spikes. For example Fig 1 and Fig 4 of his ICML 2006 paper (http://www.cs.toronto.edu/~graves/icml_2006.pdf). Which variable in the paper corresponds to those spiked outputs? Are they framewise posterior probabilities plotted against time (i.e. time instance for each frame). Or is that the result after his prefix search decoding and after applying the symbol collapsing he talks about in Section 3 of that paper?",0,2
419,2015-9-21,2015,9,21,12,3lrabw,Solving for the state of a hidden layer,https://www.reddit.com/r/MachineLearning/comments/3lrabw/solving_for_the_state_of_a_hidden_layer/,kcimc,1442804579,"I would like to know what fc7 state produces a given fc8 state in VGG. I tried to apply the technique from deep dream of using fixed step size gradient ascent, but I'm not seeing anything useful:

https://gist.github.com/kylemcdonald/4d154d48b8d9e048b103

First, I initialize fc7 with noise, do a feed forward step to fc8\* then find the error relative to a one-hot vector with only the first element on. Then I do a backward step from fc8 to fc7 and modify the state at fc7 to get closer to giving me the fc8 I want.

I also tried going between fc7 and the prob/softmax layer (where the one-hot representation makes more sense), more iterations with smaller step sizes, smaller random numbers, and a few other things. But nothing gives me consistent states in the fc7 layer each time I run this. Is this because I'm doing something wrong, or is it because there are many different fc7 states that can produce the same fc8 state?

\* For some reason I have to use start=fc8, because start=fc7 will destroy the values in fc7.",2,3
420,2015-9-21,2015,9,21,12,3lre62,Hugo Larochelle's Notes on Dropout as data augmentation,https://www.reddit.com/r/MachineLearning/comments/3lre62/hugo_larochelles_notes_on_dropout_as_data/,muktabh,1442806436,,0,23
421,2015-9-21,2015,9,21,13,3lrltq,Why do we have to shuffle the data? Especially for unsupervised word embedding training,https://www.reddit.com/r/MachineLearning/comments/3lrltq/why_do_we_have_to_shuffle_the_data_especially_for/,ihsgnef,1442810768,"When Mikolov discussed with other people about training Paragraph Vector in [this google group thread](https://groups.google.com/forum/#!searchin/word2vec-toolkit/paragraph$20vector/word2vec-toolkit/Q49FIrNOQRo/t5vv5zCtmaMJ), he said that you have to shuffle the data in order for hierarchical softmax to give the correct result. You don't have to shuffle when negative sampling is used. 

Can you explain why is that? Especially why not shuffling the data  when using hierarchical softmax will produce better than correct result as in the case of paragraph vector?",1,2
422,2015-9-21,2015,9,21,14,3lroyo,Hugo Larochelle's Neural Net lectures are 16.5 hours long (16:32:11 to be precise),https://www.reddit.com/r/MachineLearning/comments/3lroyo/hugo_larochelles_neural_net_lectures_are_165/,ideasm,1442812613,"[YouTube playlist](https://www.youtube.com/playlist?list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH). I started watching these today, and couldn't figure out how long it was in total. Here's the JavaScript I used to get '16:32:11':

    var totalSeconds = 0;
    var timestamps = document.getElementsByClassName('timestamp');
    for (var i = 0; i &lt; timestamps.length; i++) {
      var timeParts = timestamps[i].innerText.split(':');
      totalSeconds += 60 * parseInt(timeParts[0], 10) + parseInt(timeParts[1], 10);
    }

    console.log(totalSeconds);
    console.log(parseInt(totalSeconds / 3600) + ':' + parseInt((totalSeconds % 3600) / 60) + ':' + (totalSeconds % 60));",0,0
423,2015-9-21,2015,9,21,14,3lrrfv,A really cool contest challenging participants to determine emotions behind voice samples,https://www.reddit.com/r/MachineLearning/comments/3lrrfv/a_really_cool_contest_challenging_participants_to/,raghum2015,1442814159,,2,3
424,2015-9-21,2015,9,21,14,3lrruo,Can analogue computers solve optimisation problems any faster than digital ones?,https://www.reddit.com/r/MachineLearning/comments/3lrruo/can_analogue_computers_solve_optimisation/,fuckinghelldad,1442814425,,9,11
425,2015-9-21,2015,9,21,15,3lrxwp,Key detection on #Beatles with simple LogisticRegression: ~38% accuracy.,https://www.reddit.com/r/MachineLearning/comments/3lrxwp/key_detection_on_beatles_with_simple/,cast42,1442818528,,0,0
426,2015-9-21,2015,9,21,16,3lryh8,ECML-PKDD 2015 Review,https://www.reddit.com/r/MachineLearning/comments/3lryh8/ecmlpkdd_2015_review/,cast42,1442818921,,0,5
427,2015-9-21,2015,9,21,16,3lrzbe,Label propagation using deep learning,https://www.reddit.com/r/MachineLearning/comments/3lrzbe/label_propagation_using_deep_learning/,spidey-fan,1442819491,"Hi,

Long time stalker, first time poster here.
I was wondering if there has been any research on label propagation for semi-supervised learning using deep neural networks specifically?

Thanks in advance.",0,1
428,2015-9-21,2015,9,21,17,3ls4ar,NASA Puts its Faith in Deep Belief Networks for Massive Satellite Datasets,https://www.reddit.com/r/MachineLearning/comments/3ls4ar/nasa_puts_its_faith_in_deep_belief_networks_for/,[deleted],1442823193,[deleted],2,0
429,2015-9-21,2015,9,21,19,3lsh9g,Stan: A Probabilistic Programming Language,https://www.reddit.com/r/MachineLearning/comments/3lsh9g/stan_a_probabilistic_programming_language/,siddharth-agrawal,1442832906,,45,79
430,2015-9-21,2015,9,21,20,3lsldl,"Software Is Smart Enough for SAT, but Still Far From Intelligent",https://www.reddit.com/r/MachineLearning/comments/3lsldl/software_is_smart_enough_for_sat_but_still_far/,superfunny,1442835649,,0,4
431,2015-9-21,2015,9,21,21,3lsrgo,Extracting Structured Data From Recipes Using Conditional Random Fields,https://www.reddit.com/r/MachineLearning/comments/3lsrgo/extracting_structured_data_from_recipes_using/,mikkom,1442839312,,0,7
432,2015-9-21,2015,9,21,22,3lsv5i,How do I get into machine learning?,https://www.reddit.com/r/MachineLearning/comments/3lsv5i/how_do_i_get_into_machine_learning/,ph3rn,1442841336,"I'm a first year computer science student, and I'm very interested in machine learning. 

Also, are there any extra courses I should be taking to focus on this field? Calculus is only mandatory at my uni for computer science students till calculus II. Should I be taking III as well?

**EDIT**: Thank you, everyone, for the great advice!",34,20
433,2015-9-21,2015,9,21,22,3lszrn,Is there a typical way to preprocess images before learning in a neural network?,https://www.reddit.com/r/MachineLearning/comments/3lszrn/is_there_a_typical_way_to_preprocess_images/,LyExpo,1442843605,"I have played with the toy MNIST dataset, where everything was nice, small, clean, and black and white. I would now like to play with some ""real"" images. I have RGB images that are approximately 1000 pixels wide by 500 tall. This is a big jump from 28x28. What is typically done in this situation? Can I just encode the each image into one very long vector (1000 x 500 x 3)? Should I just focus on one color, say blue? Can I reduce the resolution somehow?",9,3
434,2015-9-21,2015,9,21,23,3lt1yz,Adapting Resilient Propagation for Deep Learning,https://www.reddit.com/r/MachineLearning/comments/3lt1yz/adapting_resilient_propagation_for_deep_learning/,NasenSpray,1442844623,,4,2
435,2015-9-21,2015,9,21,23,3lt4mt,StrepHit: automatic Wikidata references via NLP,https://www.reddit.com/r/MachineLearning/comments/3lt4mt/strephit_automatic_wikidata_references_via_nlp/,hell_j,1442845879,,0,1
436,2015-9-22,2015,9,22,0,3ltbbh,The Bayes Classifier: building a tweet sentiment analysis tool,https://www.reddit.com/r/MachineLearning/comments/3ltbbh/the_bayes_classifier_building_a_tweet_sentiment/,[deleted],1442848728,[deleted],0,0
437,2015-9-22,2015,9,22,1,3ltl1r,Comparing Four Machine Learning APIs Performance,https://www.reddit.com/r/MachineLearning/comments/3ltl1r/comparing_four_machine_learning_apis_performance/,dabshitty,1442852686,,0,1
438,2015-9-22,2015,9,22,1,3ltms4,Predicting match outcomes for the 2015 Rugby World Cup,https://www.reddit.com/r/MachineLearning/comments/3ltms4/predicting_match_outcomes_for_the_2015_rugby/,ArnuP,1442853387,"Hey everyone! My friend and I have created a web application for the 2015 Rugby World Cup. It uses machine learning to compute a probability for each team winning a match against an opponent. 

Link:  http://rwcpredictor.herokuapp.com/

We have also included a page describing the entire creation process, for those interested.",9,2
439,2015-9-22,2015,9,22,2,3ltsnt,Has anybody tried using dropout with logistic regression?,https://www.reddit.com/r/MachineLearning/comments/3ltsnt/has_anybody_tried_using_dropout_with_logistic/,fjeg,1442855708,"I usually use L1 or L2 regularization for classification with logistic regression, but considering logistic regression is a single node neural network, there is some justification for using dropout as a regularization method.

The cons are obviously that it takes a much longer time to learn the model, but I'm wondering if there are any gains from a classification performance perspective. Clearly it has done wonders in neural networks beyond L1/L2 regularization, but I don't know if such results only work in deep models.",8,2
440,2015-9-22,2015,9,22,2,3ltsvv,For dictionary learning what's the advantage of alternating minimization VS plain SGD,https://www.reddit.com/r/MachineLearning/comments/3ltsvv/for_dictionary_learning_whats_the_advantage_of/,nivwusquorum,1442855801,"Hi,

I am completely new to Dictionary Learning problem, so this might be a stupid question. I implemented DL by taking the derivative of the joint objective wrt. dictionary matrix and sparse vectors and I did SGD on both parameters at the same time. It seems to work well, at least for my application.

From what I've heard people often perform alternating minimization which is if I understand correctly just like the method above except we keep one of the parameters constant at even iterations and the other at the odd iterations. Here keeping constant means not taking a step with a solver.

Is there any advantage of using the second method over the first one?",4,2
441,2015-9-22,2015,9,22,3,3lu2io,Choosing machine learning for the graduation work.,https://www.reddit.com/r/MachineLearning/comments/3lu2io/choosing_machine_learning_for_the_graduation_work/,Larleyt,1442859719,"Hello, people! 
I'm 4th grade CS student and I have to choose my bachelor's degree work subject in a month. What I'm thinking about as a subject is ML. I even have some experience in this (have done some courses on coursera.org, watched CalTech lectures, etc) and I quite like it. But I have, like, zero realistic ideas about what in ML can be chosen to work on in my paper. I don't have this, you know, big picture yet that high experienced people have. So, maybe you, guys, could advise me something that is not too complicated for graduation work but has enough space to work on? Most of the ideas I could think about were quite impossible to implement in real life.
Thanks in advance.",4,1
442,2015-9-22,2015,9,22,3,3lu3u2,LPIC-1 Linux certification: Server Professional (10 courses),https://www.reddit.com/r/MachineLearning/comments/3lu3u2/lpic1_linux_certification_server_professional_10/,[deleted],1442860255,[deleted],0,1
443,2015-9-22,2015,9,22,3,3lu5je,Looking for Connections in Your Data  Correlation Coefficients,https://www.reddit.com/r/MachineLearning/comments/3lu5je/looking_for_connections_in_your_data_correlation/,czuriaga,1442860964,,0,1
444,2015-9-22,2015,9,22,4,3lud70,Plotting ImageNet object similarity with VGG &amp; t-SNE,https://www.reddit.com/r/MachineLearning/comments/3lud70/plotting_imagenet_object_similarity_with_vgg_tsne/,kcimc,1442863947,,3,5
445,2015-9-22,2015,9,22,5,3lujrf,resources for learning about neural nets?,https://www.reddit.com/r/MachineLearning/comments/3lujrf/resources_for_learning_about_neural_nets/,[deleted],1442866597,[deleted],0,0
446,2015-9-22,2015,9,22,6,3lus2w,Uncertainty in Artificial Intelligence 2015: videos are online,https://www.reddit.com/r/MachineLearning/comments/3lus2w/uncertainty_in_artificial_intelligence_2015/,pierrelux,1442869925,,0,27
447,2015-9-22,2015,9,22,7,3lv1pn,Access to {AI} - a conference for introducing AI/ML to programmers who've never done it before,https://www.reddit.com/r/MachineLearning/comments/3lv1pn/access_to_ai_a_conference_for_introducing_aiml_to/,Aaronontheweb,1442873897,,0,3
448,2015-9-22,2015,9,22,7,3lv4nn,PRs are welcomed for other not-yet-implemented metric learning algorithms in R,https://www.reddit.com/r/MachineLearning/comments/3lv4nn/prs_are_welcomed_for_other_notyetimplemented/,[deleted],1442875159,[deleted],0,4
449,2015-9-22,2015,9,22,8,3lvdiz,How do Neural Machine Translation systems compare to Google Translate?,https://www.reddit.com/r/MachineLearning/comments/3lvdiz/how_do_neural_machine_translation_systems_compare/,SuperFX,1442879101,"Are the recent Neural Machine Translation systems competitive with Google Translate or not at all? Looking at recent papers like [1], the BLEU scores reported are around ~25 on English-German, while this [2] analysis suggests that Google Translate scores around ~80 on English-German. Is there really that large a gap? Or is it a function of the data set that the state of the art is reported on?

[1] http://arxiv.org/abs/1508.04025

[2] http://faculty.bus.olemiss.edu/maiken/pairs.htm",25,30
450,2015-9-22,2015,9,22,11,3lw2aj,How can I get motivated with VAE? Why should I make the effort to learn it? Why is so many people publishing papers about it?,https://www.reddit.com/r/MachineLearning/comments/3lw2aj/how_can_i_get_motivated_with_vae_why_should_i/,andrewbarto28,1442890599,,15,25
451,2015-9-22,2015,9,22,13,3lwat6,Enabling GoTo in Python,https://www.reddit.com/r/MachineLearning/comments/3lwat6/enabling_goto_in_python/,john_philip,1442895015,,3,0
452,2015-9-22,2015,9,22,15,3lwlim,Barrel Puller,https://www.reddit.com/r/MachineLearning/comments/3lwlim/barrel_puller/,dongfengpacking,1442901793,,1,1
453,2015-9-22,2015,9,22,15,3lwnz0,Auotmatic 4 cavities blow molding machine,https://www.reddit.com/r/MachineLearning/comments/3lwnz0/auotmatic_4_cavities_blow_molding_machine/,dongfengpacking,1442903501,,1,1
454,2015-9-22,2015,9,22,15,3lwo08,Deep Belief Networks at Heart of NASA Image Classification,https://www.reddit.com/r/MachineLearning/comments/3lwo08/deep_belief_networks_at_heart_of_nasa_image/,pilooch,1442903529,,4,5
455,2015-9-22,2015,9,22,15,3lwoqf,filling capping labeling machine inline,https://www.reddit.com/r/MachineLearning/comments/3lwoqf/filling_capping_labeling_machine_inline/,dongfengpacking,1442904110,,1,1
456,2015-9-22,2015,9,22,16,3lwrcc,How much is deep learning useful for social scientists or information scientists?,https://www.reddit.com/r/MachineLearning/comments/3lwrcc/how_much_is_deep_learning_useful_for_social/,napsternxg,1442906063,"Deep learning systems are very popular these days, especially because of their amazing performance on image, audio and text data. I would consider different kinds of multi layered ANN architectures a part of Deep Learning systems. However, most of the recent research in Deep Learning has been on performing well on prediction tasks while using large amounts of data. 

For a social scientist, in my opinion, most times the data is not very big and the focus is on fully understanding the data, identifying interesting patterns in the data, make inferences and maybe do a few predictions. Also, most of the time social scientists will deal with text data like news reports, social media posts, bibliometric records, etc. 

Considering all this how useful is it for a social scientist to use Deep Learning techniques and what gain will they get in their general analysis e.g. Topic Modelling, feature correlation, etc. How can deep learning be used in a better way in this domain rather than using Logistic Regression or SVM, even for predictive tasks?

What are some resources where people from non CS and non industry domain have started using Deep Learning techniques?

Is deep learning effective because:
- it can learn non-linear decision boundaries?
- it can be trained in an online fashion?
- it can be used to model recurrent or recursive dependencies in the data (HMM can also do some of these things)?
- there is less focus on feature engineering?
- it is a lego type stack and move approach?
- it relies of huge amounts of data?

Is it effective in text analysis domain because: 
- of Word Vectors ?
- RNN (long range sequence learning)?
- RBMs usage in collaborative filtering tasks?

I have been quite fascinated by Neural Networks and Deep Learning advancements, however, I am skeptical if it is only usable for industries where they have large amounts of data. Are there examples of it being used on smaller scale with useful results. (I know the Facebook bABI dataset is small and the results using RNN on that dataset have been pretty decent)

I would love to hear your thoughts on this. ",1,7
457,2015-9-22,2015,9,22,17,3lwxaj,"10 Scikit Learn Case Studies, Examples &amp; Tutorials",https://www.reddit.com/r/MachineLearning/comments/3lwxaj/10_scikit_learn_case_studies_examples_tutorials/,earleanperchinski,1442910963,,1,27
458,2015-9-22,2015,9,22,21,3lxd4a,How to generate feature of whole song based on features of frame.,https://www.reddit.com/r/MachineLearning/comments/3lxd4a/how_to_generate_feature_of_whole_song_based_on/,raasalguu,1442923250,I want to perform a classification task on a music data set. I am not sure how to generate the features for the whole song. How can I use MEL coefficients(Or any other) of each frame and generate feature for whole song.,9,8
459,2015-9-22,2015,9,22,22,3lxl6i,"Could we, with current technology, have AIs with an internal model of reality, even if it's not perfect and only limited to very basic informations?",https://www.reddit.com/r/MachineLearning/comments/3lxl6i/could_we_with_current_technology_have_ais_with_an/,2Punx2Furious,1442927668,"I mean, I know we can, but could it be *useful* like it is for humans? Could the AI actually understand and use the data? Could it guess, and refine the guesses of data it doesn't have based on hypotheses?

For example, we know that if it's 12:00 there's a good chance there is the sun outside, even if we are in a sealed room and can't see outside, so we can use this data. We know that if someone leaves our field of view, they probably still exist, even if we can't be sure of it unless we check.

I know some AIs can predict some data, but can they do it at this level of abstraction yet?",10,1
460,2015-9-23,2015,9,23,0,3ly01d,Machine Learning &amp; Deep Neural Networks Explained,https://www.reddit.com/r/MachineLearning/comments/3ly01d/machine_learning_deep_neural_networks_explained/,OneTwelve,1442934691,,0,7
461,2015-9-23,2015,9,23,0,3ly1u3,Google Science Fair project that generates multiple choice questions from inputted texts.,https://www.reddit.com/r/MachineLearning/comments/3ly1u3/google_science_fair_project_that_generates/,venommax,1442935443,,0,10
462,2015-9-23,2015,9,23,0,3ly4f2,Torch7 blog post : Recurrent Models for Visual Attention. Code included. REINFORCE algorithm. 0.85% on MNIST.,https://www.reddit.com/r/MachineLearning/comments/3ly4f2/torch7_blog_post_recurrent_models_for_visual/,torch7,1442936527,,5,38
463,2015-9-23,2015,9,23,0,3ly4po,Sentiment Analysis Benchmark: why sentiment analysis is hard and how the available APIs perform,https://www.reddit.com/r/MachineLearning/comments/3ly4po/sentiment_analysis_benchmark_why_sentiment/,wildcodegowrong,1442936654,,0,1
464,2015-9-23,2015,9,23,1,3ly72w,AI system solves SAT geometry questions as well as average human test taker,https://www.reddit.com/r/MachineLearning/comments/3ly72w/ai_system_solves_sat_geometry_questions_as_well/,aperrien,1442937625,,0,1
465,2015-9-23,2015,9,23,1,3lyaid,A Beginners Guide to Deep Neural Networks,https://www.reddit.com/r/MachineLearning/comments/3lyaid/a_beginners_guide_to_deep_neural_networks/,dabshitty,1442938999,,1,20
466,2015-9-23,2015,9,23,1,3lydz3,I work in the film industry and have strange request / question,https://www.reddit.com/r/MachineLearning/comments/3lydz3/i_work_in_the_film_industry_and_have_strange/,Nickisnoble,1442940410,"**Some background:**

I work in the art dept for movies and TV shows. We have to make everything you see in a scene, from police paperwork to beer labels, no matter how short the screen time. 

We usually have extremely short deadlines, and creating hundreds of pieces (especially since a lot of these won't ever be seen up close or even in focus) is super 'expensive' time wise.

I'm also a front end dev and a huge nerd, and when I saw [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) and more specifically [DRAW](https://www.youtube.com/watch?v=Zt-7MI9eKEo) it hit me that something like this might be able to be used to generate a large amount art in a short(er) amount of time.

**My question is this:**

Is the tech there yet where I could drop in a folder (or two for good/bad examples) of images as a training set, and after a while have the program generate a great deal more of similar images. 

They don't have to *be* right, they just have to *look* right at a distance.

Good examples of use cases would be this [fake police paperwork](http://imgur.com/a/rbBfw) (maybe even with 'handwriting' if that was included in the training set? Note that these also would have to always be 8.5x11in) or these [1970s magazine ads](http://imgur.com/a/IjF6z) (nothing needs to be real english  just look like it from a distance where it wouldn't be readable anyway, if that makes sense). Of course those are considerably less images than I'd have to find initially to train (right?), but it's a good example of the possible targets.

Possible? Not possible? Wanna help me do it? 

I realize that by being not *super* technical, I might be imagining what can/can't be done.

Anyway, thanks for reading, if you have any pointers or thoughts let me know!

**TL;DR: I want to generate large quantities of specific types of images that will be convincing from a distance, though not necessarily up close.**

*PS: This was a X-post from /r/neuralnetworks*",46,88
467,2015-9-23,2015,9,23,3,3lypfv,"Kaggle Caterpillar Winners' Interview: 1st place, Gilberto | Josef | Leustagos | Mario",https://www.reddit.com/r/MachineLearning/comments/3lypfv/kaggle_caterpillar_winners_interview_1st_place/,ledmmaster,1442944990,,0,2
468,2015-9-23,2015,9,23,5,3lz82i,In-depth analysis on the use of data science in the 2008 and 2012 Presidential Elections | MIT Technology Review,https://www.reddit.com/r/MachineLearning/comments/3lz82i/indepth_analysis_on_the_use_of_data_science_in/,dive118,1442952437,,1,16
469,2015-9-23,2015,9,23,8,3m02vm,Using Machine Learning to map out Human Values and Morality,https://www.reddit.com/r/MachineLearning/comments/3m02vm/using_machine_learning_to_map_out_human_values/,DeltaPositionReady,1442965801,"I'm currently doing a research paper on AI and I've stumbled across some interesting literature about undefined human values and the potential impacts. 

From reading up on Bostrom and Kurzweil, Kahneman, etc etc, they all seem to parrot this same idea. 

So my question is, could we use machine learning to have a program that learns what humans value and continues the projection into the future? In an effort to determine what it is about human values that makes them exist? 

Sorry for philosophical wankery. I am doing a Comp Sci degree and really would like to specialise in Machine Learning and know it's not the same as AI.",2,0
470,2015-9-23,2015,9,23,10,3m0ccf,Comparing/Mapping K-means Cluster Centers Month-by-month,https://www.reddit.com/r/MachineLearning/comments/3m0ccf/comparingmapping_kmeans_cluster_centers/,slaw07,1442970278,"A colleague of mine has some monthly data that they've normalized using Z-score. So, each month, the data is normalized relative to the mean and stddev of that month and K-means is performed (K = 10). This is repeated again for the next month with that month's mean and stddev and so forth for every month. The assumption that he's made is that the cluster centers don't change that much over time and so they can be mapped to the previous month's cluster centers (based on euclidean distance).

I pointed out that while it is okay to use Z-score normalization for K-means clustering, you may not be able to compare cluster centers month-over-month as you aren't guaranteed that the normalized cluster centers actually mean the same thing. In other words, you could potentially have the same set of the values for the cluster centers from one month to the next (or some time in the next 5 years) but the distribution could have shifted its mean and stddev without affecting the Z-score. 

Now, we have a nice way of mapping one set of cluster centers onto another set so that's settled. However, is there any other reasons that Z-score normalized cluster centers would prevent me from comparing/mapping cluster centers from month to month and other methods might be better (like applying a log transformation - which might squash lower values)?",0,2
471,2015-9-23,2015,9,23,12,3m0rbk,"ZT ( Zhongtuo Roll Forming Machinery Co., Ltd.) is a company specialising in the field of cold roll forming machines and metal sheet processing.",https://www.reddit.com/r/MachineLearning/comments/3m0rbk/zt_zhongtuo_roll_forming_machinery_co_ltd_is_a/,laomeng,1442977351,,1,0
472,2015-9-23,2015,9,23,12,3m0sif,"ZT ( Zhongtuo Roll Forming Machinery Co., Ltd.) is a company specialising in the field of cold roll forming machines and metal sheet processing.",https://www.reddit.com/r/MachineLearning/comments/3m0sif/zt_zhongtuo_roll_forming_machinery_co_ltd_is_a/,laomeng,1442977965,[removed],0,0
473,2015-9-23,2015,9,23,18,3m1ng4,"I want to perform Linear Regression on passages of text, (articles)",https://www.reddit.com/r/MachineLearning/comments/3m1ng4/i_want_to_perform_linear_regression_on_passages/,wan2Kno,1442999302,"I would ideally like to use a large number of features in to my LR, where can I find a large list of possible candidate features?",4,1
474,2015-9-23,2015,9,23,18,3m1oyi,Help with Weka,https://www.reddit.com/r/MachineLearning/comments/3m1oyi/help_with_weka/,janonthecanon7,1443000544,"I want to create a bayesian network in Weka with quite a lot of data. I have looked at the documentation for the .arff format, and there I found relational attributes, but I can't find any info about adding other relational attributes within a relational attribute. Basically, what I'm wondering is if the format bellow would work. 
Also I'm wondering if there is a way to visualize the network. I need this to explain a prediction. 

    @relation testData
    
    @attribute name {first, second, third}
    @attribute levelOne relational
        @attribute levelTwoFirst relational
            @attribute someData1 numeric
            @attribute someData2 numeric
            @attribute someData3 numeric
        @attribute levelTwoSecond relational
            @attribute someMoreData1 numeric
            @attribute someMoreData2 numeric
            @attribute someMoreData3 numeric
    @attribute predictMe {One, Two Three}
",0,3
475,2015-9-23,2015,9,23,19,3m1tkc,Using Distributed Representations for logical inferences/queries,https://www.reddit.com/r/MachineLearning/comments/3m1tkc/using_distributed_representations_for_logical/,muktabh,1443004102,,2,16
476,2015-9-23,2015,9,23,21,3m22ha,"""The Master Algorithm"" by Pedro Domingus seems to be getting good reviews. Have you read it, and what do you think?",https://www.reddit.com/r/MachineLearning/comments/3m22ha/the_master_algorithm_by_pedro_domingus_seems_to/,[deleted],1443010292,[deleted],0,1
477,2015-9-23,2015,9,23,21,3m22il,"""The Master Algorithm"" by Pedro Domingus seems to be getting good reviews. Have you read it, and what do you think?",https://www.reddit.com/r/MachineLearning/comments/3m22il/the_master_algorithm_by_pedro_domingus_seems_to/,onewugtwowugs,1443010321,[Amazon](http://www.amazon.com/Master-Algorithm-Ultimate-Learning-Machine-ebook/dp/B012271YB2/ref=sr_1_1?s=digital-text&amp;ie=UTF8&amp;qid=1443010128&amp;sr=1-1&amp;keywords=the+master+algorithm). [Goodreads](https://www.goodreads.com/book/show/24612233-the-master-algorithm?ac=1).,11,12
478,2015-9-23,2015,9,23,21,3m256i,"3rd Deep Learning Summit is taking place in London tomorrow! I know you guys were interested in the past so if you can't join us, follow news on twitter with #reworkDL - join the discussion!",https://www.reddit.com/r/MachineLearning/comments/3m256i/3rd_deep_learning_summit_is_taking_place_in/,reworksophie,1443011938,,0,2
479,2015-9-23,2015,9,23,22,3m27wp,Text Classification,https://www.reddit.com/r/MachineLearning/comments/3m27wp/text_classification/,ChotiDon,1443013423,"I am working on text classification. Previously, I have classified text into three classes.i.e. Positive negative and neutral.
Now i further want to classify text into 12 classes derived from positive and negative. I am unable to figure out what strategy should I use. I am working in R language.",2,1
480,2015-9-23,2015,9,23,22,3m29yq,Unleash Your Creativity with Laser Marking Machines and CO2 laser Engraving Machine,https://www.reddit.com/r/MachineLearning/comments/3m29yq/unleash_your_creativity_with_laser_marking/,HangRex,1443014402,[removed],0,0
481,2015-9-23,2015,9,23,22,3m2bwa,deep learning - million parameters?,https://www.reddit.com/r/MachineLearning/comments/3m2bwa/deep_learning_million_parameters/,apple-sauce,1443015325,"Hi all, I'm a newb to deep learning (though familiar with neural nets). 

I was reading this [paper](https://www.nvidia.cn/content/tesla/pdf/machine-learning/imagenet-classification-with-deep-convolutional-nn.pdf) where they mention a staggering 60 million parameters! I know with regular neural nets you have momentum and learning rate. So my questions is, how did they get to 60M parameters?",7,1
482,2015-9-23,2015,9,23,23,3m2iwb,[1509.06664] Reasoning about Entailment with Neural Attention,https://www.reddit.com/r/MachineLearning/comments/3m2iwb/150906664_reasoning_about_entailment_with_neural/,egrefen,1443018535,,3,16
483,2015-9-23,2015,9,23,23,3m2l3s,An interview with Pedro Domingos on his new book: A Master Algorithm in Machine Learning Could Change Everything - Modern Notion,https://www.reddit.com/r/MachineLearning/comments/3m2l3s/an_interview_with_pedro_domingos_on_his_new_book/,sameyeam,1443019470,,11,53
484,2015-9-24,2015,9,24,1,3m2wjj,bug in torch program?,https://www.reddit.com/r/MachineLearning/comments/3m2wjj/bug_in_torch_program/,[deleted],1443024288,[deleted],1,3
485,2015-9-24,2015,9,24,1,3m308o,"Looking for ""Data Engineers"" to interview [xpost /r/datascience]",https://www.reddit.com/r/MachineLearning/comments/3m308o/looking_for_data_engineers_to_interview_xpost/,mmmayo13,1443025768,"I am a current computer science grad student working a machine learning/parallel programming thesis. I am a contributor to a well-known data science information website, and am re-starting my own blog dedicated to similar information. I'm not sure which arena a forthcoming informational post on Data Engineers would end up in, but I will obviously provide links in said post to willing participants contact/bio information.

That said, I'm looking to interview a few Data Engineers for an upcoming post. There would be 5 simple open-ended style questions which I assume could be answered in a relatively short amount of time. I intend to put together a post detailing the best advice from some people who carry the title of Data Engineer, and since there is a lot of overlap between data { scientists | engineers | analysts | etc. }, this post will both highlight and attempt to somewhat clarify this point.

If you are a Data Engineer, would like take part in a quick interview to help clarify what a DE is and what he or she does, and you have a few minutes to spare, I'd genuinely like to hear from you. Please send me a DM.

Thanks.",2,1
486,2015-9-24,2015,9,24,2,3m36wq,What are the best known training functions for Deep Neural Networks for function approximation tasks?,https://www.reddit.com/r/MachineLearning/comments/3m36wq/what_are_the_best_known_training_functions_for/,Aerospacio,1443028427,"Pretty much the title, i know sometimes its dependent on the problem but is there some paper that compares them for different tasks?",5,1
487,2015-9-24,2015,9,24,2,3m37eh,"Word, graph and manifold embedding from Markov processes",https://www.reddit.com/r/MachineLearning/comments/3m37eh/word_graph_and_manifold_embedding_from_markov/,muktabh,1443028624,,2,14
488,2015-9-24,2015,9,24,2,3m3c4m,Need grip and advice with gpu set up,https://www.reddit.com/r/MachineLearning/comments/3m3c4m/need_grip_and_advice_with_gpu_set_up/,Pysnap,1443030419,[removed],0,1
489,2015-9-24,2015,9,24,3,3m3l55,The Future Impact of Machine Learning,https://www.reddit.com/r/MachineLearning/comments/3m3l55/the_future_impact_of_machine_learning/,czuriaga,1443033876,,0,1
490,2015-9-24,2015,9,24,7,3m4f1u,"Nuit Blanche: Saturday Morning Videos: Videos and Slides, Deep Learning Summer School, Montreal 2015",https://www.reddit.com/r/MachineLearning/comments/3m4f1u/nuit_blanche_saturday_morning_videos_videos_and/,True-Creek,1443045779,,0,7
491,2015-9-24,2015,9,24,7,3m4kmb,the machine learning challenge at apple,https://www.reddit.com/r/MachineLearning/comments/3m4kmb/the_machine_learning_challenge_at_apple/,toisanji,1443048138,,2,0
492,2015-9-24,2015,9,24,9,3m51vk,What sorts of math classes are essential to be competitive for ML Masters programs?,https://www.reddit.com/r/MachineLearning/comments/3m51vk/what_sorts_of_math_classes_are_essential_to_be/,foxh8er,1443055824,"I'm considering a math minor - what sorts of classes would be ideal for ML that aren't typically in CS undergrad coursework?

I have taken or will take Multivariable Calculus, Calculus based Statistics,  Linear Algebra, Discrete Math.",11,5
493,2015-9-24,2015,9,24,10,3m59bg,EMNLP'15 Recap - The deep learning tsunami continues to take over NLP,https://www.reddit.com/r/MachineLearning/comments/3m59bg/emnlp15_recap_the_deep_learning_tsunami_continues/,vkhuc,1443059208,,16,55
494,2015-9-24,2015,9,24,10,3m59r1,Facebook Is Using Our Data To Build The 'World's Best' Artificial Intelligence Lab,https://www.reddit.com/r/MachineLearning/comments/3m59r1/facebook_is_using_our_data_to_build_the_worlds/,[deleted],1443059393,[deleted],0,1
495,2015-9-24,2015,9,24,12,3m5pd7,DEEP REINFORCEMENT LEARNING WITH DOUBLE Q-LEARNING,https://www.reddit.com/r/MachineLearning/comments/3m5pd7/deep_reinforcement_learning_with_double_qlearning/,john_philip,1443066920,,3,0
496,2015-9-24,2015,9,24,19,3m6m6e,Dr. Wang: a robot doctor based on PubMed abstracts,https://www.reddit.com/r/MachineLearning/comments/3m6m6e/dr_wang_a_robot_doctor_based_on_pubmed_abstracts/,godspeed_china,1443090514,www.drwang.top,0,0
497,2015-9-24,2015,9,24,20,3m6rkb,Learning in LSTMs with other methods than BPTT,https://www.reddit.com/r/MachineLearning/comments/3m6rkb/learning_in_lstms_with_other_methods_than_bptt/,[deleted],1443094522,[deleted],7,2
498,2015-9-24,2015,9,24,20,3m6rv0,"Machine Learning (ML) and big data analytics go hand-in-hand so ML focuses on prediction, based on properties learned from earlier data.",https://www.reddit.com/r/MachineLearning/comments/3m6rv0/machine_learning_ml_and_big_data_analytics_go/,BrysonAVincent2,1443094721,,0,1
499,2015-9-24,2015,9,24,21,3m6z7x,Best way to implement naive bayes on text data stored in a sqlite database?,https://www.reddit.com/r/MachineLearning/comments/3m6z7x/best_way_to_implement_naive_bayes_on_text_data/,nanermaner,1443099150,"I have a bunch of classified data in a sqlite database. I'm hoping to use the scikit learn naive bayes in python to train and test a naive bayes classifier on this data. [Here is the library I'm using](http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html).

I basically have a sqlite database with two columns, a chunk of text data, and then that data's class.

Thanks for your help!",1,0
500,2015-9-24,2015,9,24,22,3m71fu,tube filling and sealing machine plastic tube,https://www.reddit.com/r/MachineLearning/comments/3m71fu/tube_filling_and_sealing_machine_plastic_tube/,dongfengpacking,1443100392,,1,1
501,2015-9-24,2015,9,24,23,3m7b23,Beginners Guide to Deep Neural Networks Explained By Google Researchers,https://www.reddit.com/r/MachineLearning/comments/3m7b23/beginners_guide_to_deep_neural_networks_explained/,[deleted],1443104882,[deleted],1,0
502,2015-9-24,2015,9,24,23,3m7dtx,The Data Science Workflow,https://www.reddit.com/r/MachineLearning/comments/3m7dtx/the_data_science_workflow/,balgan,1443106107,,1,4
503,2015-9-25,2015,9,25,0,3m7il3,Good Implementations of RNNs + Fully Differentiable Data Structures?,https://www.reddit.com/r/MachineLearning/comments/3m7il3/good_implementations_of_rnns_fully_differentiable/,Ameren,1443108057,"Are there any well-known implementations of recurrent neural networks combined with fully differentiable data structures? I was reading Deepmind's paper [""Learning to Transduce with Unbounded Memory""](http://arxiv.org/abs/1506.02516) which came out a few months ago, and I wanted to tinker with some of the ideas presented in that paper. I found an implementation of the algorithms [here](https://github.com/PrajitR/NeuralStacksQueues).

There was a minor bug in that code, and I fixed it, but it would be nice if I could find other implementations that did similar things so that I could validate its correctness (it would also be nice to have some alternatives to choose from also).

I'm having to compare different RNN approaches for some research as part of my dissertation, and it would be nice to present, say, a stack-augmented network alongside a range of plain RNN/GRU/LSTM networks so that I can compare their relative performance on different tasks.

Thanks!
",6,7
504,2015-9-25,2015,9,25,1,3m7p3t,Any one read 'The master algorithm' ?,https://www.reddit.com/r/MachineLearning/comments/3m7p3t/any_one_read_the_master_algorithm/,[deleted],1443110714,[deleted],0,1
505,2015-9-25,2015,9,25,1,3m7rlc,Are image segmentation problems typically handled as regression or classification problems?,https://www.reddit.com/r/MachineLearning/comments/3m7rlc/are_image_segmentation_problems_typically_handled/,LyExpo,1443111677,"Suppose we have a data set of images of humane faces with eyeball locations as labelled. If a convolutional neural network is trained, what is the output in the last layer? I can see at least two options:

1) (regression) output 2 pixel locations corresponding to eyeball regions

2) (classification) output an ""eyeball/non-eyeball"" value for every pixel

Of course this depends on how the images are labelled to begin with, but it isn't clear to me how these problems are generally handled. Option 1) above seems much more reasonable, but I'm not sure how well it generalizes to other problems consisting of more labels that may or may not appear in each image.",3,2
506,2015-9-25,2015,9,25,1,3m7rqp,"Python-based urban data science, visualization, and machine learning course at UC Berkeley with IPython notebook lectures and materials on GitHub",https://www.reddit.com/r/MachineLearning/comments/3m7rqp/pythonbased_urban_data_science_visualization_and/,drsosci,1443111734,,4,100
507,2015-9-25,2015,9,25,1,3m7uic,When to retrain your model?,https://www.reddit.com/r/MachineLearning/comments/3m7uic/when_to_retrain_your_model/,[deleted],1443112815,[deleted],7,0
508,2015-9-25,2015,9,25,2,3m81ak,Are neural nets just a type of genetic algorithm?,https://www.reddit.com/r/MachineLearning/comments/3m81ak/are_neural_nets_just_a_type_of_genetic_algorithm/,omniron,1443115391,"I've been learning about NN in more depth, and it occurred to me that NN with back propagation were just a type of genetic algorithm. 

The pathways through a NN are merely a collection of individual polynomial functions initially randomly calculated from an input data, and the process of back-propagation simply ""selects"" for the most successful polynomials, making the impact of randomly seeded polynomials that are poor fits drop to 0. I don't recall seeing this concept pointed out anywhere though, is this an apt understanding of the idea?",9,0
509,2015-9-25,2015,9,25,2,3m864h,Google voice search: faster and more accurate,https://www.reddit.com/r/MachineLearning/comments/3m864h/google_voice_search_faster_and_more_accurate/,vonnik,1443117295,,21,52
510,2015-9-25,2015,9,25,3,3m8ar1,A new kind of summarization technology - context-controllable,https://www.reddit.com/r/MachineLearning/comments/3m8ar1/a_new_kind_of_summarization_technology/,[deleted],1443119104,[deleted],0,1
511,2015-9-25,2015,9,25,7,3m9eql,AWS + Neural Networks = ?,https://www.reddit.com/r/MachineLearning/comments/3m9eql/aws_neural_networks/,ReadItOnRedditt,1443135481,"Has anyone had an experience running Theano based Neural Networks on Amazon Web Services? 

**TLDR: Should I run Theano based NNs on AWS?**

----

They have GPU Instances and apparently Cluster GPU servers (no idea what that means) so NN training should lightning fast. 

I have a Macbook Pro and training NNs and preprocessing data absolutely kills the CPU and RAM. Is it worth the cost to put my NNs up on the cloud?

---

I have ~30GB of data (I use 3-4GB sample). Right now my working data (3-4GB) is on a sqlite3 database on my computer but thats crazy slow and heats up my CPU like crazy (especially for more complex queries). Local Apache + MySQL seems like more trouble than it's worth and I'm not sure that would be much better for my pc.

My NNs destroy my CPU. ~30,000 neurons in total but I only use ~500 - 5,000 at a time while testing components. 

---

Should I invest in learning AWS and setting up a GPU (or maybe just CPU) instance? What about a MySQL database (for local and/or cloud based testing)?


 ",14,1
512,2015-9-25,2015,9,25,9,3m9slk,EOS tag for seq2seq architectures where decode stage involves regression?,https://www.reddit.com/r/MachineLearning/comments/3m9slk/eos_tag_for_seq2seq_architectures_where_decode/,logrech,1443141575,"[Here's](http://i.imgur.com/Tb32eC6.png) a generic seq2seq network. There's an encode stage and a decode stage. The decode stage is prompted by passing in a EOS (end of sequence) tag. Most if not all of the applications of seq2seq I've seen usually generate a sentence (char or word based) for the second sequence, in which case the EOS tag would simply be implemented as an extra word or character in the vocabulary. 

However, if the second sequence is a regression model, how do you prompt the decode stage? So in general, think of an example that takes in a sequence of words and outputs a sequence of numbers. 

There's no way you could assign a number (in the case of single variable regression) or a vector (in the case of multivariate regression) that would function solely as the EOS tags. You could imagine a situation where you use 0 as the EOS tag. If the training data has 0s, meaning you want to train the network to predict 0s as a legitimate output rather than an EOS tag this would be a problem. Of course, you could extend this to any number besides 0. So how do you do this then? 

You could maybe generate a value that is not present in the training data? 

Another concern is, because the EOS and outputs are real valued, how do you know if the network is outputting the tag or outputting another prediction. For example if the tag is 3.5 and the network outputs 3.7. There's no way the network will output the exact value of the tag. ",5,1
513,2015-9-25,2015,9,25,10,3m9zeh,"Other than regressions (including SVM regression), what machine learning algorithms can be used to predict continuous outputs?",https://www.reddit.com/r/MachineLearning/comments/3m9zeh/other_than_regressions_including_svm_regression/,winstonl,1443144449,,2,2
514,2015-9-25,2015,9,25,12,3mae9v,Is the weka platform secure to use with protected data?,https://www.reddit.com/r/MachineLearning/comments/3mae9v/is_the_weka_platform_secure_to_use_with_protected/,St_OP_to_u_chin_me,1443150872,"I'm about to ask my technical security support group and I can't find any relevant links to answer in a basic way "" is the data imported by a user secure "" and at no risk for theft by other groups.",6,0
515,2015-9-25,2015,9,25,13,3maqlt,Parsing XML data,https://www.reddit.com/r/MachineLearning/comments/3maqlt/parsing_xml_data/,maddy2u,1443156940,I have an access database of XMLs that i want to parse and save as a data frame in R. The complexity comes in going through each XML and parsing it and saving it as a dataframe. How do  i go about it? ,5,0
516,2015-9-25,2015,9,25,15,3may3u,How Lubrication System Can Help Prolong the Lifetime Of Your Machinery,https://www.reddit.com/r/MachineLearning/comments/3may3u/how_lubrication_system_can_help_prolong_the/,jackerfrinandis,1443161193,,0,1
517,2015-9-25,2015,9,25,19,3mbhos,Machine generated commentary - where to start?,https://www.reddit.com/r/MachineLearning/comments/3mbhos/machine_generated_commentary_where_to_start/,DucksHaveLowAPM,1443175449,"Hi,  
My company has a lot of data and there are some trends that we measure and display. I was asked to estimate a following task: How hard it would be to generate meaningful comments on why did figures changed?  
I have no idea where to start - I can look at algorithms that describe pictures but I don't know if comments like ""the line goes up"" would suffice. Clients are asking if it would be possible to go further and generate 1 - 3 sentences along the lines:  
- The profit is down 10%, the reason is in February there was an outlier cost in branch X  
- The profit is down 10%, the reason is all sales are down across all branches
So it could understand both outliers and the whole trends (this is a task on it's own but I have experience in that). It doesn't need to be a more sophisticated than that.  
Anyone could give me an idea of what to read / where to start estimating this kind of work? And if anyone did something similar - how complicated is this task, I don't want to build another Watson right now.",4,1
518,2015-9-25,2015,9,25,20,3mbp85,Image Recognition Libraries and Projects List by Language,https://www.reddit.com/r/MachineLearning/comments/3mbp85/image_recognition_libraries_and_projects_list_by/,joandungee,1443181255,,2,18
519,2015-9-25,2015,9,25,21,3mbw96,Can you identify Bumble Bees vs. Honey Bees? - DrivenData Competition,https://www.reddit.com/r/MachineLearning/comments/3mbw96/can_you_identify_bumble_bees_vs_honey_bees/,dat-um,1443185575,,0,13
520,2015-9-25,2015,9,25,22,3mc0ed,What does it mean for an MLP to be a universal approximator?,https://www.reddit.com/r/MachineLearning/comments/3mc0ed/what_does_it_mean_for_an_mlp_to_be_a_universal/,PeterIanStaker,1443187792,"I understand the basic concept of universal approximators, it's basically the same idea as a fourier transform, but with sigmoids, or whatever non-linearity. 

I was having showerthoughts this morning, and couldn't quite wrap my head around why, if I could really represent any function this way, do I need CNN's, or RNN's, or any sort of deep architecture? Shouldn't a sufficiently large MLP be capable of learning whatever it is that these other typologies are doing? 

My intuition is that saying an MLP is a universal approximator is a lot like me saying that a room full of monkeys will type up Shakespeare in an infinite amount of time. Yes, the monkeys may be a ""universal writer"", but you can't really hope for them to do it, because there's no way to communicate the end goal to the monkey. 

I'm not so sure though, because that implies that there is a set of weights for the MLP which will perfectly recreate the behavior of any other type of learner, it just may not be feasible to find. I don't know if that's true here. 

Also, if it is true, what's holding the MLP back? Data? Compute power? Or is finding such a minimum in such a big model via SGD impossible even if you had enough data and power? ",6,5
521,2015-9-25,2015,9,25,22,3mc0ez,Image Classification On Android,https://www.reddit.com/r/MachineLearning/comments/3mc0ez/image_classification_on_android/,pacmanofgb,1443187802,"I'm trying to make an image classifier for an Android App. Right now, I'm looking at trying to use a neural network, but I'm not sure that I'll get reasonable run-times. I'd prefer to do things locally. Is it feasible to do a CNN on an Android App, or should I really just stick to easier things like KNN?",2,0
522,2015-9-25,2015,9,25,23,3mc84f,"With only 22 hours left, the podcast Talking Machines is just a few thousand dollars away from reaching its $45K Kickstarter goal.",https://www.reddit.com/r/MachineLearning/comments/3mc84f/with_only_22_hours_left_the_podcast_talking/,onewugtwowugs,1443191369,,2,37
523,2015-9-26,2015,9,26,0,3mcdvs,Need help finding article ranking ML performance.,https://www.reddit.com/r/MachineLearning/comments/3mcdvs/need_help_finding_article_ranking_ml_performance/,holidaytie,1443193670,"Within the past year I think, there was a buzz-worthy article that ranked performance of MANY predictive/ML methods.  I think the authors were testing Kaggle data.  The nutshell was that about 3 or 4 techniques were consistent winners.  I spent 15 minutes on google and I can't find it :(",2,2
524,2015-9-26,2015,9,26,0,3mcf3w,Why we need journalism about machine learning,https://www.reddit.com/r/MachineLearning/comments/3mcf3w/why_we_need_journalism_about_machine_learning/,stellabot,1443194130,,0,1
525,2015-9-26,2015,9,26,0,3mch3m,Machine Learning and Cognitive Computing,https://www.reddit.com/r/MachineLearning/comments/3mch3m/machine_learning_and_cognitive_computing/,ancatrusca,1443194932,,0,2
526,2015-9-26,2015,9,26,1,3mcscd,Neural Art Applied to Video,https://www.reddit.com/r/MachineLearning/comments/3mcscd/neural_art_applied_to_video/,zhengwy,1443199487,,3,7
527,2015-9-26,2015,9,26,1,3mctnc,"The Data Incubator, 7 week fellowship for data scientists, $150k+ salary guaranteed after completition",https://www.reddit.com/r/MachineLearning/comments/3mctnc/the_data_incubator_7_week_fellowship_for_data/,datainc_michaell,1443200027,,13,0
528,2015-9-26,2015,9,26,2,3mcw2l,The Data Incubator,https://www.reddit.com/r/MachineLearning/comments/3mcw2l/the_data_incubator/,DSLSharedTask,1443201004,[removed],0,0
529,2015-9-26,2015,9,26,2,3mcw8r,"""Python Machine Learning"" (the book) is finally out - a reflection of what it is all about and what it's not",https://www.reddit.com/r/MachineLearning/comments/3mcw8r/python_machine_learning_the_book_is_finally_out_a/,[deleted],1443201070,[deleted],0,1
530,2015-9-26,2015,9,26,2,3mcwbe,"""Python Machine Learning"" (the book) is finally out - a reflection on what it is all about and what it's not",https://www.reddit.com/r/MachineLearning/comments/3mcwbe/python_machine_learning_the_book_is_finally_out_a/,rasbt,1443201098,,2,40
531,2015-9-26,2015,9,26,5,3mdmzf,[AMA Crosspost] Google researchers working on Deep Learning. /r/IAmA,https://www.reddit.com/r/MachineLearning/comments/3mdmzf/ama_crosspost_google_researchers_working_on_deep/,nhammel,1443211635,,1,63
532,2015-9-26,2015,9,26,6,3mdvxv,Neural Net in C++ for absolute beginners : Super easy step-by-step walkthrough into programming a neural net - by David Miller,https://www.reddit.com/r/MachineLearning/comments/3mdvxv/neural_net_in_c_for_absolute_beginners_super_easy/,hockiklocki,1443215790,,13,163
533,2015-9-26,2015,9,26,7,3me9sm,2015 Data Science Salary Survey Report,https://www.reddit.com/r/MachineLearning/comments/3me9sm/2015_data_science_salary_survey_report/,[deleted],1443221578,[deleted],0,2
534,2015-9-26,2015,9,26,8,3meas2,2015 Data Science Salary Survey Report,https://www.reddit.com/r/MachineLearning/comments/3meas2/2015_data_science_salary_survey_report/,gradientflow,1443222012,,6,7
535,2015-9-26,2015,9,26,8,3mebsg,How a cascade classifier is trained?,https://www.reddit.com/r/MachineLearning/comments/3mebsg/how_a_cascade_classifier_is_trained/,cudoer,1443222428,"I am currently working on Pedestrian detection using SVM. My first attempt was to test the traditional Dalal &amp; Triggs algorithm which basically train a HOG Classifier from positive and negative data to build the separating Hyperplane.

However, I've recently stumbled upon articles like [1] using cascade training. Could someone quickly tell me how it works ?

Cheers!

[1] http://www.merl.com/publications/docs/TR2006-068.pdf",0,4
536,2015-9-26,2015,9,26,11,3mey4g,Suppose I have examples with 10 continuous attributes and that I have a lot of data. Can a 3 (or another number) hidden layer fully connected neural network (maybe with Relu and dropout) beat all other methods such as SVM and Random forests and others?,https://www.reddit.com/r/MachineLearning/comments/3mey4g/suppose_i_have_examples_with_10_continuous/,andrewbarto28,1443233136,,6,2
537,2015-9-26,2015,9,26,11,3meys7,8 heads filling machine with capping machine,https://www.reddit.com/r/MachineLearning/comments/3meys7/8_heads_filling_machine_with_capping_machine/,dongfengpacking,1443233462,,0,1
538,2015-9-26,2015,9,26,16,3mfszl,Any recommendation about the Initial State of RNNs?,https://www.reddit.com/r/MachineLearning/comments/3mfszl/any_recommendation_about_the_initial_state_of_rnns/,yhg0112,1443251344,"this week, i was implementing simple RNNs for simple test cases using theano.

i'm just curious about the relation between the initial hidden state value of RNNs vs. performance.

do any one have ideas or references about this? in my case, which is too simple to refer, i've set it all 0s or randomly initialized, and they doesen't make difference in fact. ",10,7
539,2015-9-26,2015,9,26,16,3mftg8,Has Facebook's AI inadvertently learnt dating?,https://www.reddit.com/r/MachineLearning/comments/3mftg8/has_facebooks_ai_inadvertently_learnt_dating/,heltok,1443251679,"I have been doing some online dating and I guess I have friended, stalked and been stalked by some of my online dates on Facebook, so I there should be plenty of training data for the algos. When I today checked ""people you might know"" a lot of other people I see as matches on the online dating sites, but have not contacted, showed up. And it was not like it was just one or two, but it actually suggested that I might know at least 4 persons, from the dating community that shows up as matches for me(I would guess the pool of 100% matches is like 50-100 women so you see the same people every day). With all of them I had 0 to 1 friends in common. 


So my theory is that it the unsupervised AI has picked up some group of people, online daters, who are likely to intermingle and it might even have learnt the preferences of some the subgroups who seems to belong to this group. What's your take on this? ",2,0
540,2015-9-26,2015,9,26,16,3mfu2z,Taming the ReLU with Parallel Dither in a Deep Neural Network,https://www.reddit.com/r/MachineLearning/comments/3mfu2z/taming_the_relu_with_parallel_dither_in_a_deep/,ajrs,1443252160,,5,0
541,2015-9-26,2015,9,26,16,3mfvez,Philosophy Graduate to Machine Learning Practitioner,https://www.reddit.com/r/MachineLearning/comments/3mfvez/philosophy_graduate_to_machine_learning/,jasonb,1443253173,,0,0
542,2015-9-26,2015,9,26,20,3mgd65,Top Deep Learning Employers Based On LinkedIn Data,https://www.reddit.com/r/MachineLearning/comments/3mgd65/top_deep_learning_employers_based_on_linkedin_data/,omerodonnel,1443267312,,2,4
543,2015-9-26,2015,9,26,22,3mgp3w,Why is the log-likelihood value reported in the paper on Generative Adversarial Networks positive?,https://www.reddit.com/r/MachineLearning/comments/3mgp3w/why_is_the_loglikelihood_value_reported_in_the/,wildtales,1443275076,"Log likelihood is the log of probability and hence should be negative.  On the other hand, if we assume that the values reported in the paper on generative adversarial networks correspond to negative log-likelihood, then lower value should imply a better model which is not the case in this paper.",4,1
544,2015-9-27,2015,9,27,3,3mhn8d,Need guidance for setting up Nvidia GTX 980 Ti for deep learning,https://www.reddit.com/r/MachineLearning/comments/3mhn8d/need_guidance_for_setting_up_nvidia_gtx_980_ti/,Pysnap,1443290747,[removed],0,1
545,2015-9-27,2015,9,27,3,3mho6q,New CIFAR-100 Result: 75.68%,https://www.reddit.com/r/MachineLearning/comments/3mho6q/new_cifar100_result_7568/,anqmj,1443291141,,11,5
546,2015-9-27,2015,9,27,4,3mhvlv,Yoshua Bengio: An objective function for STDP,https://www.reddit.com/r/MachineLearning/comments/3mhvlv/yoshua_bengio_an_objective_function_for_stdp/,downtownslim,1443294641,,5,8
547,2015-9-27,2015,9,27,4,3mhyju,"How do I extract relevant, useful information from a particular sentence? Perhaps using Python?",https://www.reddit.com/r/MachineLearning/comments/3mhyju/how_do_i_extract_relevant_useful_information_from/,n00bto1337,1443296020,"I have a sentence, ""Let's meet for coffee at 8 tomorrow night."" I need to extract the time from this particular sentence, in this case it will be 8pm. Maybe even extend this further to calculate, from today's date, the exact date and time mentioned in the sentence. I believe I can do this using NLTK in Python, but exactly how should I proceed?",2,1
548,2015-9-27,2015,9,27,4,3mi05d,Surya Ganguli: The statistical physics of deep learning,https://www.reddit.com/r/MachineLearning/comments/3mi05d/surya_ganguli_the_statistical_physics_of_deep/,downtownslim,1443296773,,12,40
549,2015-9-27,2015,9,27,6,3mia0c,50 external machine learning / data science resources and articles,https://www.reddit.com/r/MachineLearning/comments/3mia0c/50_external_machine_learning_data_science/,[deleted],1443301449,[deleted],0,1
550,2015-9-27,2015,9,27,6,3midhd,How are the layers chosen in a big conv net?,https://www.reddit.com/r/MachineLearning/comments/3midhd/how_are_the_layers_chosen_in_a_big_conv_net/,jamesj,1443303102,"Are there any good tutorials on this? I've been training RNN's, smaller NN's and playing around with pre-trained nets like GoogLeNet and VGG-19 and I'm wondering how they arrived at their current layer configurations, and how you reason about where to use conv, pool, relu, etc layers and what their settings should be.",5,17
551,2015-9-27,2015,9,27,8,3misj7,Newbie here: I want to make a program that sorts music based on my preferences,https://www.reddit.com/r/MachineLearning/comments/3misj7/newbie_here_i_want_to_make_a_program_that_sorts/,EyeZiS,1443310563,"I want to make some kind of program that sorts my large collection of music (1000+ items) based on my previous ratings of music (I have ~600 of them).

I have experience working with numpy and python, but nothing to do with machine learning or neural networks.

I think that converting the tracks to uncompressed wavs and teaching a neural network to correlate the data with a rating (similar to how image recognition with the MNIST data set works) is the correct way to do this.

My question is: What's the fastest, most efficient, and noob friendly way to do this?

EDIT: I have read all of your comments and will begin learning how to build and train convolutional neural networks. I'll keep all of you posted on my progress over [here](https://redd.it/3mo0jt) (it's just a reserved text post as of now). ",14,18
552,2015-9-27,2015,9,27,9,3miwvs,Elephas: Keras Deep Learning on Apache Spark,https://www.reddit.com/r/MachineLearning/comments/3miwvs/elephas_keras_deep_learning_on_apache_spark/,fariax,1443312769,"Elephas: Keras Deep Learning on Apache Spark

http://flip.it/4NBUB",3,12
553,2015-9-27,2015,9,27,9,3mixxn,How complex can neural network manipulation of input data get?,https://www.reddit.com/r/MachineLearning/comments/3mixxn/how_complex_can_neural_network_manipulation_of/,[deleted],1443313320,[deleted],4,4
554,2015-9-27,2015,9,27,9,3mj1ud,Implementing ML into a flight controller,https://www.reddit.com/r/MachineLearning/comments/3mj1ud/implementing_ml_into_a_flight_controller/,confusedX,1443315444,"Hey guys, 

I have a quadcopter for which I'm designing a controller to perform several tasks autonomously. I'm somewhat new to ML but I want to use what I've recently learned for this project.

First thing first, I have a ""reasonable"" estimate for the physical characteristics of the craft - the location of the CG and the moments of inertia. I need better, and at this point I can either run some complicated spin-up experiments on the quadcopter, or I can fly it around and try to use an algorithm. 

I have the equations of motion, I just need the algorithm to provide better estimates of these physical characteristics. I plan to use my flight log data and run stochastic gradient descent to minimize a least square error function. I figure I can keep training the algorithm during flight if I use stochastic.

Second, I want to be able to give an ML algorithm a path that I want to take, and it will come up with the command set using my simulator. It'll probably be a reinforcement learning algorithm, any suggestions as to which one would be best?

Thanks",0,5
555,2015-9-27,2015,9,27,11,3mjcl0,"The Boosting Margin, or Why Boosting Doesn't Overfit",https://www.reddit.com/r/MachineLearning/comments/3mjcl0/the_boosting_margin_or_why_boosting_doesnt_overfit/,carmichael561,1443321522,,23,45
556,2015-9-27,2015,9,27,14,3mjsdq,Need Help with a project. Bird-voice recognition,https://www.reddit.com/r/MachineLearning/comments/3mjsdq/need_help_with_a_project_birdvoice_recognition/,arvindkumar422,1443331583,"Hello guys , this is an aspiring student seeking help in a real world project. The purpose of the project is basically bird voice recognition.Given a sample of audio clips containing bird songs, prediction of the bird species has to be done. I've looked at some papers and I need help to decide the optimal way of doing as this is my first project and I have no research exp as such.
A clear approach would be appreciated.
Thank you !",4,3
557,2015-9-27,2015,9,27,18,3mk9xt,"My paper: Tensorizing neural networks, NIPS-2015",https://www.reddit.com/r/MachineLearning/comments/3mk9xt/my_paper_tensorizing_neural_networks_nips2015/,bihaqo,1443347672,"Any questions and feedback are highly appreciated.

[pdf](http://arxiv.org/pdf/1509.06569v1.pdf), [Matlab code](https://github.com/Bihaqo/TensorNet)",8,25
558,2015-9-27,2015,9,27,18,3mka00,Deep Unsupervised Learning using Nonequilibrium Thermodynamics,https://www.reddit.com/r/MachineLearning/comments/3mka00/deep_unsupervised_learning_using_nonequilibrium/,rndnum123,1443347729,,4,30
559,2015-9-27,2015,9,27,21,3mkmdd,word2vec in gensim: how to generate translation matrix,https://www.reddit.com/r/MachineLearning/comments/3mkmdd/word2vec_in_gensim_how_to_generate_translation/,l8_miller,1443358294,"Hi, I'm pretty new to both machine learning and reddit so my apoligies if this topic is out of place, in the wrong subreddit, or not appropriate.

This semester, my professor has asked me to investigate word2vec, by T Milokov and his team at Google, and particularly with regards to machine translation. For this task, I'm using the implementation of word2vec in the gensim package for python.

In the paper (link below) Milokov describes how after training two monolingual models, they generate a translation matrix on the most frequently occurring 5000 words, and using this translation matrix, evaluate the accuracy of the translations of the following 1000 words. Paper: http://arxiv.org/pdf/1309.4168.pdf 

Here are two screencaps, one of the description of the matrix in the paper and one of some clarification Milokov posted on a board.

From paper: http://imgur.com/vdYyy2N
Milokov post: http://imgur.com/UmMNHWY

I have been playing around with the models I have generated for Japanese and English in gensim quite a bit. I downloaded wikipedia dumps, processed and tokenised them, and generated the models with gensim. 

I would like to emulate what Milokov did and see how accurate the translation are for Japanese/English (my two languages). I am unsure how to get the top 6000 words (5000 for make the trans vector, 1000 for testing), and especially how to produce the vector. I have read the papers and seen the algorithms but can't quite put it into code. 

If anyone has some ideas/suggestions on how to do so, provide some pseudocode or has gensim knowledge and can lend a hand it would be greatly appreciated. I'm very motivated for this task but having difficulty progressing.",7,11
560,2015-9-28,2015,9,28,3,3mlps2,Implications of patents on Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/3mlps2/implications_of_patents_on_machine_learning/,[deleted],1443378492,[deleted],2,1
561,2015-9-28,2015,9,28,5,3mm62g,Looking for research papers comparing AI success with and without machine learning,https://www.reddit.com/r/MachineLearning/comments/3mm62g/looking_for_research_papers_comparing_ai_success/,[deleted],1443385701,"I realize ML dominates this field and has for many years, but was looking for a solid comparison between AI without machine learning to compare exactly how much better machine learning makes AI for my research project.
I was hoping r/MachineLearning could help my so far fruitless search",8,1
562,2015-9-28,2015,9,28,8,3mmqop,Build your own Nvidia DevBox,https://www.reddit.com/r/MachineLearning/comments/3mmqop/build_your_own_nvidia_devbox/,sparcity_of_time,1443395091,,9,75
563,2015-9-28,2015,9,28,8,3mmt2z,YCombinator 2015 predictions (based on machine learning),https://www.reddit.com/r/MachineLearning/comments/3mmt2z/ycombinator_2015_predictions_based_on_machine/,belfand,1443396223,,1,0
564,2015-9-28,2015,9,28,8,3mmug3,50 external machine learning / data science resources and articles,https://www.reddit.com/r/MachineLearning/comments/3mmug3/50_external_machine_learning_data_science/,hooting_corax,1443396876,,0,1
565,2015-9-28,2015,9,28,9,3mn36j,Any self-serve deep-learning products/toolkits?,https://www.reddit.com/r/MachineLearning/comments/3mn36j/any_selfserve_deeplearning_productstoolkits/,mikos,1443401011,Any products that let users upload their own training set and create their own CNNs? Clarifai etc. seem to provide a comprehensive model using the imagenet dataset. BUt any applications that let you do your own training set/models?,3,1
566,2015-9-28,2015,9,28,10,3mn6gu,AUC to compare models,https://www.reddit.com/r/MachineLearning/comments/3mn6gu/auc_to_compare_models/,stua8992,1443402635,"Hi all,

I'm not sure if I completely understand AUC as a metric. In my understanding for an SVM for instance, to generate the ROC curve you (could) essentially compare true and false positive rates as you move the max-margin hyperplane along a direction normal to itself. Once you have this the AUC is just the integral from 0 to 1 basically. 

First is this correct? Secondly I can see that for a neural network with two output nodes you could come up with a metric to threshold, like the difference between the nodes (whether or not this is the way it's generally done I don't know). The question then: is it valid to compare AUC scores between a neural network and SVM when the metrics might be doing very different things? ",4,5
567,2015-9-28,2015,9,28,14,3mnxer,Need help for project(Bird image recogonition),https://www.reddit.com/r/MachineLearning/comments/3mnxer/need_help_for_projectbird_image_recogonition/,arvindkumar422,1443416802,"This is my first research project and hence, I have little experience in the same. The purpose of the project is to identify the specie of a bird,given an image of the bird. I've looked at some papers and I'm confused about the optimal approach that I can/should follow.Kindly , send help !
THank you.",5,0
568,2015-9-28,2015,9,28,16,3mo8tx,Interview with Clarifai founder Matthew Zeiler on Deep Learning,https://www.reddit.com/r/MachineLearning/comments/3mo8tx/interview_with_clarifai_founder_matthew_zeiler_on/,ghazalbig,1443424648,,0,1
569,2015-9-28,2015,9,28,20,3moruw,Automate various machine learning/data science steps,https://www.reddit.com/r/MachineLearning/comments/3moruw/automate_various_machine_learningdata_science/,kunjaan,1443439691,,0,5
570,2015-9-28,2015,9,28,20,3mos38,Interactive introduction to R in the browser (needs registration),https://www.reddit.com/r/MachineLearning/comments/3mos38/interactive_introduction_to_r_in_the_browser/,kunjaan,1443439863,,0,8
571,2015-9-28,2015,9,28,21,3movmw,Streamline your predictive pipeline with runtime feature extraction,https://www.reddit.com/r/MachineLearning/comments/3movmw/streamline_your_predictive_pipeline_with_runtime/,ahousley,1443442215,,0,0
572,2015-9-28,2015,9,28,21,3moz37,What do you think of NeuroBayes?,https://www.reddit.com/r/MachineLearning/comments/3moz37/what_do_you_think_of_neurobayes/,d3pd,1443444119,What are your views of NeuroBayes? How does it compare with traditional feedforward neural networks? How does it compare with deep learning?,2,2
573,2015-9-28,2015,9,28,22,3mp239,"""Inverse"" Clustering",https://www.reddit.com/r/MachineLearning/comments/3mp239/inverse_clustering/,FireOnSomething,1443445794,"Hello!

I'm used to work with clustering algorithms, which maximize the variance between groups, and minimize the variance within groups.

But now, the problem I'm facing is with learinging groups. The aim is to group people in a way that they have a high variance in experience in the topic. This allows people with less experience to learn with the wiser ones.

But this problem is a ""inverse"" clustering, because it aims to maximize the variance within groups, and minimize the variance between groups.

Does anyone know a method that works according to this problem?

Thanks in advance!",12,4
574,2015-9-28,2015,9,28,23,3mp8jo,Methods for modeling or exploring future events?,https://www.reddit.com/r/MachineLearning/comments/3mp8jo/methods_for_modeling_or_exploring_future_events/,Oreska,1443449055,"Hello everyone,

I am working on a certain dataset with dates that a certain event happens, for example the day that a user first downloads a certain app (with covariates about each user). As you might imagine, most users haven't downloaded the app yet, and don't have a date available. Therefore, a standard regression approach doesn't really work, because when you only use the users that already have dates for the training, you will never predict dates in the future.

Of course I'm not looking for perfect predictions, but does anyone have ideas on where to start looking, or what kind of techniques to try for these kinds of problems? Even techniques that don't make solid predictions, but just work in an exploratory way might help. (for example, finding groups of users that are more likely to download the app sooner, even if we can't predict an exact date)",3,3
575,2015-9-28,2015,9,28,23,3mpb2f,Collection of SciPy and NumPy tutorials,https://www.reddit.com/r/MachineLearning/comments/3mpb2f/collection_of_scipy_and_numpy_tutorials/,EllisLongmore,1443450189,,0,16
576,2015-9-29,2015,9,29,0,3mpk8z,Programming Models for Deep Learning,https://www.reddit.com/r/MachineLearning/comments/3mpk8z/programming_models_for_deep_learning/,antinucleon,1443454093,,3,64
577,2015-9-29,2015,9,29,0,3mpkyd,Machine Learning Blog(Please visit the link for Machine Learning Information),https://www.reddit.com/r/MachineLearning/comments/3mpkyd/machine_learning_blogplease_visit_the_link_for/,Pavan19485,1443454372,,0,0
578,2015-9-29,2015,9,29,0,3mpltc,How do I deal with thin data when creating scores for products based on reviews?,https://www.reddit.com/r/MachineLearning/comments/3mpltc/how_do_i_deal_with_thin_data_when_creating_scores/,Secret_Identity_,1443454721,"Suppose I have two competing products. One of the products has 10 reviews and average 4/5, the other product has one review that is 4/5. I want to bake in my lack of confidence in the second products score. 

I assume there is an industry best practice, but I don't know what kind of key words to use in my Googling.",4,9
579,2015-9-29,2015,9,29,1,3mpu5f,Tutorials for building an image classifier with deep learning?,https://www.reddit.com/r/MachineLearning/comments/3mpu5f/tutorials_for_building_an_image_classifier_with/,futrawo,1443458174,"I spent some time last night looking for a tutorial on how to develop an image classifier using deep learning but I didn't find quite what I was looking for - does anyone know of a tutorial they would recommend?

Thanks.",6,1
580,2015-9-29,2015,9,29,3,3mqec4,Data Science Hub: Connecting to other data scientists and ask questions,https://www.reddit.com/r/MachineLearning/comments/3mqec4/data_science_hub_connecting_to_other_data/,junling,1443466115,,1,0
581,2015-9-29,2015,9,29,3,3mqf98,New Sentence Extractor,https://www.reddit.com/r/MachineLearning/comments/3mqf98/new_sentence_extractor/,wildcodegowrong,1443466482,,2,1
582,2015-9-29,2015,9,29,10,3mry4f,Learning Game of Life with a Convolutional Neural Network,https://www.reddit.com/r/MachineLearning/comments/3mry4f/learning_game_of_life_with_a_convolutional_neural/,Rotaway,1443489297,,5,35
583,2015-9-29,2015,9,29,11,3ms4qp,What optimizer do you use?,https://www.reddit.com/r/MachineLearning/comments/3ms4qp/what_optimizer_do_you_use/,rantana,1443492279,"Just wanted to do a quick survey on what optimizer people use for their machine learning algorithms. No specific rules on what's considered an optimizer, could be an algorithm like SGD/LBFGS or a library like CVX.

",5,4
584,2015-9-29,2015,9,29,11,3msabs,"39 Machine Learning Libraries for Spark, Categorized [OC]",https://www.reddit.com/r/MachineLearning/comments/3msabs/39_machine_learning_libraries_for_spark/,michaelmalak,1443494859,,0,4
585,2015-9-29,2015,9,29,12,3msg7w,Empirical Questions about Exploding Gradients in RNNs,https://www.reddit.com/r/MachineLearning/comments/3msg7w/empirical_questions_about_exploding_gradients_in/,alexmlamb,1443497700,"Two questions.  Naturally the answers will depend on the dataset and the architecture.  I suppose I'm most interested in LSTMs: 

1.  When exploding gradients occur, do they tend to occur for all instances, or just for a subset of the instances?  If the latter, roughly how large is that fraction?  

2.  If one does gradient clipping and moves along the norm of the ""exploding gradient"", does the ""exploding gradient"" tend to go away after a few updates, or is it a persistent problem?  

3.  Have you tried doing gradient clipping separately for each instance in the mini batch?  If so, did it help?  

Best, 

Alex.  ",3,5
586,2015-9-29,2015,9,29,14,3msusb,Is it possible to turn a deep NN model into an ASIC?,https://www.reddit.com/r/MachineLearning/comments/3msusb/is_it_possible_to_turn_a_deep_nn_model_into_an/,[deleted],1443506191,[deleted],0,2
587,2015-9-29,2015,9,29,16,3mt13p,Encoding Reality: Prediction-Assisted Cortical Learning Algorithm in Hierarchical Temporal Memory,https://www.reddit.com/r/MachineLearning/comments/3mt13p/encoding_reality_predictionassisted_cortical/,fergbyrne,1443510660,,10,7
588,2015-9-29,2015,9,29,19,3mte74,"Question about deep learning, distributed representations and how they could advance neuroscience. [question in post]",https://www.reddit.com/r/MachineLearning/comments/3mte74/question_about_deep_learning_distributed/,dsocma,1443521058,"""There's something about distributed representations that makes them seem very biological""  -Ryan Adams [the talking machines podcast]

So he seems to believe that the brain uses a similar mechanism as distributed representations.  If that were the case, are there any guesses on how the brain can work with vectors?

Do ANN's and simulations provide any hints?",3,2
589,2015-9-29,2015,9,29,19,3mtgw4,"Introducing Machine Dreaming, the new homepage for Data Scientists!",https://www.reddit.com/r/MachineLearning/comments/3mtgw4/introducing_machine_dreaming_the_new_homepage_for/,giraffeafavouritepet,1443523148,,13,13
590,2015-9-29,2015,9,29,21,3mto25,"Data: MindBigData, the ""MNIST"" of Brain Digits",https://www.reddit.com/r/MachineLearning/comments/3mto25/data_mindbigdata_the_mnist_of_brain_digits/,compsens,1443528161,,1,3
591,2015-9-29,2015,9,29,22,3mtxm6,Help: Regular Expression in R language,https://www.reddit.com/r/MachineLearning/comments/3mtxm6/help_regular_expression_in_r_language/,ChotiDon,1443533290,"string1&lt;-""ccjar_neutral v_neutral vaux_neutral nnp_neutral prn_neutral v_neutral inj_neutral""

How to write regular expression to
1. Extract only ""v_neutral"" from string1
2. Extract ""v_neutral vaux_neutral"" from string1",5,1
592,2015-9-29,2015,9,29,23,3mu1wg,What Technical Skills are Required for Machine Learning Careers?,https://www.reddit.com/r/MachineLearning/comments/3mu1wg/what_technical_skills_are_required_for_machine/,[deleted],1443535378,[deleted],0,0
593,2015-9-29,2015,9,29,23,3mu1x7,My first neural network,https://www.reddit.com/r/MachineLearning/comments/3mu1x7/my_first_neural_network/,kupo1,1443535388,"I've taken Andrew Ng's machine learning course on coursera and got started with a simple neural network. The data I'm using is for breast cancer with 699 example and 9 features. I've split the dataset into 499, 100 and 100 examples for training, cross-validation and testing respectively. I started with two hidden layers of nine units each. After a lot of testing, I found that 3 units for each hidden layer works best. The accuracy for training data is 97.39% and 100% for cross-validation data. I'm also using regularization to lower variance in training. Any ideas on what to do next or should I just include cross-validation data into training to increase the overall accuracy of the neural network?


Update: I decided to randomize my data before moving on to nth-fold CV. I randomized the data and saved it into a new file. Used new data in the program and gave it a go. Now my training accuracy is at 98.3% and validation accuracy at 98%! Thanks for the suggestions everyone. I'll play a bit with the architecture of network and later on move to nth-fold CV to make the model more reliable. 


2nd update: Just implemented 10-fold cross validation. It's quite time consuming as compared to validation but it's also very accurate. Now I'm looking for lowest cross-validation error. With current architecture, training accuracy is at 98.1% and cross-validation accuracy is at 96.7%.",24,32
594,2015-9-29,2015,9,29,23,3mu3nq,Which is the best approach for research?,https://www.reddit.com/r/MachineLearning/comments/3mu3nq/which_is_the_best_approach_for_research/,arvindkumar422,1443536188,Is reading submitted papers the best/only way to achieve fruitful goals with respect to general machine research problems??,3,4
595,2015-9-29,2015,9,29,23,3mu6oh,Hitachi says it can predict crimes before they happen,https://www.reddit.com/r/MachineLearning/comments/3mu6oh/hitachi_says_it_can_predict_crimes_before_they/,iltoroblu,1443537531,,14,18
596,2015-9-30,2015,9,30,0,3mua3l,Machine learning Thesis in a University,https://www.reddit.com/r/MachineLearning/comments/3mua3l/machine_learning_thesis_in_a_university/,deathstone,1443539011,"Hello r/MachineLearning

This might seem out of the ordinary but this is a genuine request. I am an undergraduate student majoring in Computer Science. I am in the final year of my bachelors with One semester remaining. I have worked in the fields of ML and NLP and hope to stick to this field; 

I wish to conduct a Thesis next semester in another University. This is where my request comes in, I have been mailing professors working in these fields, after reading a few of their research papers; I state my research interests and a few other details but I feel that this methodology is counter-productive and sort of a hit or a miss. 

I expect quite a few of the r/MachineLearning regulars to be working in research/Industry. I would appreciate it if you could tell me if there are any research/work opportunities for 6 months (Jan-July '16) in your department or you might know of any such opportunities. I would gladly provide you with my resume/transcripts/sop if and when required.

I apologize for rambling a bit, English is not my first language.

Regards
 ",0,0
597,2015-9-30,2015,9,30,1,3mukx7,Optimizing RNN Performance,https://www.reddit.com/r/MachineLearning/comments/3mukx7/optimizing_rnn_performance/,ekelsen,1443543482,,1,31
598,2015-9-30,2015,9,30,2,3muxrm,Neural network to analyze physiological parameters in real time,https://www.reddit.com/r/MachineLearning/comments/3muxrm/neural_network_to_analyze_physiological/,DersPeroni,1443548574,"The goal of this project is to predict the outcome of a disease given the progression of various physiological parameters such as heart rate, blood pressure, intracranial pressure, EEG, ECG, cortical blood oxygenation, etc. From this data you can easily calculate a running stream of secondary indices that give you more information about the patient. For example, pressure reactivity index (PRx) is the running correlation between ICP and ABP. It reflects the state of cerebral autoregulation (how well the blood vessels can adapt to changes in blood flow). If it is too high or too low the patient has impaired autoregulation and treatment is required. I have approximately 10 such running variables I was hoping to integrate into a neural network that could identify a pattern to better predict outcome.

I'm pretty new to machine learning though and I'm not really sure where to start. Can any provide an example of a similar project or give me advice on how to set this up? Any input would be greatly appreciated!",5,1
599,2015-9-30,2015,9,30,2,3muyfg,"Unique projects and concepts, based on Blockchain technology",https://www.reddit.com/r/MachineLearning/comments/3muyfg/unique_projects_and_concepts_based_on_blockchain/,professorXY,1443548833,,0,0
600,2015-9-30,2015,9,30,2,3muyy9,Make Classification Tree From Reading Wikipedia,https://www.reddit.com/r/MachineLearning/comments/3muyy9/make_classification_tree_from_reading_wikipedia/,LeavesBreathe,1443549032,"Hey Everyone,

For a long time now, I've been trying to make a deep learning program that will read Wikipedia and make some sort of classification tree.

For example, it would come up with the category of ""animals"" on its own and make a sub classifications (""fox, dog, cat, etc.). 

I thought the best way to tackle this would to be use Deep Belief Networks (http://deeplearning.net/tutorial/DBN.html) but it has been a nightmare to get this to work. Do you guys have any suggestions?

I regularly use Theano and Python for Deep Learning. I would highly recommend Keras (which uses these two) for anyone as it helped me enormously. If this can be done in any way using Keras it would be a miracle to me. 
",8,3
601,2015-9-30,2015,9,30,3,3mv99u,What are some resources for practical machine learning?,https://www.reddit.com/r/MachineLearning/comments/3mv99u/what_are_some_resources_for_practical_machine/,omnipresent101,1443553149,"I've taken Machine Learning course form OMSCS: http://www.omscs.gatech.edu/cs-7641-machine-learning/

The course was predominantly theory mixed with research. 

I'm looking for some practical ways to experiment with machine learning. One example that comes to mind is Face Detection. How does facebook seem to know where the faces are in images when it suggests users to tag their friends. I suspect this is done using ML. 

I can do simple face detection using OpenCV (http://docs.opencv.org/master/d7/d8b/tutorial_py_face_detection.html) but it would have no ML involved. An ML solution, I think, would involve positive and negative images. We would train the classifier using these images and test the results on the test set. 

Approach I would follow:

- Have a set of 100 images with faces
- Store coordinates of faces in each image in a separate file
- use 60 images for the training using decision tree. As part of the training tell the algorithm where each face is in the image
- use 40 images for the testing using decision tree.

The above approach is very vague. For example, how will the algorithm ""know"" where the faces are in the test set? Can someone explain it better?

My questions are:

1. Is there a ""playground area"" where I can experiment with this face detection ML problem?
2. Would Sparks MLlib be good for this type of problem?
3. Is there a book or online resource that would talk about face detection using ML? I understand it is a broad topic but I'm simply looking for the most naive implementation. It doesn't have to at the scale of facebook. ",2,3
602,2015-9-30,2015,9,30,5,3mvkuv,Measuring time spent by the visual cortex,https://www.reddit.com/r/MachineLearning/comments/3mvkuv/measuring_time_spent_by_the_visual_cortex/,actionkick,1443557832,"Hi guys,

I'm working on a object detection problem in medical applications, and lately I've been trying out some convolutional neural networks with success. These are inspired by how the brain works, and I've heard heard statements like ""if the brain can do it in less than x milliseconds, it can be easily solved by deep learning.""

So I was thinking I could estimate the ""difficulty"" of my object detection task by trying to measure how long a doctor needs to locate the same object in an image, by flashing images as different intervals.

My question is whether you have seen any experiments like this done before, and if you have any links to papers or other resources? Papers linking deep learning to the visual cortex would also be great!",0,2
603,2015-9-30,2015,9,30,5,3mvldg,"Facebook just released the code for their paper ""End-To-End Memory Networks""",https://www.reddit.com/r/MachineLearning/comments/3mvldg/facebook_just_released_the_code_for_their_paper/,vkhuc,1443558010,,36,85
604,2015-9-30,2015,9,30,5,3mvn4c,Best Algorithms for Analyzing Few Examples with Many Parameters,https://www.reddit.com/r/MachineLearning/comments/3mvn4c/best_algorithms_for_analyzing_few_examples_with/,sp_0_rt,1443558712,"What are some good algorithms for dealing with datasets with about 5 samples, and each sample has about 200 parameters? Each parameter is a continuous variable. Sometimes the training data will have labels and sometimes it may not, so some supervised and unsupervised suggestions would be greatly appreciated. 

I realize this is a very open-ended question and a definite answer is probably not possible. I would appreciate some knowledgeable input on the topic and even if you just direct me to good literature on the topic, I would really appreciate your help. Thanks!",9,2
605,2015-9-30,2015,9,30,6,3mvy27,Azure Machine Learning: a cloud-based predictive analytics service,https://www.reddit.com/r/MachineLearning/comments/3mvy27/azure_machine_learning_a_cloudbased_predictive/,asdrubaldone,1443563109,,0,3
606,2015-9-30,2015,9,30,7,3mw0gx,A beginners guide to NLP,https://www.reddit.com/r/MachineLearning/comments/3mw0gx/a_beginners_guide_to_nlp/,[deleted],1443564093,[deleted],0,1
607,2015-9-30,2015,9,30,8,3mwen2,Using pitch features to classify prosodic boundaries,https://www.reddit.com/r/MachineLearning/comments/3mwen2/using_pitch_features_to_classify_prosodic/,rodgom,1443570390,,4,6
608,2015-9-30,2015,9,30,9,3mwk3c,Why Machine Learning is Next Big Thing,https://www.reddit.com/r/MachineLearning/comments/3mwk3c/why_machine_learning_is_next_big_thing/,shugert,1443572933,,2,0
609,2015-9-30,2015,9,30,15,3mxocf,Variational Information Maximisation for Intrinsically Motivated Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/3mxocf/variational_information_maximisation_for/,modeless,1443594284,,4,11
610,2015-9-30,2015,9,30,18,3my1ny,A brief history of word embeddings (and some clarifications),https://www.reddit.com/r/MachineLearning/comments/3my1ny/a_brief_history_of_word_embeddings_and_some/,onewugtwowugs,1443604606,,5,19
611,2015-9-30,2015,9,30,20,3mybje,GUI integration with any awesome CV Project.,https://www.reddit.com/r/MachineLearning/comments/3mybje/gui_integration_with_any_awesome_cv_project/,jinjamaverick,1443612149,I have been following CV quite for sometime. I am actually a noob in CV but pretty good at coding. If I wish to make a GUI taking up any existing codebase related to CV. Are there any good CV projects which can be integrated with some GUI framework ?,7,13
612,2015-9-30,2015,9,30,22,3myoq1,"Recurrent Neural Networks Tutorial, Part 2  Implementing a Language Model RNN with Python, Numpy and Theano",https://www.reddit.com/r/MachineLearning/comments/3myoq1/recurrent_neural_networks_tutorial_part_2/,pogopuschel_,1443619397,,5,119
